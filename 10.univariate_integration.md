---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---



# üìñ Univariate integration 

<small>‚è± <span class="eta"></span> | <span class="words"></span> words</small>

````{dropdown} Sources and reading guide

```{figure} _static/img/bibliography/shsc2016.png
:width: 100px
:align: left
```
{cite:ps}`sydsaeter2016`

Chapter 9 (pp. 319-373)

<div style="clear: both"></div>

**Univariate integration section**

These are references that are suitable for an elementary course on mathematical economics:
- {cite:ps}`bradley2013`: Chapter 8 (pp. 427-476)
- {cite:ps}`haeussler1987`: Chapters 14 and 15 (pp. 533-645)
- {cite:ps}`shannon1995`: Chapter 9 (pp. 408-449)

These are references that are suitable for a first-year undergraduate course for mathematics majors on calculus:
- {cite:ps}`kline1967`: Chapters 3, 5, 6, 9, 10, 11, 12, 14, 15, and 16
- {cite:ps}`silverman1969`: Chapters 7 and 14
- {cite:ps}`spivak2006`: Chapters 13, 14, 18, and 19

These references are suitable for an intermediate course on mathematical economics:
- {cite:ps}`chiang2005`: Chapter 14
- {cite:ps}`leonard1992`: The Appendix to Chapter 2 (pp. 111-113)
- {cite:ps}`simon1994`: Appendix A4


**Consumer demand and consumer welfare section**

These are references that are suitable for a first-year undergraduate economics course:
- {cite:ps}`alchian1983`: Chapter 2 (pp. 13-44)
- {cite:ps}`case1989`: Chapters 4-6 (pp. 77-162)
- {cite:ps}`gans2009`: Chapters 4, 5, 7 and 22 (pp. 62-112, 134-154 and 486-515)
- {cite:ps}`hamermesh2006`: Chapters 2, 5 and 6 (pp. 15-24 and 49-77)
- {cite:ps}`heyne1991`: Chapter 2 (pp. 15-45)

These are references that are suitable for a second-year undergraduate economics course:
- {cite:ps}`hirshleifer1988`: Chapters 2 and 7E (pp. 23-54 and 204-212)
- {cite:ps}`hirshleifer2005`: Chapters 2 and 7.3
- {cite:ps}`nicholson1987`: Chapters 1 and 12 (Consumers Surplus) (pp. 5-21 and 334)
- {cite:ps}`varian1987`: Chapters 1 and 15 (pp. 1-19 and 242-266)


These are references that are suitable for a third-year undergraduate economics course:
- {cite:ps}`gravelle1981`: Section C of Chapter 4 (pp. 103-111)
- {cite:ps}`nicholson1998`: Chapters 1, 5 (Consumer Surplus), and 15 (pp. 3-22, 152-157, and 438-458)
- {cite:ps}`takayama1993`: Appendix C (pp. 621-647)
- {cite:ps}`varian1992`: Chapter 10 (pp. 160-171)

**Producer supply and producer welfare section**

- {cite:ps}`alchian1983`: pp. 63-64
- {cite:ps}`hamermesh2006`: Chapters 1, 2, and 7 (pp. 3-24 and 81-90)

````


## Anti-derivative


```{admonition} Definition
:class: caution

Let $I \in \mathbb{R}$ be a non-empty open interval of real numbers that contains more than a single point. In otherwords,
$I=(a, b)=\{x \in \mathbb{R}: a<x<b\}$
wherea $\in \mathbb{R}, b \in \mathbb{R}$, and $a<b$

Let $f: I \to \mathbb{R}$ be some given univariate real-valued function. Suppose that there exists some function $F: I \to \mathbb{R}$ such that

$$
F^{\prime}(x)=\frac{d F(x)}{dx}=f(x) \text { for all } x \in I
$$

In this case, the function $F(x)$ is said to be an **anti-derivative** of the function $f(x)$ on the interval $I$

```

```{admonition} Example
:class: tip

$$
f(x) = 3x^2 + 4x + 1 \quad \implies \quad F(x) = x^3 + 2x^2 + x + 5 \text{ is anti-derivative}
$$

$$
f(x) = \exp(x) \quad \to \quad F(x) = exp(x)-1 \text{ is anti-derivative}
$$

```


### Anti-derivatives are NOT unique

If the function $F(x)$ is an anti-derivative of the function $f(x)$ on the interval $I$, then so is the function $G(x)=F(x)+C$, where $C \in \mathbb{R}$ is some fixed real number. 

This can be seen by differentiating $G(x)$ with respect to $x$. Upon doing this, we obtain

$$
\frac{d}{dx}(F(x)+C)=\frac{d F(x)}{dx}+\frac{d C}{dx}=f(x)+0=f(x)
$$

The constant $C \in \mathbb{R}$ is known as an *arbitrary constant* because it can take the form of any given real number. In effect, if the function $F(x)$ is an anti-derivative of the function $f(x)$ on the interval $I$, then there is an entire family of such anti-derivatives of $f(x)$ on the interval $I$. This family is given by the set

$$
\{F(x)+C: C \in \mathbb{R}\}
$$

% When the domain of a univariate real-valued function $f(x)$ takes the form of a connected open interval of real numbers $I=(a, b)$, and if $F(x)$ is an anti-derivative of $f(x)$, then every anti-derivative of $f(x)$ must belong to the set $\{F(x)+C: C \in \mathbb{R}\}$
% 
% However, this result is not true when the domain of the function $f(x)$ is not a connected open interval of real numbers. This is established by the following counter-example that involves a non-connected (that is, dis-joint) domain for the function $f(x)$
% 
% 
% Suppose that
% 
% $$
% \begin{array}{r}
% f(x)=2 x \text { for all } x \in(0,1) \cup(2,3), \\
% F(x)= \begin{cases}x^{2} & \text { if } x \in(0,1), \\
% x^{2}+1 & \text { if } x \in(2,3),\end{cases}
% \end{array}
% $$
% 
% and
% 
% $$
% G(x)= \begin{cases}x^{2}+1 & \text { if } x \in(0,1) \\ x^{2} & \text { if } x \in(2,3) .\end{cases}
% $$
% 
% Note that $F^{\prime}(x)=G^{\prime}(x)=f(x)$ for all $x \in(0,1) \cup(2,3)$. Thus both $F(x)$ and $G(x)$ are anti-derivatives of $f(x)$
% However, there is no single constant $C \in \mathbb{R}$ such that $G(x)=F(x)+C$ for all $x \in(0,1) \cup(2,3)$
% 
% 
% The problem that occurs in this counter-example is as follows:
% - The arbitrary constant that is required to ensure that $G(x)=F(x)+C$ for all $x \in(0,1)$ (namely $C_{1}=1$ ) is different to the arbitrary constant that is required to ensure that $G(x)=F(x)+C$ for all $x \in(2,3)$ (namely $C_{2}=-1$ )
% - While the result that the entire set of anti-derivatives of $f(x)$ can be found by adding different single arbitrary constants to a particular anti-derivative of $f(x)$ does not apply in this counter-example as a whole, it does apply to any case where we restrict the domain of the function to a subset of the entire domain that is a connected open interval
% - In particular, the result applies if we restrict the domain in the counter-example to be either $(0,1)$ or $(2,3)$


## Indefinite integral

```{admonition} Definition
:class: caution

Suppose that the function $F(x)$ is an anti-derivative of the function $f(x)$ on the connected open interval $I$. 

The **indefinite integral** of $f(x)$, denoted by $\int f(x) dx$, is the set of all anti-derivatives of fucntion $f(x)$

$$
\int f(x) dx = \{ F(x)+C: C \in \mathbb{R} \}
$$

```

Note, however, that it is common to drop the set notation and simply use the integral sign to denote a generic member of the set of all anti-derivatives of $f(x)$ on the interval $I$. 

In other words, the indefinite integral of $f(x)$ on the interval $I$ is

$$
\int f(x) dx = F(x)+C: C \in \mathbb{R}
$$

where $C \in \mathbb{R}$ is an arbitrary constant. Note that here, the arbitrary constant $C$ is interpreted as being truly arbitrary, rather than taking on a particular value

> Note that we mention that interval $I$ is connected. If it was not we could have different arbitrary constants for different parts of the domain, and thus the above expression with a single constant $C$ would not be valid


```{admonition} Example(quick exercise)
:class: tip

$$
f(x)=0 \quad \int f(x) dx = \int 0 dx = \text{?}
$$

$$
f(x)=5 \quad \int f(x) dx = \int 5 dx = \text{?}
$$

$$
f(x)=x \quad \int f(x) dx = \int x dx = \text{?}
$$

$$
f(x)=6 x^2 \quad \int f(x) dx = \int 6 x^2 dx = \text{?}
$$

$$
f(x)=\frac{1}{x} \quad \int f(x) dx = \int \frac{1}{x} dx = \text{?}
$$

```

```{admonition} Some terminology
:class: caution

Suppose that $f: I \to \mathbb{R}$, where $I \subseteq \mathbb{R}$ is a connected open interval, and $F^{\prime}(x)=f(x)$ for all $x \in I$

- $\int f(x) dx=F(x)+C$ is the **indefinite integral** of $f(x)$
- $f(x)$ is the **integrand**
- $dx$ means "with respect to $x$ ", $x$ is the variable of integration
- $F(x)$ is an **anti-derivative** of $f(x)$
- $C$ is an arbitrary constant

```

## Properties of indefinite integral


```{admonition} Fact
:class: important

Suppose that $f: I \to \mathbb{R}$ is at least once continuously differentiable on $I$, where $I \subseteq \mathbb{R}$ is a connected open interval, and $F^{\prime}(x)=f(x)$ for all $x \in I$. Then for any constant $k \ne 0$

$$
\int k f(x) dx = k \int f(x) dx = k F(x)+C
$$

```

- A constant can be taken outside of the integral

````{admonition} Proof
:class: dropdown

- Informal proof
- Note that

$$
\begin{aligned}
 \frac{d}{dx}(k F(x)+C)&=\frac{d}{dx} k F(x)+\frac{d}{dx} C \\
& =k \frac{d}{dx} F(x)+0 \\
& =\quad k F^{\prime}(x) \\
& =\quad k f(x) 
\end{aligned}
$$

- Note also that

$$
k \int f(x) dx=k\left(F(x)+C_{f}\right)=k F(x)+k C_{f}=k F(x)+C
$$

where $C_{f}$ and $C=k C_{f}$ are arbitrary constants. 
$\blacksquare$
````

```{admonition} Fact
:class: important

Suppose that $f: I \to \mathbb{R}$ and $g: I \to \mathbb{R}$ are both at least once continuously differentiable on $I$, where $I \subseteq \mathbb{R}$ is a connected open interval

$$
\int[f(x)+g(x)] dx=\int f(x) dx+\int g(x) dx=F(x)+G(X)+C
$$

```

- together with the previous fact, we establish *linearity* of indefinite integrals in the space in a certain vector space of functions

````{admonition} Proof
:class: dropdown

- Informal proof
- Note that

$$
\begin{aligned}
\frac{d}{dx}(F(x)+G(x)+C) & =\frac{d}{dx} F(x)+\frac{d}{dx} G(x)+\frac{d}{dx} C \\
& =F^{\prime}(x)+G^{\prime}(x)+0 \\
& =f(x)+g(x) 
\end{aligned}
$$

- Note also that $\int f(x) dx=F(x)+C_{f}$ and $\int g(x) dx=G(x)+C_{g}$, so that

$$
\int f(x) dx+\int g(x) dx=F(x)+C_{f}+G(x)+C_{g}=F(x)+G(x)+C
$$

where $C_{f}, C_{g}$, and $C=C_{f}+C_{g}$ are arbitrary constants.
$\blacksquare$
````

Together the previous two facts establish the following result

```{admonition} Fact
:class: important

Suppose that the functions $f_{i}: I \to \mathbb{R}, i \in\{1,2, \cdots, n\}$, are all at least once continuously differentiable on $I$, where $I \subseteq \mathbb{R}$ is a connected open interval
Suppose also that $k_{i} \in \mathbb{R}, i \in\{1,2, \cdots, n\}$, are some given constants, and $C \in \mathbb{R}$ is an arbitrary constant
Suppose as well that $F_{i}^{\prime}(x)=f_{i}(x)$ for all $x \in I$ and for each $i \in\{1,2, \cdots, n\}$
The following formula applies to $\left\{f_{i}(x)\right\}_{i=1}^{n}$

$$
\begin{aligned}
& \int\left[\sum_{i=1}^{n} k_{i} f_{i}(x)\right] dx=\sum_{i=1}^{n}\left[\int k_{i} f_{i}(x) dx\right] \\
& =\sum_{i=1}^{n}\left[k_{i} \int f_{i}(x) dx\right]=\sum_{i=1}^{n} k_{i} F_{i}(x)+C 
\end{aligned}
$$

```

````{admonition} Proof
:class: dropdown

Proof by a repeated application of the previous two facts

````

```{admonition} Fact
:class: important

Suppose that $f: I \to \mathbb{R}$ is at least once continuously differentiable on $I$, where $I \subseteq \mathbb{R}$ is a connected open interval

$$
\frac{d}{dx} \int f(x) dx=f(x)
$$

and 

$$
\int f^{\prime}(x) dx=f(x)+C
$$

```

````{admonition} Proof
:class: dropdown

The fact follows from the definition of the indefinite integral and the anti-derivative.

First formula:

- Suppose that $F(x)$ is an anti-derivative of $f(x)$
- This means that $\int f(x)=F(x)+C$
- Clearly, we must have $\frac{d}{dx} \int f(x)=\frac{d}{dx}(F(x)+C)$
- But $\frac{d}{dx}(F(x)+C)=\frac{d}{dx} F(x)+0=\frac{d}{dx} F(x)=f(x)$, because $F(x)$ is an anti-derivative of $f(x)$ and $C$ is an (albeit arbitrary) constant
- Thus we have $\frac{d}{dx} \int f(x) dx=f(x)$

Second formula:
- Suppose that $f(x)$ is at least once continuously differentiable, and denote its derivative by $f^{\prime}(x)$
- Let $F(x)=f(x)+C$, where $C$ is an arbitrary constant
- Clearly we must have $\frac{d}{dx} F(x)=\frac{d}{dx} f(x)+0=\frac{d}{dx} f(x)=f^{\prime}(x)$
- Furthermore, we have already shown that $\frac{d}{dx} \int g(x) dx=g(x)$. This means that we must have $\frac{d}{dx} \int f^{\prime}(x) dx=f^{\prime}(x)$
- Thus we have $\frac{d}{dx} \int f^{\prime}(x) dx=\frac{d}{dx} F(x)$,
- Upon integrating both sides of this equation, we obtain $\int f^{\prime}(x) dx=F(x)$, so that we have $\int f^{\prime}(x) dx=f(x)+C$

$\blacksquare$
````

```{admonition} Examples
:class: tip

$$
\int \big[6x^2 + \exp(x) \big] dx = 6 \int x^2 dx + \int \exp(x) dx = 2 x^3 + \exp(x) + C
$$

$$
\frac{d}{dx} \int \frac{1}{x} dx = 
\frac{d}{dx} \big[ \ln(|x|) + C\big] =
\frac{1}{x}
= 
\begin{cases}
\frac{1}{x}, & x > 0 \\
- \frac{1}{-x}, & x < 0
\end{cases}
$$

```

## Integration zoo

The list of most common integrals

```{admonition} Fact
:class: important

$$
\int k dx=k x+C
$$

$$
\int x^{n} dx=\frac{x^{n+1}}{n+1}+C
$$

$$
\int x^{-1} dx=\ln|x|+C
$$

$$
\int \exp^x dx=\exp^x+C
$$

$$
\int a^x dx=\frac{a^x}{\ln a}+C
$$

$$
\int \ln x dx=x \ln x -x +C
$$

```


## Integration techniques

There are a number of techniques that can be used to find the indefinite integral of a function

### Direct integration

This technique involves recognising that the integrand is the derivative of a particular function, or, rather, class of functions. It is most useful when the integrand is an elementary function.

**Direct integration** makes use of the fact that if $F^{\prime}(x)=f(x)$, then the indefinite integral of $f(x)$ is given by $\int f(x) dx=F(x)+C$


```{admonition} Example
:class: tip

Since $\frac{d}{dx} x^{\alpha}=\alpha x^{\alpha-1}$, we have $\int \alpha x^{\alpha-1} dx=x^{\alpha}+C$

Since $\frac{d}{dx} e^{a x}=a e^{a x}$, we have $\int a e^{a x} dx=e^{a x}+C$

Let $x \in(0, \infty)$. Since $\frac{d}{dx} \ln (x)=\frac{1}{x}$, we have $\int \frac{1}{x} dx=\ln (x)+C$

```


### Integration by substitution

This is the integration counterpart to the chain rule of differentiation.

Suppose that $G:(a, b) \to \mathbb{R}$ be a function that is differentiable on the interval $(a, b) \subseteq \mathbb{R}$ and let $g(t)=G^{\prime}(t)=\frac{d G(t)}{d t}$ be its derivative when $t \in(a, b)$

Suppose that $t:(c, d) \to(a, b)$ be a function that is differentiable on the interval $(c, d) \subseteq \mathbb{R}$ and let $t^{\prime}(x)=\frac{d t(x)}{dx}$ be its derivative when $x \in(c, d)$

Consider the composite function $F:(c, d) \to \mathbb{R}$ defined by $F(x)=G(t(x))$

Since $G(t)$ is differentiable on $(a, b), t:(c, d) \to(a, b)$, and $t(x)$ is differentiable on $(c, d)$, we know that $F(x)$ is differentiable on $(c, d)$. Denote its derivative by $f(x)=F^{\prime}(x)=\frac{d F(x)}{d t}$ when $x \in(c, d)$

We know from the chain rule of differentiation that $f(x)=F^{\prime}(x)=G^{\prime}(t(x)) t^{\prime}(x)=g(x) t^{\prime}(x)$

Recall that we have $F(x)=G(t(x))$ and $f(x)=F^{\prime}(x)=g(t(x)) t^{\prime}(x)$, where $g(t)=G^{\prime}(t)$
Thus we know that $F(x)+C=\int F(x) dx=\int g(t(x)) t^{\prime}(x) dx$, where $C \in \mathbb{R}$ is an arbitrary constant

Recall that $d t=\left(\frac{d t(x)}{dx}\right) dx=t^{\prime}(x) dx$. Thus we have

$$
\begin{gathered}
F(x)+C=\int F(x) dx=\int g(t(x)) t^{\prime}(x) dx \\
=\int g(t) d t=G(t)+C=G(t(x))+C
\end{gathered}
$$

when $G(t)$ is evaluated at the parameterised point $t=t(x)$

Note that we need to remember to substitute $t=t(x)$ into $G(t)$ to obtain the correct result here


We now have a framework for discussing the technique of **integration by substitution**
Clearly, this is the integral counterpart of the chain rule of differentiation
This techniques is useful in circumstances where each of the following properties is true

1. You do not know how to calculate the indefinite integral of $f(x)$ with respect to $x$

2. You can see a way to express $f(x)$ as $f(x)=g(t(x)) t^{\prime}(x)$ for which you know the functions $g(t)$ and $t(x)$

3. You do know how to calculate the indefinite integral of $g(t)$ with respect to $t$


```{admonition} Fact
:class: important

Suppose that $f: I \to \mathbb{R}$ is at least once continuously differentiable on $I$, where $I \subseteq \mathbb{R}$ is a connected open interval, and let $\omega : \Omega \to I$, where $\Omega \subseteq \mathbb{R}$, be a differentiable function. Then:

$$
\int f(x) dx = \int f(\omega(t)) \omega^{\prime}(t) dt
$$

```



```{admonition} Example
:class: tip

Suppose that we want to find $\int f(x) dx=\int \frac{\ln (x)}{x} dx$
Let $g(t)=t$ and $t(x)=\ln (x)$
This means that $g(t(x))=t(x)=\ln (x)$ and $t^{\prime}(x)=\frac{1}{x}$
Note that $f(x)=\frac{\ln (x)}{x}=g(t(x)) t^{\prime}(x)$ and
$\int g(t) d t=\int t d t=\frac{t^{2}}{2}+C$

Thus we have

$$
F(x)+C=\frac{(t(x))^{2}}{2}+C=\frac{(\ln (x))^{2}}{2}+C=\frac{\ln ^{2}(x)}{2}+C 
$$

As a check on the accuracy of our proposed solution, note that

$$
\begin{gathered}
\frac{d}{dx}\left(\frac{\ln ^{2}(x)}{2}\right)=\left(\frac{1}{2}\right)(2) \ln (x)\left(\frac{d \ln (x)}{dx}\right) \\
\quad=(1) \ln (x)\left(\frac{1}{x}\right)=\frac{\ln (x)}{x}=f(x)
\end{gathered}
$$
```

```{admonition} Example
:class: tip

Suppose that we want to find $\int f(x) dx=\int e^{x^{2}} x dx$
Let $g(t)=\left(\frac{1}{2}\right) e^{t}$ and $t(x)=x^{2}$
This means that $g(t(x))=\left(\frac{1}{2}\right) e^{t(x)}=e^{x^{2}}$ and $t^{\prime}(x)=2 x$
Note that $f(x)=e^{x^{2}} x=g(t(x)) t^{\prime}(x)$ and

$$
\int g(t) d t=\int\left(\frac{1}{2}\right) e^{t} d t=\left(\frac{1}{2}\right) \int e^{t} d t=\left(\frac{1}{2}\right) e^{t}+C
$$

Thus we have

$$
F(x)+C=\left(\frac{1}{2}\right) e^{t(x)}+C=\left(\frac{1}{2}\right) e^{x^{2}}+C 
$$

As a check on the accuracy of our proposed solution, note that

$$
\frac{d}{dx}\left(\left(\frac{1}{2}\right) e^{x^{2}}\right)=\left(\frac{1}{2}\right)(2 x) e^{x^{2}}=x e^{x^{2}}=e^{x^{2}} x
$$
```



### Integration by parts

This is the integration counterpart to the product rule of differentiation.


Suppose that $u:(a, b) \to \mathbb{R}$ and $v:(a, b) \to \mathbb{R}$ are both functions that are differentiable on the interval $(a, b) \subseteq \mathbb{R}$
Suppose also that there exist two other functions that are both differentiable on the interval $(a, b) \subseteq \mathbb{R}, G:(a, b) \to \mathbb{R}$ and $G:(a, b) \to \mathbb{R}$, such that $G^{\prime}(x)=u^{\prime}(x) v(x)$ and $H^{\prime}(x)=v^{\prime}(x) u(x)$

Consider the function $J:(a, b) \to \mathbb{R}$ defined by $J(x)=u(x) v(x)$
We know from the product rule of differentiation that

$$
\frac{d}{dx}(u(x) v(x))=\left(\frac{d u(x)}{dx}\right) v(x)+\left(\frac{d v(x)}{dx}\right) u(x)
$$

which can be rewritten as

$$
\frac{d}{dx}(u(x) v(x))=u^{\prime}(x) v(x)+v^{\prime}(x) u(x)
$$


We have obtained the expression

$$
\frac{d}{dx}(u(x) v(x))=u^{\prime}(x) v(x)+v^{\prime}(x) u(x)
$$

This expression can be rearranged to obtain

$$
u^{\prime}(x) v(x)=\frac{d}{dx}(u(x) v(x))-v^{\prime}(x) u(x)
$$

Thus we know that

$$
\begin{gathered}
\int u^{\prime}(x) v(x) dx=\int\left(\frac{d}{dx}(u(x) v(x))-v^{\prime}(x) u(x)\right) dx \\
=\int \frac{d}{dx}(u(x) v(x)) dx-\int v^{\prime}(x) u(x) dx
\end{gathered}
$$


Note that

$$
\int \frac{d}{dx}(u(x) v(x)) dx=u(x) v(x)+C
$$

where $C \in \mathbb{R}$ is an arbitrary constant

This gives us

$$
\int u^{\prime}(x) v(x) dx=u(x) v(x)-\int v^{\prime}(x) u(x) dx+C 
$$

Note also that $\int u^{\prime} v dx$ and $\int v^{\prime} u dx$ both implicitly incorporate arbitrary constants
If we absorb the explicit arbitrary constant $(C)$ in the above expression into the implicity arbitrary constants in the $\int u^{\prime} v dx$ and $\int v^{\prime} u dx$ terms in some appropriate fashion, then we can rewrite that expression as

$$
\int u^{\prime}(x) v(x) dx=u(x) v(x)-\int v^{\prime}(x) u(x) dx
$$


We have obtained the expression

$$
\int u^{\prime}(x) v(x) dx=u(x) v(x)-\int v^{\prime}(x) u(x) dx
$$

This expression forms the foundation for the integration technique known as **integration by parts**. Clearly, integration by parts is the integral counterpart to the product rule of differentiation

Integration by parts is particularly useful when you want to find $\int f(x) dx=\int u^{\prime} v dx$ and either of the following two situations occur

1. The indefinite integral $\int v^{\prime} u dx$ is easier to obtain than the indefinite integral $\int u^{\prime} v dx$

2. It turns out that $\int v^{\prime} u dx=\mathrm{k} \int u^{\prime} v dx$ for some constant $k \in \mathbb{R}$. (In this case, you get a functional equation in which the "variable" is the integrand $\int u^{\prime} v dx$. This equation can then be solved to obtain integrand $\int u^{\prime} v dx$

Sometimes, you might need to undertake multiple iterations of the technique of integration by parts

```{admonition} Fact
:class: important

Suppose that $u,v: I \to \mathbb{R}$ is at least once continuously differentiable on $I$, where $I \subseteq \mathbb{R}$ is a connected open interval. Then

$$
\int u'(x) v(x) dx = u(x) v(x) - \int v'(x) u(x) dx
$$

```



```{admonition} Example
:class: tip
We want to find $\int \ln (x) dx$
Let $u(x)=x$ and $v(x)=\ln (x)$. This means that $u^{\prime}(x)=1$ and $v^{\prime}(x)=\frac{1}{x}$
This allows us to express the required integral as

$\int \ln (x) dx=\int(1) \ln (x) dx=\int u^{\prime}(x) v(x) dx$

Integrating by parts, and ignoring any arbitrary constants, we obtain

$$
\int \ln (x) dx=x \ln (x)-\int\left(\frac{1}{x}\right) x dx=x \ln (x)-\int dx=x \ln (x)-x 
$$

Thus we can conclude that

$$
\int \ln (x) dx=x \ln (x)-x+C
$$

where $C \in \mathbb{R}$ is an arbitrary constant
```

```{admonition} Example
:class: tip
We want to find $\int \frac{\ln (x)}{x} dx$
Let $u(x)=\ln (x)$ and $v(x)=\ln (x)$. This means that $u^{\prime}(x)=\frac{1}{x}$ and $v^{\prime}(x)=\frac{1}{x}$
This allows us to express the required integral as $\int \frac{\ln (x)}{x} dx=\int u^{\prime}(x) v(x) dx$

Integrating by parts, and ignoring any arbitrary constants, we obtain

$$
\begin{gathered}
\int \frac{\ln (x)}{x} dx=\int u^{\prime}(x) v(x) dx=u(x) v(x)-\int v^{\prime}(x) u(x) dx \\
=(\ln (x))(\ln (x))-\int \frac{\ln (x)}{x} dx=(\ln (x))^{2}-\int \frac{\ln (x)}{x} dx \\
=\ln ^{2}(x)-\int \frac{\ln (x)}{x} dx
\end{gathered}
$$

Ignoring any arbitrary constants, we have shown that

$$
\int \frac{\ln (x)}{x} dx=\ln ^{2}(x)-\int \frac{\ln (x)}{x} dx
$$

This equation can be rearranged to obtain

$$
2 \int \frac{\ln (x)}{x} dx=\ln ^{2}(x)
$$

so that, ignoring any arbitrary constants, we have

$$
\int \frac{\ln (x)}{x} dx=\left(\frac{1}{2}\right) \ln ^{2}(x)
$$

Thus we know that

$$
\int \frac{\ln (x)}{x} dx=\left(\frac{1}{2}\right) \ln ^{2}(x)+C
$$

where $C \in \mathbb{R}$ is an arbitrary constant
```


% ### Integration of rational functions
%
% Integration by partial fractions. (This technique is sometimes useful when the integrand is the ratio of two polynomial functions.)
% 
% If the the integrand $f(x)$ is a rational function, then decomposing that rational function into the sum of a number of more simple rational functions can sometimes help with finding the indefinite integral $\int f(x) dx$. This decomposition technique is known as the method of "partial fractions"
% 
% A rational function $R(x)$ is simply the ratio of two polynomial functions, $P(x)$ and $Q(x)$. It takes the form
% 
% $$
% R(x)=\frac{P(x)}{Q(x)}=\frac{a_{m} x^{m}+a_{m-1} x^{m-1}+\cdots+a_{1} x+a_{0}}{b_{n} x^{n}+b_{n-1} x^{n-1}+\cdots+b_{1} x+b_{0}}
% $$
% 
% where
% 
% $$
% P(x)=a_{m} x^{m}+a_{m-1} x^{m-1}+\cdots+a_{1} x+a_{0}
% $$
% 
% is an $m$ th order polynomial (so that $a_{m} \neq 0$ ), and
% 
% $$
% Q(x)=b_{m} x^{m}+b_{m-1} x^{m-1}+\cdots+b_{1} x+b_{0}
% $$
% 
% is an $n$th order polynomial (so that $b_{n} \neq 0$ )
% 
% 
% Note that there is no requirement that the polynomial functions $P(x)$ and $Q(x)$ be of the same order. (In other words, we do not require that $m=n$.)
% 
% The most interesting case is when $m<n$. In such cases, the rational function $R(x)$ is called a "proper" rational function
% 
% When $m \geqslant n$, then we can always use the process of long division to write the original rational function $R(x)$ as the sum of a polynomial function $Y(x)$ and another proper rational function $R^{*}(x)$
% 
% This is nicely illustrated by the following example from Chapter 14 of {cite:ps}`silverman1969`. Consider the rational function $R(x)=\frac{x^{2}+x-1}{x-1}$. Note the following
% 
% ```{image} _static/img/lecture_10/longdiv.png
% :align: center
% ```
% 
% Thus we have
% 
% $$
% R(x)=\frac{x^{2}+x-1}{x-1}=x+2+\left(\frac{1}{x-1}\right) 
% $$
% 
% 
% Since any non-proper rational function $R(x)$ can be written as the sum of a polynomial function $Y(x)$ and a proper rational function $R^{*}(x)$, we know that
% 
% $$
% \int R(x) dx=\int Y(x) dx+\int R^{*}(x) dx
% $$
% 
% where
% 
% $$
% \begin{aligned}
% \int Y(x) dx &= \int\left(c_{y} x^{y}+c_{y-1} x^{y-1}+\cdots+c_{1} x+c_{0}\right) dx \\
% &=\int\left(\sum_{i=0}^{y} c_{y-i} x^{y-i}\right) dx=\sum_{i=0}^{y}\left(\int c_{y-i} x^{y-i} dx\right) \\
% &=\sum_{i=0}^{y}\left(c_{y-i} \int x^{y-i} dx\right)=\sum_{i=0}^{y}\left(\frac{c_{y-i}}{(y-i+1)} \int(y-i+1) x^{y-i} dx\right)\\
% &=\int\left(c_{y} x^{y}+c_{y-1} x^{y-1}+\cdots+c_{1} x+c_{0}\right) dx \\
% &=\int\left(\sum_{i=0}^{y} c_{y-i} x^{y-i}\right) dx=\sum_{i=0}^{y}\left(\int c_{y-i} x^{y-i} dx\right) \\
% &=\sum_{i=0}^{y}\left(c_{y-i} \int x^{y-i} dx\right)=\sum_{i=0}^{y}\left(\frac{c_{y-i}}{(y-i+1)} \int(y-i+1) x^{y-i} dx\right) \\
% &=\sum_{i=0}^{y}\left(\frac{c_{y-i}}{y-i+1}\right) x^{y-i+1} \\
% &=\left(\frac{c_{y}}{y+1}\right) x^{y+1}+\left(\frac{c_{y-1}}{y}\right) x^{y}+\cdots+\left(\frac{c_{1}}{2}\right) x^{2}+c_{0} x 
% \end{aligned}
% $$
% 
% 
% Thus when $R(x)$ is a non-proper rational function, we can easily calculate the integral of the polynomial component of that function
% Doing so yields:
% 
% $$
% \begin{gathered}
% \int R(x) dx=\int Y(x) dx+\int R^{*}(x) dx \\
% =\left(\sum_{i=0}^{y}\left(\frac{c_{y-i}}{y-i+1}\right) x^{y-i+1}\right)+\int R^{*}(x) dx 
% \end{gathered}
% $$
% 
% This leaves us with the task of finding the integral of the proper rational function that constitutes the remainder term (in other words, the task of finding $\left.\int R^{*}(x) dx\right)$
% In cases where $R(x)$ is a proper rational function to begin with, then this last step is the only task that we face
% 
% 
% Sometimes you will be able to find $\int R^{*}(x) dx$ through direct integration, or integration by substitution, or integration by parts
% In other cases, the technique of partial fractions might help you to find $\int R^{*}(x) dx$
% You might like to review the material on partial fractions that we covered earlier in the semester at this point in time
% 

## Definite integral

> It's all about the area under the curve!

How might we try and measure the area under a curve?
One way might be to try and approximate it by adding up the area of boxes whose sum seems like it should be close to the area under the curve
The advantage of this approach is that calculating the area of boxes (or, rather rectangles) is easy. We know from high school (and perhaps even primary school) that it is just length times breadth (or height times width)

```{admonition} Definition
:class: caution

A **finite partition of an interval** $[a,b] \subset \mathbb{R}$ is a finite set of points 
$T_n = \left\{t_{0}, t_{1}, t_{2}, \cdots, t_{n-1}, t_{n} \right\}$ such that $a=t_{0}<t_{1}<t_{2}<\cdots<t_{n-1}<t_{n}=b$

Let $r(T_n)=r(t_{0},t_{1},\cdots,t_{n-1},t_{n}) = \max_{0\leq i \leq n-1}(t_{i+1}-t_{i})$ denote the **norm of the partition**

```

- note that the partition covers the whole interval: $\left[a, t_{1}\right] \cup\left(t_{1}, t_{2}\right] \cup\left(t_{2}, t_{3}\right] \cup \cdots \cup\left(t_{n-1}, t_{n}\right] \cup\left(t_{n}, b\right]=[a, b]$ 
- and the sub-intervals are disjoint  $\left[a, t_{1}\right] \cap\left(t_{1}, t_{2}\right] \cap\left(t_{2}, t_{3}\right] \cap \cdots \cap\left(t_{n-1}, t_{n}\right] \cap\left(t_{n}, b\right]=\varnothing$

We can use this partition to form the bases of $n$ rectangles. The bases are given by the sub-intervals, and the heights are given by the function values at the interval.
- Which function values should we use? The left end-point, the right end-point, or the mid-point?


```{admonition} Definition (Riemann sum)
:class: caution

Let $f: [a,b] \to \mathbb{R}$ be a function defined on a connected interval $[a,b] \subset \mathbb{R}$, and let $T = \left\{t_{0}, t_{1}, t_{2}, \cdots, t_{n-1}, t_{n} \right\}$ be a partition of the interval $[a,b]$ with norm $r(T)$.

The **Riemann sum** is the sum of the form 

$$
S(T_n) = \sum_{i=1}^{n-1} f(x_i^{\star})(x_{i+1} - x_i), \quad x_i^{\star} \in [x_i, x_{i+1}] \text{ for all } i=1,\dots,n-1
$$

```

- In Riemann sums the choice of $x_i^{\star}$ within each subinterval $[x_i, x_{i+1}]$ is free!


```{admonition} Definition (Darboux sums)
:class: caution

In the same settings, the **upper and lower Darboux sums** are giveb by

$$
U(T_n) = \sum_{i=1}^{n-1} \max_{x \in [x_i, x_{i+1}]}f(x)(x_{i+1} - x_i)
$$

$$
L(T_n) = \sum_{i=1}^{n-1} \min_{x \in [x_i, x_{i+1}]}f(x)(x_{i+1} - x_i)
$$

```

```{figure} _static/img/lecture_11/lower_upper.png
:width: 30%
:align: center

Riemann and Darboux lower and upper sums
```

```{admonition} Definition
:class: caution

A function $f: [a,b] \to \mathbb{R}$ is said to be **Riemann integrable** on $[a,b]$ if for any finite partition of $[a,b]$ which norm converges to zero,
the limit of the difference of the Darboux upper and lower sums exists and is also equal to zero

$$
\lim_{n \to \infty} U(T_n)-L(T_n) = 0 \quad \text{for all} \quad 
T_n: \lim_{n \to \infty} r(T_n) = 0
$$

That is for any $\epsilon > 0$ there exists a $\delta > 0$ such that for any partition $T_n$ with $r(T_n) < \delta$ we have $|U(T_n)-L(T_n)| < \epsilon$.
```

- most of the functions we encounter in practice are Riemann integrable
- in fact all functions which are continuous on $[a,b]$ *at almost all points* (except for a set of measure zero) are Riemann integrable

```{admonition} Example
:class: tip

An example of a function which is not Riemann integrable is $f: \mathbb{R} \to \mathbb{R}$ defined as

$$
f(x)=
\begin{cases}
0, & x \in \mathbb{Q} \\
1, & x \in \mathbb{R} \setminus \mathbb{Q}
\end{cases}
$$

To see this, note that for any partition $T_n$ we have $U(T_n)-L(T_n) = 1$ because within any (open) interval on real line there are both rational and irrational numbers (where irrational are real which are not rational).
Therefore the requisite limit does not exist and the function is not Riemann integrable.

```

```{admonition} Fact
:class: important

If function $f: [a,b] \to \mathbb{R}$ is Riemann integrable on $[a,b]$, there exists a unique number $I_f(a,b) \in \mathbb{R}$ such that

$$
\lim_{n \to \infty} U(T_n) = 
\lim_{n \to \infty} L(T_n) = 
\lim_{n \to \infty} S(T_n) = 
I_f(a,b) \quad \text{for all} \quad 
T_n: \lim_{n \to \infty} r(T_n) = 0
$$

```

- both Riemann and the two Darboux sums converge to the same value

```{admonition} Definition
:class: caution

Number $I_f(a,b)$ from the previous fact is called the **definite integral** of $f$ over $[a,b]$ and is denoted by

$$
\lim_{n \to \infty} S(T_n) = \int_{a}^{b} f(x) dx
$$

This type of definite integral is known as a **Riemann definite integral**

```

```{figure} _static/img/lecture_11/riemann.png
:width: 80%
:align: center

Limit of Riemann sums
```

````{admonition} Example
:class: tip

Consider $f(x) = \exp(x)$ on $[0,1]$. Let's compute $U(T_n)$, $L(T_n)$ and $S(T_n)$ for finite partitions of the interval with uniformly spaced $n-1$ points.
Because the function is increasing, the upper sum will be the sum of the function values at the right end-points of the subintervals, and the lower sum will be the sum of the function values at the left end-points of the subintervals, and we take midpoint for the Riemann sums.

````

```{code-cell} python3
import numpy as np
fun = lambda x: np.exp(x)
a,b = 0,1
print(f"{'n':<7} {'L(T_n)':>12} {'S(T_n)':>12} {'U(T_n)':>12}")
# for n in range(2, 1000, 10):
L,U,n=0,1,2
while np.abs(U - L) > 1e-5:
  x = np.linspace(a, b, n)     # partition (uniform)
  dx = x[1] - x[0]             # norm of the partition
  U = np.sum(fun(x[1:]) * dx)  # fun at right side
  L = np.sum(fun(x[:-1]) * dx) # fun at left side
  S = np.sum(fun((x[1:] + x[:-1]) / 2) * dx) # fun at midpoint
  print(f"{n:<7.0f} {L:>12.8f} {S:>12.8f} {U:>12.8f}")
  n *= 2
```


### Negative values of integrals

> What happens if $f(x)<0$ for some $x$?

Definite integral is the aggregate *signed area* between the curve that represents the graph of that function and the axis for the independent variable between two points

Some care is needed here. When the curve is above the axis, the area enters the aggregate sum positively. When the curve is below the axis, the area enters the aggregate sum negatively

This "aggregate signed area" is related to the indefinite integral of the function. This relationship is captured through the *Fundamental Theorem of Calculus*


```{figure} _static/img/lecture_11/int_neg.png
:width: 60%
:align: center

Negative and positive values of integral
```

```{admonition} Fact
:class: important

The sign of the integral changes when the limits of the intergration are reversed

$$
\int_{a}^{b} f(x) dx = -\int_{b}^{a} f(x) dx
$$

```


## The fundamental theorem of calculus

> Direct connection between indefinite and definite integrals


### Integral with varying upper limit

```{admonition} Fact
:class: important

Let $f(x)$ be a *continuous* function on $[a,b] \subset \mathbb{R}$.
Define the function $\phi: [a,b] \to \mathbb{R}$ as

$$
\phi(t) = \int_a^t f(x) dx
$$

$\phi(t)$ has the following properties:

- $\phi(t)$ is continuous on $(a,b]$
- $\phi(t) \to 0$ as $t \to a$
- for all $t \in (a,b)$ the derivative of $\phi(t)$ is defined and given by $\phi'(t) = f(t)$
```

- it follows that $\phi$ is an anti-derivative of $f$
- therefore, **every continuous function has an anti-derivative**
- even when it is impossible to find a simple formula for the anti-derivative 

### Newton-Leibniz formula

```{admonition} Fact (Fundamental theorem of calculus)
:class: important

Let $f(x)$ be a *continuous* function on $[a,b] \subset \mathbb{R}$ and $F(x)$ is its anti-derivative on $[a,b]$. Then

$$
\int f(x) dx = F(x) + C \quad \implies \quad \int_a^b f(x) dx = F(b)-F(a)
$$

```

````{admonition} Proof
:class: dropdown

Consider the function $\phi(t) = \int_a^t f(x) dx$, then it is an anti-derivative of $f$ on $[a,b]$ by the previous fact, i.e. $F(x) = \phi(x) +C$.
We have $\phi(a) = 0$ and $\phi(b) = \int_a^b f(x) dx$ (by definition of $\phi$).
Then

$$
F(b)-F(a) = \phi(b) - \phi(a) = \int_a^b f(x) dx -0 = \int_a^b f(x) dx
$$

$\blacksquare$
````
We aslo use notation 

$$
F(x)\Big|_a^b = \Big|_a^b F(x) = F(b) - F(a)
$$


```{admonition} Example
:class: tip

$$
\int_0^1 x^2 dx = \frac{x^3}{3}\Big|_0^1 = \frac{1}{3} -\frac{0}{3} = \frac{1}{3}
$$

$$
\int_0^1 \exp(x) dx = \exp(x)\Big|_0^1 = \exp(1) - \exp(0) = e-1
$$

```

### Infinite integration limits

We can use the integral with the varying upper limit to extend the definition of the integral to infinite limits

```{admonition} Definition
:class: caution

The integral with infinite upper bound exists if the corresponding limit of the integral with the varying upper bound exists

$$
\int_{a}^{\infty} f(x) dx=\lim_{b \to \infty} \int_{a}^{b} f(x) dx
$$

Similarly, the integral with infinite lower bound exists if the corresponding limit of the integral with the varying lower bound exists (varying lower bound is equivalent to varying upper bound with opposite sign of the integral)

$$
\int_{-\infty}^{b} f(x) dx=\lim_{a \to -\infty} \int_{a}^{b} f(x) dx
$$

The integral with $[a,b] = [-\infty,\infty]$ is defined as 

$$
\int_{-\infty}^{\infty} f(x) dx = 
\lim_{a \to -\infty} \int_{a}^{0} f(x) dx +
\lim_{b \to \infty} \int_{0}^{b} f(x) dx
$$

```

```{admonition} Example
:class: tip

$$
\int_1^\infty \frac{1}{x^2} dx = \lim_{b \to \infty} \int_1^b \frac{1}{x^2} dx = \lim_{b \to \infty} \left(-\frac{1}{x}\Big|_1^b\right) = 
\lim_{b \to \infty} \left(1-\frac{1}{b}\Big|_1^b\right) = 1
$$

```

- a lot of the times Newton-Leibniz formula applies *"directly"* wihotuh having to think too much about the limit
- the integrals of this type appear a lot in probability theory
- watch the great video on Gaussian integral by [3Blue1Brown](https://youtu.be/cy8r7WSuT1I?si=corMbP8Bnd3mnyqt) 



## Integration techniques

> The main difference from the techniques applied for the indefinite integrals is that we need to be careful with the limits of integration

```{admonition} Fact (Change of variables in definite integrals)
:class: important

Suppose that $f: [a,b] \to \mathbb{R}$ is integrable on $[a,b]$ and let $\omega : \Omega \to [a,b]$, where $\Omega \subseteq \mathbb{R}$, be a differentiable function with well-defined inverse. Then:

$$
\int_a^b f(x) dx = \int_{\omega^{-1}(a)}^{\omega^{-1}(b)} f(\omega(t)) \omega^{\prime}(t) dt
$$

```

```{admonition} Example
:class: tip

$$
\int_{1}^2 x^3\sqrt{1+x^2} dx = 
$$

Let $t=\omega^{-1}(x)=\sqrt{1+x^2}$, $t\ge 1$, then $x=\omega(t)=\sqrt{t^2-1}$ and 
$\omega'(t) =\frac{t}{\sqrt{t^2-1}}$. 
The limits change as $\omega^{-1}(1) = \sqrt{1+1}$, $\omega^{-1}(2) = \sqrt{1+4}$.
Continueing the derivation

$$
= \int_{\sqrt{2}}^{\sqrt{5}} 
\left(\sqrt{t^2-1}\right)^3 t \frac{t}{\sqrt{t^2-1}} dt 
= \int_{\sqrt{2}}^{\sqrt{5}} 
(t^2-1) t^2 dt 
=
$$

$$
= \int_{\sqrt{2}}^{\sqrt{5}} 
(t^4 - t^2) dt 
= 
\left(\frac{1}{5}t^5 - \frac{1}{3}t^3 \right) 
\Big|_{\sqrt{2}}^{\sqrt{5}}
= \frac{1}{5}5^{\frac{5}{2}} - \frac{1}{3}5^{\frac{3}{2}} - \frac{1}{5}2^{\frac{5}{2}} + \frac{1}{3}2^{\frac{3}{2}}
$$

```

```{admonition} Fact (Integration by part in definite integrals)
:class: important

Suppose that $u,v: [a,b] \to \mathbb{R}$ is integrable on $[a,b]$, where $[a,b] \subseteq \mathbb{R}$ is a connected interval. Then

$$
\int_a^b u'(x) v(x) dx = u(x) v(x) \Big|_a^b - \int_a^b v'(x) u(x) dx
$$

```

```{admonition} Example
:class: tip

$$
\int_{-a}^a x \exp(x) dx =
$$

Let $u(x)=\exp(x)$, $v(x)=x$, then $u'(x)=\exp(x)$, $v'(x)=1$

$$
= x \exp(x) \Big|_{-a}^a - \int_{-a}^a \exp(x) dx
= a\left(e^a + \frac{1}{e^a} \right) - \exp(x) \Big|_{-a}^a =
$$


$$
= a\left(e^a + \frac{1}{e^a} \right) - e^a + \frac{1}{e^a}
= (a-1)e^a + \frac{a+1}{e^a}
$$

```


## Differentiating definite integrals


Consider a definite integral as a function of some parameter variable $t$. We assume that both the integrand and the limits of integration are functions of $t$

$$
I(t)=\int_{a(t)}^{b(t)} f(x,t) dx
$$

```{admonition} Fact (Leibniz' Rule)
:class: important

For a well-defined integral with parameter $t$ it holds

$$
\frac{d I(t)}{dt}=
f\big(b(t),t\big) b^{\prime}(t)-f\big(a(t),t\big) a^{\prime}(t)+
\int_{a(t)}^{b(t)} \frac{\partial f(x,t)}{\partial t} dx
$$

where $a'(t)$ and $b'(t)$ are the derivatives of the limits of integration with respect to $t$

```

- note how Leibnitz rule is a generalization of the integral with varying upper limit



We may consider some special sub-cases of Leibniz' rule:

- $a(t)$ and $b(t)$ are functions of $t$, but $f(x)$ is not a function of $t$. Here we have:

$$
\frac{d I(t)}{dt}=f\big(b(t), t \big) b^{\prime}(t)-f\big(a(t),t\big) a^{\prime}(t)
$$

- $f(x)$ is a function of $t$, but $a$ and $b$ are not functions of $t$. Here we have:

$$
\frac{d I(t)}{dt}=\int_{a}^{b} \frac{\partial f(x, t)}{\partial t} dx
$$


```{admonition} Example
:class: tip

Consider a function $g(t)$ defined as

Derivative of this function can be found directly

$$
g(t) = \int_{0}^{t} \sqrt{x + t} dx = 
\frac{2}{3} (x + t)^{3/2} \Big|_0^t =
\frac{2}{3} [(2t)^{3/2}  - t^{3/2}] =
\frac{2}{3} (2\sqrt{2}-1) t^{3/2}
$$

$$
g'(t) = (2\sqrt{2}-1) \sqrt{t}
$$

Or using the Leibniz rule

$$
g'(t) = \sqrt{2t} + \int_{0}^{t} \frac{dx}{2 \sqrt{x+1}} =
\sqrt{2t} + \frac{1}{2} 2\sqrt{x+t} \Big|_0^t = 
\sqrt{2t} + \sqrt{2t} - \sqrt{t} = (2\sqrt{2}-1) \sqrt{t}
$$


```




## Some economic and econometric applications

- Consumer Welfare: Total Consumer Benefit, Consumer Surplus and Change in Consumer Surplus. (See separate document)
- Producer Welfare: Total Operating Costs, Producers Surplus, and Change in Producers Surplus. (See separate document)
- Choice over Time: Lifetime Utility
- Choice under Uncertainty: Expected Utility
- The Relationship between Probability Density Functions and Cumulative Distribution Functions (Probability and statistics)
- The Expected Value and Other Moments of a Probability Distribution (Probability and statistics)
- The Moment Generating Function and the Characteristic Function (Probability and statistics)


### Preferences over time contingent consumption

Consider a consumer that must choose how to allocate their lifetime wealth over consumption at various points in time

If time is continuous over some interval $[0, T]$ and the consumer can potentially consume at every point in time $t \in[0, T]$, then a time-stream of consumption is a function of the form $c:[0, T] \to[0, \infty)$. This function specifies, for each point in time $t \in[0, T]$, the amount of consumption $c(t) \in[0, \infty)$ that is chosen by the consumer (at time zero) for the point in time $t \in[0, T]$

In such cases, we sometimes assume (at least partly for tractability reasons) that such a consumer's preferences over alternative consumption streams can be represented by a stationary "lifetime utility function" of the form

$$
U\left(\{c(t)\}_{t \in[0, T]}\right)=\int_{0}^{T} u(c(t)) e^{-\rho t} d t
$$

where $\rho \in(0,1)$ is a fixed rate of time preference and $u(c)$ is a stationary per-period (or point-in-time) utility function

### Preferences over state contingent consumption

Suppose that there are an uncountably infinite number of potential states of the world, with each point in the interval non-empty closed interval $(a, b)$ corresponding to a particular state. Denote the state variable by $\omega \in[a, b]$

Nature chooses which state of nature will actually prevail by drawing one state from $(a, b)$ according to a commonly known probability distribution that has a probability density function of the form $f:(a, b) \to[0,1]$

Suppose that a consumer's consumption level varies with the random state of Nature, so that it is given by a function of the form $c:(a, b) \to[0, \infty)$

If the consumer is a von Neumann-Morgenstern (vNM) expected utility maximiser with a Bernoulli utility function $u:[0, \infty] \to \mathbb{R}$ of the form $u(c)$, then the "vNM expected utility function" for this consumer is

$$
U\left(\{c(\omega)\}_{\omega \in[a, b]}\right)=\int_{a}^{b} u(c(\omega)) f(\omega) d \omega 
$$










```{dropdown} Further reading and self-learning
- Gaussian integral [YouTube 3Blue1Brown](https://youtu.be/cy8r7WSuT1I?si=corMbP8Bnd3mnyqt)
```