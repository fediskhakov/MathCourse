# Introduction to optimisation


## Sources and reading guide

```{dropdown} Sources and reading guide

**Introduction to optimisation section**

Primary readings:

{cite:ps}`sydsaeter2016`: Chapters 8, 13, and 14.

Other sources/readings are below.

Introductory level references:
- {cite:ps}`bradley2008`: Chapter 7 (Section 4) (pp. 408-420).
- {cite:ps}`haeussler1987`: Chapter 17 (Section 8) (pp. 706-714).
- {cite:ps}`shannon1995`: Chapter 10, Section 7 (pp. 489-501).

Intermediate level references:
- {cite:ps}`chiang2005`: Chapters 9 and 11-13 (pp. 220-254 and 291-442).
- {cite:ps}`dixit1990`: Chapters 1 to 9 and the Appendix (pp. 1-144 and 181-185).
- {cite:ps}`leonard1992`: Chapter 1 (pp. 1-86).
- {cite:ps}`silberberg1990`: Chapters 4, 6-14 and 17 (pp. 107-134, 156-490 and 573-612).
- {cite:ps}`simon1994`: Chapters 16-22, 23 (Section 8) and 30 (pp. 375-576, 626-629 and 822-844).
- {cite:ps}`starr1997`: (pp. 207-209).
- {cite:ps}`takayama1993`: Chapter 2-5, Appendix B and Appendix C (pp. 75-321 and 605-647).

Advanced level references:
- {cite:ps}`ausubel1993`
- {cite:ps}`corbae2009`: Chapters 5 and 6 (pp. 172-354).
- {cite:ps}`sundaram1996`: Chapters 2-10 (pp. 74-267).
- {cite:ps}`takayama1985`: Chapters 1 and 2 (pp. 59-294).


**Comparative statics section**

- {cite:ps}`chiang2005`: Chapters 6, 7, 8, 12, and 13.
- {cite:ps}`henderson1958`: Chapter 2 and Appendix A-1.
- {cite:ps}`intriligator1971`: Chapters 7.3 and 7.4.
- {cite:ps}`kreps1990`: Chapter 2 and Appendix 1.
- {cite:ps}`kreps2013`: Chapters 3, 4, 10, and 11, and Appendices 2, 3, 4, 5, and 7.
- {cite:ps}`leonard1992`: Chapter 1.
- {cite:ps}`mas-colell1995`: Chapters 2 and 3, and the Mathematical Appendix.
- {cite:ps}`silberberg2001`: Chapters 1, 3, 5, 6, 7, 10, and 11.
- {cite:ps}`simon1994`: Chapters 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, and 30.
- {cite:ps}`takayama1985`: Chapter 1.
- {cite:ps}`takayama1993`: Chapters 2 and 3, and Appendices A and B.
- {cite:ps}`varian1992`: Chapters 7, 8, and 9.

```





## Types of optimisation problems

### Why do economists care about optimisation?

Economists try to explain social phenomena in terms of the behaviour of an individual who is confronted with scarcity and the interaction of that individual with other individuals who also face scarcity. This is perhaps best captured by Malinvaude's definition of economics:

> *“· · · economics is the science which studies how scarce resources are employed for the satisfaction of the needs of men living in society: on the one hand, it is interested in the essential operations of production, distribution and consumption of goods, and on the other hand, in the institutions and activities whose object
it is to facilitate these operations.” (Italics in original.)*

-- (From page one of Malinvaude, E. (1972), Lectures on microeconomic theory, Advanced Textbooks in Economics Volume 2, North Holland Publishing Company, Scotland, translated by Mrs. A. Silvey.)

Scarcity:
- This is the defining feature of economics.
- It is this feature that distinguishes economics from other social sciences.


The behaviour of an individual who is faced with scarcity:
- Often modelled using "constrained optimisation" techniques.


The interaction of individuals that face scarcity:
- Economic equilibrium (eg competitive equilibrium and Nash equilibrium).
- When does a system of equations have at least one solution?
- How do we find such a solution (if it exists)?
- Use techniques from linear algebra and (for nonlinear cases) fixed point theorems.


According to {cite:ps}`ausubel1993` (p. 99): 
> *“Almost every economic problem involves the study of an agent’s optimal choice as a function of certain parameters or state variables. For example, demand theory is concerned with an agent’s optimal consumption as a function of prices and income, while capital theory studies the optimal investment rule as a function of the existing capital stock.”*


### Maximisation or minimisation?

Optimisation problems come in two broad varieties: maximisation problems and minimisation problems.
- A maximisation problem involves finding a maximum for some function $f: S \longrightarrow \mathbb{R}$, where $S \subseteq \mathbb{R}^{n}$, on some subset of its domain $C \subseteq S$.
- A minimisation problem typically involves finding a minimum for some function $f: S \longrightarrow \mathbb{R}$, where $S \subseteq \mathbb{R}^{n}$, on some subset of its domain $C \subseteq S$.

Some terminology:
- The function $f$ is known as the **objective function**.
- The set $C$ is known as the **constraint set** (or the **feasible set**).
- It turns out that the problem of finding a minimum value for $f$ on the set $C$ is equivalent to the problem of finding a maximum value for $(-f)$ on the set $C$.
- Thus we can restrict our attention to maximisation problems without loss of generality.
- If we are faced with a minimisation problem, we can simply multiply the objective function by $(-1)$ and treat it like a maximisation problem.


### Unconstrained or constrained?

Consider an optimisation problem that involves finding a maximum for some function $f: S \longrightarrow \mathbb{R}$, where $S \subseteq \mathbb{R}^{n}$, on some subset of its domain $C \subseteq S$.
- If $C=S$, then the problem is an unconstrained optimisation problem.
- If $C \subset S$ (that is, if $C \subseteq S$ and $C \neq S$ ), then the problem is a constrained optimisation problem.

In this course, we will only look at unconstrained problems and equality-constrained problems. But please note that inequality-constraints are also possible. These are covered in more advanced courses.


### The existence and properties of a solution

- As we shall see, there are some optimisation problems that have no solutions.
- It would be nice to know that the problem we are trying to solve has a solution before we attempt to solve it.
- In cases where $f$ is a function, a set of conditions that are sufficient to guarantee the existence of a solution to an optimisation problem is provided by Weierstrass' Theorem of the Maximum.
- A set of conditions that are sufficient to guarantee that that the solution set and the set of optimal choice variables in an optimisation problem have some desirable properties is provided by Berge's Theorem of the maximum.
- These two theorems are beyond the scope of this course. However, it is comforting to know that they exist!


### The potential non-existence of a solution

We will illustrate the potential non-existence of a solution by means of some examples. Consider the following optimisation problems:
- Find a maximum for $f(x)=x$ when $x \in(-\infty, \infty)$.
    - No solution exists because $f(x) \rightarrow \infty$ as $x \rightarrow \infty$.
- Find a minimum for $f(x)=x$ when $x \in(-\infty, \infty)$.
    - No solution exists because $f(x) \rightarrow-\infty$ as $x \rightarrow-\infty$.
- Find a maximum for $f(x)=x$ when $x \in(a, b)$ where $-\infty<a<b<\infty$.
    - No solution exists because of the constraint set is open.
    - $f(x) \rightarrow f(b)$ as $x \rightarrow b$, but never quite gets there.
    - What is the largest real number that is strictly less than $b$ ?
- Find a minimum for $f(x)=x$ when $x \in(a, b)$ where $-\infty<a<b<\infty$.
    - No solution exists because of the constraint set is open.
    - $f(x) \rightarrow f(a)$ as $x \rightarrow a$, but never quite gets there.
    - What is the largest real number that is strictly greater than $a$ ?

*Illustrate these examples on the white-board.*


### Solutions and control variables

The solution to a constrained *maximisation* problem is the maximum value of the objective function $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ over the constraint set $C$.  It is denoted by the maximum value function

$$
f^{*}(\cdot) \equiv \max \{f(x): x \in C\} .
$$

The optimal choices of the $x_{i}$ variables are known as "arg max"s:

$$
\begin{aligned}
\left\{x^{*}(\cdot)\right\} & =\left\{\left(\begin{array}{c}
x_{1}^{*}(\cdot) \\
x_{2}^{*}(\cdot) \\
\vdots \\
x_{n}^{*}(\cdot)
\end{array}\right)\right\} \\
& =\arg \max \{f(x): x \in C\} .
\end{aligned}
$$


We can find the maximum value function from the "arg max"s as follows:

$$
f^{*}(\cdot)=f\left(x^{*}\right)
$$

where

$$
x^{*} \in \arg \max \{f(x): x \in C\}
$$

If the "arg max" is unique, then this becomes

$$
f^{*}(\cdot)=f\left(x^{*}\right)
$$

where

$$
x^{*}=\arg \max \{f(x): x \in C\}
$$


The solution to a constrained *minimisation* problem is the minimum value of the objective function $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ over the constraint set $C$.  It is denoted by the minimum value function

$$
f^{*}(\cdot) \equiv \min \{f(x): x \in C\} .
$$

The optimal choices of the $x_{i}$ variables are known as "arg min"s:

$$
\begin{aligned}
\left\{x^{*}(\cdot)\right\} & =\left\{\left(\begin{array}{c}
x_{1}^{*}(\cdot) \\
x_{2}^{*}(\cdot) \\
\vdots \\
x_{n}^{*}(\cdot)
\end{array}\right)\right\} \\
& =\arg \min \{f(x): x \in C\} .
\end{aligned}
$$

We can find the minimum value function from the "arg min"s as follows:

$$
f^{*}(\cdot)=f\left(x^{*}\right)
$$

where

$$
x^{*} \in \arg \min \{f(x): x \in C\}
$$

If the "arg min" is unique, then this becomes

$$
f^{*}(\cdot)=f\left(x^{*}\right)
$$

where

$$
x^{*}=\arg \min \{f(x): x \in C\}
$$


### Global optima and local optima

Let $f: S \longrightarrow \mathbb{R}$ and $C \subseteq S$, where $S \subseteq \mathbb{R}^{n}$.

Global and local maxima:
- $f\left(x^{*}\right)$ is a global maximum of $f$ on $C$ if $x^{*} \in C$ and $f\left(x^{*}\right) \geqslant f(x)$ for all $x \in C$.
- $f\left(x^{*}\right)$ is a local maximum of $f$ on $C$ if $x^{*} \in C$ and $f\left(x^{*}\right) \geqslant f(x)$ for all $x \in C$ that are "sufficiently close to" $x^{*}$.

Global and local minima:
- $f\left(x^{*}\right)$ is a global minimum of $f$ on $C$ if $x^{*} \in C$ and $f\left(x^{*}\right) \leqslant f(x)$ for all $x \in C$.
- $f\left(x^{*}\right)$ is a local minimum of $f$ on $C$ if $x^{*} \in C$ and $f\left(x^{*}\right) \leqslant f(x)$ for all $x \in C$ that are "sufficiently close to" $x^{*}$.


<br>

- Note that every global maximum is a local maximum, but not all local maxima will be a global maximum.
- Note that every global minimum is a local minimum, but not all local minima will be a global minimum.
- *Illustrate this on the white-board.*


## Unconstrained optimisation

Let $f: S \longrightarrow \mathbb{R}$ be a function that is at least twice continuously differentiable and $C=S$, where $S \subseteq \mathbb{R}^{n}$ is an open set. Since $C=S$, this an unconstrained optimisation proiblem.

The point $x^{*} \in S$ will yield a local maximum of $f$ if both of the following sets of conditions are satisfied.
- (First-Order Conditions) (FOC): The gradient vector $D_{x} f$ equals the null vector at the point $x^{*}$. (In other words, $D_{x} f\left(x^{*}\right)=0$.
    - This requires that $\frac{\partial f\left(x^{*}\right)}{\partial x_{i}}=0$ for all $i \in\{1,2, \cdots, n\}$.
- (Second-Order Conditions) (SOC): The Hessian matrix $D_{x x^{T}}^{2} f\left(x^{*}\right)$ is negative definite at the point $x^{*}$

The point $x^{*} \in S$ will yield a local minimum of $f$ if both of the following sets of conditions are satisfied.
- (First-Order Conditions) (FOC): The gradient vector $D_{x} f$ equals the null vector at the point $x^{*}$. (In other words, $D_{x} f\left(x^{*}\right)=0$.
    - This requires that $\frac{\partial f\left(x^{*}\right)}{\partial x_{i}}=0$ for all $i \in\{1,2, \cdots, n\}$.
- (Second-Order Conditions) (SOC): The Hessian matrix $D_{x x^{T}}^{2} f\left(x^{*}\right)$ is positive definite at the point $x^{*}$


### Univariate unconstrained optimisation examples

Work through the following examples on the whiteboard:
- Minimise $f(x)=(x-1)^{2}$ on $\mathbb{R}$. (A minimum of $f=0$ at $x=1$.)
- Maximise $f(x)=-(x-1)^{2}$ on $\mathbb{R}$. (A maximum of $f=0$ at $x=1$.)
- Maximise $f(x)=\left(\frac{1}{3}\right) x^{3}-x+1$ on $\mathbb{R}$. (A "minimum" turning point at $x=1$ and a "maximum" turning point at $x=-1$. A local maximum of $f=\frac{5}{3}$ at $x=-1$.)
- Minimise $f(x)=\left(\frac{1}{3}\right) x^{3}-x+1$ on $\mathbb{R}$. (A "minimum" turning point at $x=1$ and a "maximum" turning point at $x=-1$. A local minimum of $f=\frac{1}{3}$ at $x=1$.)


### A multivariate unconstrained optimisation example

Consider the function $f: \mathbb{R}^{2} \longrightarrow \mathbb{R}$ defined by $f(x, y)=-x^{2}-y^{2}$.

- The first-order conditions for either a maximum or a minimum are:
- $f_{x}(x, y)=-2 x=0 \Longrightarrow x^{*}=0$; and
- $f_{y}(x, y)=-2 y=0 \Longrightarrow y^{*}=0$.
- Thus $\left(x^{*}, y^{*}\right)=(0,0)$ is the unique critical point for $f(x, y)$.
- The Hessian matrix for $f(x, y)$ is

$$
H=\left(\begin{array}{ll}
f_{x x}(x, y) & f_{x y}(x, y) \\
f_{y x}(x, y) & f_{y y}(x, y)
\end{array}\right)=\left(\begin{array}{cc}
-2 & 0 \\
0 & -2
\end{array}\right)
$$

Note that:
- $H$ is a symmetric matrix,
- $\operatorname{det}\left(H_{1}\right)=-2<0$, and
- $\operatorname{det}\left(H_{2}\right)=\operatorname{det}(H)=4>0$.

Thus $H$ is negative definite for all $(x, y) \in \mathbb{R}^{2}$. 
This means that $\left(x^{*}, y^{*}\right)=(0,0)$ yields a global maximum of $f(x, y)$. 
The maximum value of $f(x, y)$ is $f^{*}=f\left(x^{*}, y^{*}\right)=f(0,0)=0$.


### Univariate profit maximisation by a price-taking firm

Suppose that a single-product price-taking firm faces an output price of $P$ per unit and has a (total) cost function that is given by $C(Q)$. We will assume that the cost function is at least twice continuously differentiable.

The firm's profits are given by the function $\Pi(Q)=P Q-C(Q)$.
Suppose that this firm wants to choose $Q$ to maximise its profits.
We will ignore the shut-down condition as a matter of convenience.


The first-order condition for profit maximisation is

$$
\frac{d \Pi}{d Q}=P-\frac{d C}{d Q}=0
$$

This can be rearranged to obtain $P=M C(Q)$, where $M C(Q)=\frac{d C}{d Q}$.
- This first-order condition implicitly defines one or more critical values of $Q$ for the function $\Pi(Q)$.

uppose that $Q^{*}$ is a critical value of $Q$ for the function $\Pi(Q)$. The second-order condition for a local maximum requires that

$$
\frac{d^{2} \Pi}{d Q^{2}}=-\frac{d^{2} C}{d Q^{2}}=-\frac{d M C}{d Q}<0
$$

when it is evaluated at the point $Q^{*}$.

Note that this requires that $\frac{d M C}{d Q}>0$ when it is evaluated at the point $Q^{*}$.
In other words, we need marginal cost to be an increasing function of the output quantity in a non-empty neighbourhood around the point $Q^{*}$.


### Bivariate profit maximisation by a price-taking firm

Suppose that a two-product price-taking firm faces output prices of $P_{1}$ per unit for one of its products and and $P_{1}$ per unit for its other product.
Suppose also that this firm has a (total) cost function that is given by $C\left(Q_{1}, Q_{2}\right)$, where $Q_{1}$ is its output of good one and $Q_{2}$ is its output of good two.
We will assume that the cost function is at least twice continuously differentiable in all of its arguments.

The firm's profits are given by the function $\Pi\left(Q_{1}, Q_{2}\right)=P_{1} Q_{1}+P_{2} Q_{2}-C\left(Q_{1}, Q_{2}\right)$.
Suppose that this firm wants to choose $Q_{1}$ and $Q_{2}$ to maximise its profits.
We will ignore the shut-down condition as a matter of convenience.


The first-order conditions for profit maximisation by the firm are

$$
\frac{\partial \Pi}{\partial Q_{1}}=P_{1}-\frac{\partial C}{\partial Q_{1}}=0
$$

and

$$
\frac{\partial \Pi}{\partial Q_{2}}=P_{2}-\frac{\partial C}{\partial Q_{2}}=0
$$

These two conditions can be rearranged to obtain

$$
P_{1}=M C_{1}\left(Q_{1}, Q_{2}\right)
$$

and

$$
P_{2}=M C_{2}\left(Q_{1}, Q_{2}\right)
$$

where $M C_{i}\left(Q_{1}, Q_{2}\right)=\frac{\partial C}{\partial Q_{i}}$ for $i \in\{1,2\}$.

The two first-order conditions for this bivariate profit maximisation problem jointly define one or more critical points for the function $\Pi\left(Q_{1}, Q_{2}\right)$.
Let $Q^{*}=\left(Q_{1}^{*}, Q_{2}^{*}\right)$ be one such point.

The second-order condition for $Q^{*}$ to yield a local maximum of the function $\Pi\left(Q_{1}, Q_{2}\right)$ requires that the Hessian matrix for $\Pi\left(Q_{1}, Q_{2}\right)$ be negative definite when it is evaluated at the point $Q^{*}=\left(Q_{1}^{*}, Q_{2}^{*}\right)$.


The Hessian matrix for the function $\Pi\left(Q_{1}, Q_{2}\right)$ is

$$
\begin{gathered}
H=D^{2} \Pi\left(Q_{1}, Q_{2}\right) \\
=\left(\begin{array}{cc}
\frac{\partial^{2} \Pi}{\partial Q_{1}^{2}} & \frac{\partial^{2} \Pi}{\partial Q_{2} \partial Q_{1}} \\
\frac{\partial^{2} \Pi}{\partial Q_{1} \partial Q_{2}} & \frac{\partial^{2} \Pi}{\partial Q_{2}^{2}}
\end{array}\right) \\
=\left(\begin{array}{cc}
-\frac{\partial^{2} C}{\partial Q_{1}^{2}} & -\frac{\partial^{2} C}{\partial Q_{2} \partial Q_{1}} \\
-\frac{\partial^{2} C}{\partial Q_{1} \partial Q_{2}} & -\frac{\partial^{2} C}{\partial Q_{2}^{2}}
\end{array}\right) \\
=\left(\begin{array}{cc}
-\frac{\partial M C_{1}}{\partial Q_{1}} & -\frac{\partial M C_{1}}{\partial Q_{2}} \\
-\frac{\partial M C_{2}}{\partial Q_{1}} & -\frac{\partial M C_{2}}{\partial Q_{2}}
\end{array}\right) .
\end{gathered}
$$


A sufficient condition for the critical point $Q^{*}=\left(Q_{1}^{*}, Q_{2}^{*}\right)$ to yield a local maximum of the function $\Pi\left(Q_{1}, Q_{2}\right)$ is that the Hessian matrix $(H)$ be negative definite at the point $Q^{*}=\left(Q_{1}^{*}, Q_{2}^{*}\right)$

Assuming that the function $\Pi\left(Q_{1}, Q_{2}\right)$ is at least twice continuously differentiable in a non-empty neighbourhood around the point $Q^{*}=\left(Q_{1}^{*}, Q_{2}^{*}\right)$ (so that Young's theorem applies and the Hessian matrix is symmetric at the critical point being considered), this requires that:

(1) $\operatorname{det}\left(H_{1}\right)=-\frac{\partial M C_{1}}{\partial Q_{1}}<0$ at $Q^{*}=\left(Q_{1}^{*}, Q_{2}^{*}\right)$, and

(2) $\operatorname{det}\left(H_{2}\right)=\operatorname{det}(H)=\left(\frac{\partial M C_{1}}{\partial Q_{1}}\right)\left(\frac{\partial M C_{2}}{\partial Q_{2}}\right)-\left(\frac{\partial M C_{1}}{\partial Q_{2}}\right)\left(\frac{\partial M C_{2}}{\partial Q_{1}}\right)=$

$$
\left(\frac{\partial M C_{1}}{\partial Q_{1}}\right)\left(\frac{\partial M C_{2}}{\partial Q_{2}}\right)-\left(\frac{\partial M C_{1}}{\partial Q_{2}}\right)^{2}>0 \text { at } Q^{*}=\left(Q_{1}^{*}, Q_{2}^{*}\right) \text {. }
$$

This in turn requires that:

(1) $\frac{\partial M C_{1}}{\partial Q_{1}}>0$ at $Q^{*}=\left(Q_{1}^{*}, Q_{2}^{*}\right)$,

(2) $\frac{\partial M C_{2}}{\partial Q_{2}}>0$ at $Q^{*}=\left(Q_{1}^{*}, Q_{2}^{*}\right)$, and

(3) $\left(\frac{\partial M C_{1}}{\partial Q_{1}}\right)\left(\frac{\partial M C_{2}}{\partial Q_{2}}\right)>\left(\frac{\partial M C_{1}}{\partial Q_{2}}\right)^{2}$ at $Q^{*}=\left(Q_{1}^{*}, Q_{2}^{*}\right)$.



## Equality constrained optimisation

Let $f: S \longrightarrow \mathbb{R}$ be a function that is at least twice continuously differentiable and $C \subseteq S$, where $S \subseteq \mathbb{R}^{n}$ is an open set. Suppose that the constraint set $C$ consists only of equality restrictions. In other words, suppose that

$$
C=\left\{x \in \mathbb{R}^{n}: g^{i}(x)=b^{i} \text { for all } i \in\{1,2, \cdots, m\}\right\}
$$

where $b^{i} \in \mathbb{R}$ for $i \in\{1,2, \cdots, m\}$ are all constant terms.

Consider the problem of maximising $f$ on $C$.  We can resrict our attention to a constrained maximisation problem without loss of generality. The reason for this is that minimising $f$ on $C$ is equivalent to maximising $(-f)$ on $C$. Note that a necessary condition for there to be a solution to a constrained optimisation problem is that the constraint set be non-empty.


### The Lagrangian function

Let $S \subseteq \mathbb{R}^{n}$ be an open set, $f: S \longrightarrow \mathbb{R}$ be a function that is at least twice continuously differentiable, and

$$
C=\left\{x \in \mathbb{R}^{n}: g^{i}(x)=b^{i} \text { for all } i \in\{1,2, \cdots, m\}\right\} \text {, }
$$

be a constraint set.

Consider the problem of maximising $f$ on $C$.

The **Lagrangian function** for this constrained maximisation problem is

$$
\mathcal{L}(x, \lambda)=f(x)+\sum_{i=1}^{m} \lambda_{i}\left[b^{i}-g^{i}(x)\right]
$$

The $\lambda_{i}$ variables are known as **Lagrange multipliers**.


### Equality constrained optimisation solutions

Theorem: $x^{*} \in C$ yields a local maximum of $f$ if the following three sets of conditions are satisfied.
- (FOC Part 1): $D_{x} \mathcal{L}\left(x^{*}, \lambda^{*}\right)=0$.
- (FOC Part 2): $D_{\lambda} \mathcal{L}\left(x^{*}, \lambda^{*}\right)=0$.
- An appropriate rank condition and appropriate second-order conditions are satisfied. (These are beyond the scope of this course.)


### The first-order conditions for equality-constrained maximisation problems

Recall that the Lagrangian function for the constrained maximisation problem under consideration is

$$
\mathcal{L}(x, \lambda)=f(x)+\sum_{i=1}^{m} \lambda_{i}\left[b^{i}-g^{i}(x)\right]
$$

The first-order conditions for this problem are:
- $\frac{\partial \mathcal{L}}{\partial x_{k}}=\frac{\partial f}{\partial x_{k}}+\sum_{i=1}^{m}\left(-\lambda_{i} \frac{\partial g}{\partial x_{k}}\right)=0$ for each $k \in\{1,2, \cdots, n\}$, and
- $\frac{\partial \mathcal{L}}{\partial \lambda_{j}}=b^{j}-g^{j}(x)=0$ for each $j \in\{1,2, \cdots, m\}$.


### The second-order conditions for equality-constrained maximisation problems

It is possible to express the second-order conditions for this problem in terms of something known as a bordered Hessian matrix. However, this is beyond the scope of this course. We will instead look at converting equality-constrained optimisation problems into unconstrained optimisation problems in fewer variables by using the constraints to express some variables in terms of others. This will allow us to use the second-order conditions for uncontrained optimisation problems.


### Interpreting Lagrange multipliers

The optimal value of the Lagrange multiplier for the $i$th constraint (that is, the value of that Lagrange multiplier when all of the first-order conditions are satisfied) tells us the degree of sensitivity of the optimal value of the objective function over the constraint set, $f^{*}\left(x^{*}, b\right)$, to a change in the $i$ th constraint value $b^{i}$, where $b=\left\{b^{1}, b^{2}, \cdots, b^{m}\right\}$ is the vector of constraint values.

- It is, in effect, a "shadow price".
- The optimal value of the Lagrange multiplier in a standard utility maximisation problem can be interpreted as the marginal utility of income.
- The optimal value of the Lagrange multiplier in a cost minimisation problem can be interpreted as marginal cost.


### Some economic applications

- Budget-contrained utility maximisations problems
- Utlity-constrained expenditure minimisation problems
- Output-constrained cost minimisation problems
- Technology-constrained profit maximisation problems
- Pareto efficiency problems


### Equality constrained optimisation examples

Work through the following examples on the board or from other notes.
- Maximise $f(x, y)=3 x-y+6$ subject to the constraint that $x^{2}+y^{2}=4$, where $(x, y) \in \mathbb{R}^{2}$.
- Solve some UMP examples.
- Solve a cost-minimisation problem for a particular two-input version of a Cobb-Douglas production function and show that the optimal value of the Lagrange multiplier on the "minimum output" constraint is equal to marginal cost.


### Converting equality constrained optimisation problems into unconstrained optimisation problems

Consider an equality constrained optimisation problem of the form

$$
\begin{gathered}
\operatorname{Max} f\left(x_{1}, x_{2}, \cdots, x_{n}\right) \text { subject to } \\
g^{1}\left(x_{1}, x_{2}, \cdots, x_{n}\right)=b_{1}, \\
g^{2}\left(x_{1}, x_{2}, \cdots, x_{n}\right)=b_{2}, \\
\vdots \\
g^{m}\left(x_{1}, x_{2}, \cdots, x_{n}\right)=b_{m} .
\end{gathered}
$$

The constraint set for this problem is $C=$

$$
\left\{\left(x_{1}, x_{2}, \cdots, x_{n}\right): g^{j}\left(x_{1}, x_{2}, \cdots, x_{n}\right)=b_{j} \text { for all } j \in\{1,2, \cdots, m\}\right\}
$$

Assume that there are fewer constraints than there are choice variables $(m<n)$ and that the constraint set is nonempty $(C \neq \varnothing)$.

Since $m<n$ and $C \neq \varnothing$, we know that if the constraint equations are all linear and the domain of each choice variable is an infinite subset of $\mathbb{R}$, then there will be an infinite number of solutions for the system of equations given by

$$
\begin{gathered}
g^{1}\left(x_{1}, x_{2}, \cdots, x_{n}\right)=b_{1} \\
g^{2}\left(x_{1}, x_{2}, \cdots, x_{n}\right)=b_{2} \\
\vdots \\
g^{m}\left(x_{1}, x_{2}, \cdots, x_{n}\right)=b_{m}
\end{gathered}
$$

These solutions would be parametric. They would pin down the value of $m$ of the choice variables in terms of the other $(n-m)$ choice variables and the constraint values. However, each of the remaining $(n-m)$ choice variables would be free to vary over their entire domain.


Suppose that this is the case even when the constraint equations are nonlinear. (This may impose some restrictions on the constraint equations. Think about the implicit function theorem and the inverse function theorem, for example.)
In this case, we can use the system of constraint equations to express $m$ of the choice variables as implicit functions of the remaining $(n-m)$ choice variables and the $m$ constraint values.

These implicit functions might take the form

$$
\begin{gathered}
x_{n-m+1}=h^{n-m+1}\left(x_{1}, x_{2}, \cdots, x_{n-m}, b_{1}, b_{2}, \cdots, b_{m}\right) \\
x_{n-m+2}=h^{n-m+2}\left(x_{1}, x_{2}, \cdots, x_{n-m}, b_{1}, b_{2}, \cdots, b_{m}\right) \\
\vdots \\
x_{n}=h^{n}\left(x_{1}, x_{2}, \cdots, x_{n-m}, b_{1}, b_{2}, \cdots, b_{m}\right) .
\end{gathered}
$$

Having used the system of constraint equations to express $m$ of the choice variables as implicit functions of the remaining $(n-m)$ choice variables and the $m$ constraint values, we can now substitute these implicit functions back into the objective function to obtain the following "reduced-form" unconstrained optimisation problem:

$$
\operatorname{Max} f\left(x_{1}, x_{2}, \cdots, x_{n-m}, h^{n-m+1}(\cdot), h^{n-m+2}(\cdot), \cdots, h^{n}(\cdot)\right) .
$$

Note that there are only $(n-m)$ choice variables in this "reduced-form" unconstrained optimisation problem.

But each of these variables potentially enters the objective function multiple times; once directly and up to $m$ times indirectly. The indirect entries come through the implicit functions that solve the set of constraint equations (in other words, the $h$ functions).


The technique for solving unconstrained optimisation problems that we have already covered can now be applied to this 'reduced-form" unconstrained optimisation problem in order to solve the oringinal equality-constrained optimisation problem.

Once optimal values for the $(n-m)$ choice variables in this "reduced-form" unconstrained optimisation problem have been obtained, the optimal values of remaining $m$ choice variables from the original equality-constrained optimisation problem can be obtained by substituting them into the $h$ functions. This will give us the optimal choice values for all $n$ of the choice variables.

The solution to the equality-constrained optimisation problem can then be found by substituting the optimal values for all $n$ of the choice variables into the objective function.



### Conversion example

Consider the following UMP:

$$
\begin{gathered}
\operatorname{Max} U\left(x_{1}, x_{2}, x_{3}\right)=\alpha \ln \left(x_{1}\right)+\beta \ln \left(x_{2}\right)+(1-\alpha-\beta) \ln \left(x_{3}\right) \\
\text { subject to } p_{1} x_{1}+p_{2} x_{2}+p_{3} x_{3}=y,
\end{gathered}
$$

where $0<\alpha<1,0<\beta<1$, and $\alpha+\beta<1$.

We will assume throughout that $x_{1}>0, x_{2}>0$, and $x_{3}>0$.

We can use the budget constraint to express $x_{3}$ as a function of $x_{1}$ and $x_{2}$ as follows:

$$
x_{3}=\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} x_{1}-\frac{p_{2}}{p_{3}} x_{2}
$$

Upon substituting this into the utility function, we obtain the following unconstrained optimisation problem:

$$
\begin{gathered}
\operatorname{Max} \hat{U}\left(x_{1}, x_{2}\right)= \\
\alpha \ln \left(x_{1}\right)+\beta \ln \left(x_{2}\right)+(1-\alpha-\beta) \ln \left(\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} x_{1}-\frac{p_{2}}{p_{3}} x_{2}\right) .
\end{gathered}
$$


The first-order conditions for this problem are

$$
\frac{\partial \hat{U}}{\partial x_{1}}=\frac{\alpha}{x_{1}}-\frac{(1-\alpha-\beta) \frac{p_{1}}{p_{3}}}{\left(\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} x_{1}-\frac{p_{2}}{p_{3}} x_{2}\right)}=0
$$

and

$$
\frac{\partial \hat{U}}{\partial x_{2}}=\frac{\beta}{x_{2}}-\frac{(1-\alpha-\beta) \frac{p_{2}}{p_{3}}}{\left(\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} x_{1}-\frac{p_{2}}{p_{3}} x_{2}\right)}=0
$$

These can be rearranged and simplified to obtain

$$
(1-\alpha-\beta) p_{1} x_{1}=\alpha\left(y-p_{1} x_{1}-p_{2} x_{2}\right),
$$

and

$$
(1-\alpha-\beta) p_{2} x_{2}=\beta\left(y-p_{1} x_{1}-p_{2} x_{2}\right) .
$$


These can be further rearranged and simplified to obtain the following system of two equations in two unknowns:

$$
(1-\beta) p_{1} x_{1}+\alpha p_{2} x_{2}=\alpha y
$$

and

$$
\beta p_{1} x_{1}+(1-\alpha) p_{2} x_{2}=\beta y .
$$

It is probably easiest to solve this system of equations by using expenditure on each commodity as the variables. Upon doing this, we obtain $p_{1} x_{1}=\alpha y$ and $p_{2} x_{2}=\beta y$, so that

$$
x_{1}=\frac{\alpha y}{p_{1}} \text { and } x_{2}=\frac{\beta y}{p_{2}}
$$

This means that

$$
x_{3}=\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} \frac{\alpha y}{p_{1}}-\frac{p_{2}}{p_{3}} \frac{\beta y}{p_{2}}=\frac{(1-\alpha-\beta) y}{p_{3}} .
$$


Thus our candidate solution is

$$
\begin{gathered}
V\left(p_{1}, p_{2}, p_{3}, y\right)=U^{*} \\
=U\left(\frac{\alpha y}{p_{1}}, \frac{\beta y}{p_{2}}, \frac{(1-\alpha-\beta) y}{p_{3}}\right) \\
=\alpha \ln \left(\frac{\alpha y}{p_{1}}\right)+\beta \ln \left(\frac{\beta y}{p_{2}}\right)+(1-\alpha-\beta) \ln \left(\frac{(1-\alpha-\beta) y}{p_{3}}\right) \\
=\ln \left(\left(\frac{\alpha y}{p_{1}}\right)^{\alpha}\left(\frac{\beta y}{p_{2}}\right)^{\beta}\left(\frac{(1-\alpha-\beta) y}{p_{3}}\right)^{(1-\alpha-\beta)}\right) .
\end{gathered}
$$

What about second-order conditions?  We need to examine the definiteness (or otherwise) of the Hessian matrix for the utility function from the "reduced-form" unconstrained optimisation problem.


Recall that

$$
\hat{U}_{1}=\frac{\alpha}{x_{1}}-\frac{(1-\alpha-\beta) \frac{p_{1}}{p_{3}}}{\left(\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} x_{1}-\frac{p_{2}}{p_{3}} x_{2}\right)}=0
$$

and

$$
\hat{U}_{2}=\frac{\beta}{x_{2}}-\frac{(1-\alpha-\beta) \frac{p_{2}}{p_{3}}}{\left(\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} x_{1}-\frac{p_{2}}{p_{3}} x_{2}\right)}=0 .
$$

Thus we have

$$
\begin{gathered}
\hat{U}_{11}=-\frac{\alpha}{x_{1}^{2}}-\frac{(1-\alpha-\beta)\left(\frac{p_{1}}{p_{3}}\right)^{2}}{\left(\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} x_{1}-\frac{p_{2}}{p_{3}} x_{2}\right)^{2}}<0 \text { for all } x_{1} \text { and } x_{2}, \\
\hat{U}_{12}=\hat{U}_{21}=-\frac{(1-\alpha-\beta)\left(\frac{p_{1} p_{2}}{p_{3}^{2}}\right)}{\left(\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} x_{1}-\frac{p_{2}}{p_{3}} x_{2}\right)^{2}}, \text { and } \\
\hat{U}_{22}=-\frac{\beta}{x_{2}^{2}}-\frac{(1-\alpha-\beta)\left(\frac{p_{2}}{p_{3}}\right)^{2}}{\left(\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} x_{1}-\frac{p_{2}}{p_{3}} x_{2}\right)^{2}}
\end{gathered}
$$

Recall that

$$
H=\left(\begin{array}{ll}
\hat{U}_{11} & \hat{U}_{12} \\
\hat{U}_{21} & \hat{U}_{22}
\end{array}\right)
$$

Note that

$$
\begin{gathered}
\operatorname{det}\left(H_{1}\right)=\operatorname{det}\left(\hat{U}_{11}\right)=\hat{U}_{11} \\
=-\frac{\alpha}{x_{1}^{2}}-\frac{(1-\alpha-\beta)\left(\frac{p_{1}}{p_{3}}\right)^{2}}{\left(\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} x_{1}-\frac{p_{2}}{p_{3}} x_{2}\right)^{2}}<0 \text { for all } x_{1} \text { and } x_{2}, \text { and }\\
\operatorname{det}\left(H_{2}\right)=\operatorname{det}(H)=\hat{U}_{11} \hat{U}_{22}-\hat{U}_{12} \hat{U}_{21} .
\end{gathered}
$$

After some algebra and simplification, this becomes:

$$
\begin{gathered}
\operatorname{det}\left(H_{2}\right)=\frac{\alpha \beta}{x_{1}^{2} x_{2}^{2}}+\frac{(1-\alpha-\beta)\left(\frac{p_{2}}{p_{3}}\right)^{2} \alpha x_{1}^{2}}{\left(\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} x_{1}-\frac{p_{2}}{p_{3}} x_{2}\right)^{2}}+\frac{(1-\alpha-\beta)\left(\frac{p_{1}}{p_{3}}\right)^{2} \beta x_{2}^{2}}{\left(\frac{y}{p_{3}}-\frac{p_{1}}{p_{3}} x_{1}-\frac{p_{2}}{p_{3}} x_{2}\right)^{2}} \\
>0 \text { for all } x_{1} \text { and } x_{2} .
\end{gathered}
$$

This means that the Hessian matrix is negative definite for all combinations of $\left(x_{1}>0\right.$ and $\left.x_{2}>0\right)$. Thus we can conclude that our candidate solution is a global maximum for the optimisation problem under consideration.







<br><br><br>



# A further comment on comparative statics 








## A very useful approach

In this set of lecture notes, we will explore a very useful approach to comparative static analysis. This approach starts with a system of simultaneous equations that jointly characterise some economic phenomena of interest. The solution to this system of equations will be a set of functions (or possibly correspondences) that express the outcome values taken by various economic choice variables as functions of the values taken by the economic parameters in the system.

It is sometimes convenient to be able to obtain expressions for the impact of a change in one of the economic parameters, holding all of the other economic parameters constant, on one or more of the economic choice variables, without explicitly finding the functions (or correspondences) that are the solution to the original system of simultaneous equations. This can sometimes be achieved by the following process.


First, write each of the $n \in \mathbb{N}$ equations in the initial simultaneous equations system in the form

$$
f^{i}\left(x_{1}, x_{2}, \cdots, x_{n} ; \alpha_{1}, \alpha_{2}, \cdots, \alpha_{m}\right)=0
$$

where

- the $i$ superscript denotes a particular equation in this $n$ equation system, with $i \in\{1,2, \cdots, n\}$;
- the economic choice variables are $x_{1}, x_{2}, \cdots$, and $x_{n}$; and
- the economic parameters are $\alpha_{1}, \alpha_{2}, \cdots$, and $\alpha_{m}$.

Second, calculate the total differential for each of the $f^{i}(\cdot)$ functions on the left-hand side of the (possibly rewritten) system of simultaneous equations.

The total differential of a function is the linear, or first-order, differential approximation of the change in the value that is taken by the function that is induced by changes in the economic variables and economic parameters that make up the arguments of the function.  The total differential for $f^{i}(\cdot)$ is given by

$$
d f^{i}(\cdot) \approx \sum_{j=1}^{n}\left(\frac{\partial f^{i}(\cdot)}{\partial x_{j}}\right) d x_{j}+\sum_{k=1}^{m}\left(\frac{\partial f^{i}(\cdot)}{\partial \alpha_{k}}\right) d \alpha_{k}
$$

where $d f^{i}(\cdot)$ is the change in the value of the $f^{i}(\cdot)$ function, $d x_{j}$ is the change in the value of the $x_{j}$ variable, and $d \alpha_{k}$ is the change in the value of the $\alpha_{k}$ parameter.

If the changes in the economic variables and the economic parameters that induce the changes in the $f^{i}(\cdot)$ functions are sufficiently small, then this approximation should be reasonably good.

Third, note that, since the economic phenomenon in which we are interested requires that

$$
f^{i}\left(x_{1}, x_{2}, \cdots, x_{n} ; \alpha_{1}, \alpha_{2}, \cdots, \alpha_{m}\right)=0
$$

for all $i \in\{1,2, \cdots, n\}$, any combination of changes in the economic parameters of the system that preserves the phenomenon of interest must not move the value of the $f^{i}(\cdot)$ functions away from zero (that is, it must not induce any changes in the values that are taken by the $f^{i}(\cdot)$ functions).

In other words, we require that $d f^{i}(\cdot)=0$ for all $i \in\{1,2, \cdots, n\}$, which is approximately equivalent to a requirement that

$$
\sum_{j=1}^{n}\left(\frac{\partial f^{i}(\cdot)}{\partial x_{j}}\right) d x_{j}+\sum_{k=1}^{m}\left(\frac{\partial f^{i}(\cdot)}{\partial \alpha_{k}}\right) d \alpha_{k}=0
$$

for all $i \in\{1,2, \cdots, n\}$, when the changes in the economic variables and economic parameters are sufficiently small.


Fourth, rearrange each of the equations in the system of simultaneous "total differential equations" that have just been obtained so that all of the terms involving changes in the economic variables appear on the left-hand-side of the equations, and all of the terms involving changed in the economic parameters appear on the right-hand-side of the equations.

This yields a system of simultaneous equations of the form

$$
\sum_{j=1}^{n}\left(\frac{\partial f^{i}(\cdot)}{\partial x_{j}}\right) d x_{j}=-\left(\sum_{k=1}^{m}\left(\frac{\partial f^{i}(\cdot)}{\partial \alpha_{k}}\right) d \alpha_{k}\right)
$$

for all $i \in\{1,2, \cdots, n\}$.


Fifth, note that in comparative statics exercises, we typically focus on the impact of a change in only one of the economic parameters on the economic phenomenon of interest, while holding all of the other economic parameters constant.

Suppose that we want to analyse the impact of a change in the $\alpha_{s}$ parameter alone. In this case, we should assume that $d \alpha_{s} \neq 0$, and set $d \alpha_{k}=0$ for all $k \neq s$, in each of the equations in the system of simultaneous equations that we obtained in step four.

Upon doing this, we obtain a system of simultaneous equations of the form

$$
\sum_{j=1}^{n}\left(\frac{\partial f^{i}(\cdot)}{\partial x_{j}}\right) d x_{j}=-\left(\frac{\partial f^{i}(\cdot)}{\partial \alpha_{s}}\right) d \alpha_{s}
$$

for all $i \in\{1,2, \cdots, n\}$.


Sixth, note that we can divide both sides of each of the equations in in the system of simultaneous equations that we obtained in step five by $d \alpha_{s}$, because $d \alpha_{s} \neq 0$.

Upon doing this, we obtain a system of simultaneous equations of the form

$$
\sum_{j=1}^{n}\left(\frac{\partial f^{i}(\cdot)}{\partial x_{j}}\right)\left(\frac{d x_{j}}{d \alpha_{s}}\right)=-\left(\frac{\partial f^{i}(\cdot)}{\partial \alpha_{s}}\right)
$$

for all $i \in\{1,2, \cdots, n\}$.

Seventh, note that

$$
\lim _{d \alpha_{s} \longrightarrow 0}\left(\left.\frac{d x_{j}}{d \alpha_{s}}\right|_{d \alpha_{k}=0 \quad \forall \quad k \neq 0}\right)=\frac{\partial x_{j}}{\partial \alpha_{s}}
$$

for all $i \in\{1,2, \cdots, n\}$, where $\frac{\partial x_{j}}{\partial \alpha_{s}}$ is the partial derivative of the implicitly defined function $x_{j}\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{m}\right)$ with respect to $\alpha_{s}$.


Eighth, note that if we take the limit as $d \alpha_{s} \longrightarrow 0$ of both sides of each of the equations in in the system of simultaneous equations that we obtained in step six, we obtain

$$
\begin{aligned}
& \lim _{d \alpha_{s} \longrightarrow 0}\left\{\sum_{j=1}^{n}\left(\frac{\partial f^{i}(\cdot)}{\partial x_{j}}\right)\left(\frac{d x_{j}}{d \alpha_{s}}\right)\right\}=\lim _{d \alpha_{s} \longrightarrow 0}\left\{-\left(\frac{\partial f^{i}(\cdot)}{\partial \alpha_{s}}\right)\right\} \\
\Longleftrightarrow & \sum_{j=1}^{n}\left(\frac{\partial f^{i}(\cdot)}{\partial x_{j}}\right)\left\{\lim _{d \alpha_{s} \longrightarrow 0}\left(\left.\frac{d x_{j}}{d \alpha_{s}}\right|_{d \alpha_{k}=0 \forall k \neq 0}\right)\right\}=-\left(\frac{\partial f^{i}(\cdot)}{\partial \alpha_{s}}\right) \\
\Longleftrightarrow & \sum_{j=1}^{n}\left(\frac{\partial f^{i}(\cdot)}{\partial x_{j}}\right)\left(\frac{\partial x_{j}}{\partial \alpha_{s}}\right)=-\left(\frac{\partial f^{i}(\cdot)}{\partial \alpha_{s}}\right)
\end{aligned}
$$

for all $i \in\{1,2, \cdots, n\}$.


Ninth, note that the system of simultaneous equations that we obtained in step eight can be expressed in the form of a matrix equation as

$$
\left(\begin{array}{cccc}
\frac{\partial f^{1}(\cdot)}{\partial x_{1}} & \frac{\partial f^{1}(\cdot)}{\partial x_{2}} & \cdots & \frac{\partial f^{1}(\cdot)}{\partial x_{n}} \\
\frac{\partial f^{2}(\cdot)}{\partial x_{1}} & \frac{\partial f^{2}(\cdot)}{\partial x_{2}} & \cdots & \frac{\partial f^{2}(\cdot)}{\partial x_{n}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f^{n}(\cdot)}{\partial x_{1}} & \frac{\partial f^{n}(\cdot)}{\partial x_{2}} & \cdots & \frac{\partial f^{n}(\cdot)}{\partial x_{n}}
\end{array}\right)\left(\begin{array}{c}
\frac{\partial x_{1}}{\partial \alpha_{s}} \\
\frac{\partial x_{2}}{\partial \alpha_{s}} \\
\vdots \\
\frac{\partial x_{n}}{\partial \alpha_{s}}
\end{array}\right)=\left(\begin{array}{c}
-\left(\frac{\partial f^{1}(\cdot)}{\partial \alpha_{s}}\right) \\
-\left(\frac{\partial f^{2}(\cdot)}{\partial \alpha_{s}}\right) \\
\vdots \\
-\left(\frac{\partial f^{n}(\cdot)}{\partial \alpha_{s}}\right)
\end{array}\right) .
$$


Tenth, note that if

$$
\operatorname{det}\left\{\left(\begin{array}{cccc}
\frac{\partial f^{1}(\cdot)}{\partial x_{1}} & \frac{\partial f^{1}(\cdot)}{\partial x_{2}} & \cdots & \frac{\partial f^{1}(\cdot)}{\partial x_{n}} \\
\frac{\partial f^{2}(\cdot)}{\partial x_{1}} & \frac{\partial f^{2}(\cdot)}{\partial x_{2}} & \cdots & \frac{\partial f^{2}(\cdot)}{\partial x_{n}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f^{n}(\cdot)}{\partial x_{1}} & \frac{\partial f^{n}(\cdot)}{\partial x_{2}} & \cdots & \frac{\partial f^{n}(\cdot)}{\partial x_{n}}
\end{array}\right)\right\} \neq 0,
$$

so that the the coefficient matrix on the left-hand-side of the matrix equation that was obtained in step nine is non-singular (that is, it is invertible), then we can solve that matrix equation to obtain expressions for the $n$ comparative static effects in the vector

$$
\left(\begin{array}{llll}
\frac{\partial x_{1}}{\partial \alpha_{s}} & \frac{\partial x_{2}}{\partial \alpha_{s}} & \cdots & \frac{\partial x_{n}}{\partial \alpha_{s}}
\end{array}\right)^{T}
$$



### The budget-constrained utility maximisation problem

We will illustrate this process for a simple two commodity version of an individual's budget-constrained utility maximisation problem.

The individual's problem is to choose $x_{1} \geqslant 0$ and $x_{2} \geqslant 0$ to maximise $U\left(x_{1}, x_{2}\right)$, subject to $p_{1} x_{1}+p_{2} x_{2} \leqslant y$.

We will assume that the individual's preferences are sufficiently well behaved to ensure that he or she will optimally choose to both consume strictly positive amounts of both commodities and exhaust his or her budget.
The Lagrangian function for this constrained optimisation problem is

$$
\mathcal{L}\left(x_{1}, x_{2}, \lambda ; p_{1}, p_{2}, y\right)=U\left(x_{1}, x_{2}\right)+\lambda\left[y-p_{1} x_{1}-p_{2} x_{2}\right] .
$$


The first-order conditions that characterise any critical points of the Lagrangian function for this constrained maximisation problem are

$$
\left\{
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial x_1} &= \frac{\partial \mathcal{U}}{\partial x_1} - \lambda p_1 &= 0, \\
\frac{\partial \mathcal{L}}{\partial x_2} &= \frac{\partial \mathcal{U}}{\partial x_2} - \lambda p_2 &= 0, \\
\frac{\partial \mathcal{L}}{\partial \lambda} &= y - p_1 x_1 - p_2 x_2 &= 0.
\end{aligned}
\right\}
$$


In order to simplify the notation somewhat, let $\mathcal{L}_{1}=\frac{\partial \mathcal{L}}{\partial x_{1}}, \mathcal{L}_{2}=\frac{\partial \mathcal{L}}{\partial x_{2}}$, $\mathcal{L}_{\lambda}=\frac{\partial \mathcal{L}}{\partial \lambda}, U_{1}=\frac{\partial U}{\partial x_{1}}$, and $U_{2}=\frac{\partial U}{\partial x_{2}}$.
Note that both $U_{1}$ and $U_{2}$ are functions of $x_{1}$ and $x_{2}$.



The simplified notation allows us to rewrite the first-order conditions as

$$
\left\{\begin{array}{cccc}
\mathcal{L}_{1}= & U_{1}-\lambda p_{1} & = & 0, \\
\mathcal{L}_{2}= & U_{2}-\lambda p_{2} & = & 0, \\
\mathcal{L}_{\lambda}= & y-p_{1} x_{1}-p_{2} x_{2} & = & 0 .
\end{array}\right\}
$$

Suppose that there is a unique critical point, $\left(x_{1}^{*}, x_{2}^{*}, \lambda^{*}\right)$, of the Lagrangian function for this budget-constrained utility maximisation problem. If an appropriate rank condition and an appropriate second-order condition are satisfied for this constrained optimisation problem, then $x_{1}^{*}=x_{1}^{D}\left(p_{1}, p_{2}, y\right)$ will be the individual's ordinary demand function for commodity one, $x_{2}^{*}=x_{2}^{D}\left(p_{1}, p_{2}, y\right)$ will be the individual's ordinary demand function for commodity two, and $\lambda^{*}=\lambda\left(p_{1}, p_{2}, y\right)$ will be the individual's marginal utility of money income function.

At this point, it is convenient to introduce some more notation.
The second-order partial derivatives of the utility function will be denoted by $U_{11}=\frac{\partial U_{1}}{\partial x_{1}}=\frac{\partial^{2} U}{\partial x_{1}^{2}}, U_{12}=\frac{\partial U_{1}}{\partial x_{2}}=\frac{\partial^{2} U}{\partial x_{2} \partial x_{1}}$, $U_{21}=\frac{\partial U_{2}}{\partial x_{1}}=\frac{\partial^{2} U}{\partial x_{1} \partial x_{2}}$, and $U_{22}=\frac{\partial U_{2}}{\partial x_{2}}=\frac{\partial^{2} U}{\partial x_{2}^{2}}$.

The second-order partial derivatives of the Lagrangian function for this constrained maximisation problem will be denoted by

$\mathcal{L}_{11}=\frac{\partial \mathcal{L}_{1}}{\partial x_{1}}=\frac{\partial^{2} \mathcal{L}}{\partial x_{1}^{2}}, \mathcal{L}_{12}=\frac{\partial \mathcal{L}_{1}}{\partial x_{2}}=\frac{\partial^{2} \mathcal{L}}{\partial x_{2} \partial x_{1}}, \mathcal{L}_{1 \lambda}=\frac{\partial \mathcal{L}_{1}}{\partial \lambda}=\frac{\partial^{2} \mathcal{L}}{\partial \lambda \partial x_{1}}$,

$\mathcal{L}_{21}=\frac{\partial \mathcal{L}_{2}}{\partial x_{1}}=\frac{\partial^{2} \mathcal{L}}{\partial x_{1} \partial x_{2}}, \mathcal{L}_{22}=\frac{\partial \mathcal{L}_{2}}{\partial x_{2}}=\frac{\partial^{2} \mathcal{L}}{\partial x_{2}^{2}}, \mathcal{L}_{2 \lambda}=\frac{\partial \mathcal{L}_{2}}{\partial \lambda}=\frac{\partial^{2} \mathcal{L}}{\partial \lambda \partial x_{2}}$,

$\mathcal{L}_{\lambda 1}=\frac{\partial \mathcal{L}_{\lambda}}{\partial x_{1}}=\frac{\partial^{2} \mathcal{L}}{\partial x_{1} \partial \lambda}, \mathcal{L}_{\lambda 2}=\frac{\partial \mathcal{L}_{\lambda}}{\partial x_{2}}=\frac{\partial^{2} \mathcal{L}}{\partial x_{2} \partial \lambda}$, and $\mathcal{L}_{\lambda \lambda}=\frac{\partial \mathcal{L}_{\lambda}}{\partial \lambda}=\frac{\partial^{2} \mathcal{L}}{\partial \lambda^{2}}$.


The Hessian matrix (that is, the matrix of second-order partial derivatives) for the Lagrangian function for this budget-constrained utility maximisation problem is a bordered Hessian matrix for the individual's utility function, where the borders take the form of the partial derivatives of the budget constraint function $\left(b\left(x_{1}, x_{2}\right)=y-p_{1} x_{1}-p_{2} x_{2}\right)$ with respect to $x_{1}, x_{2}$, and $\lambda$.

This bordered Hessian matrix is

$$
H=\left(\begin{array}{ccc}
\mathcal{L}_{\lambda \lambda} & \mathcal{L}_{\lambda 1} & \mathcal{L}_{\lambda 2} \\
\mathcal{L}_{1 \lambda} & \mathcal{L}_{11} & \mathcal{L}_{12} \\
\mathcal{L}_{2 \lambda} & \mathcal{L}_{21} & \mathcal{L}_{22}
\end{array}\right)=\left(\begin{array}{ccc}
0 & -p_{1} & -p_{2} \\
-p_{1} & U_{11} & U_{12} \\
-p_{2} & U_{21} & U_{22}
\end{array}\right)
$$

Note that if the utility function is at least twice continuously differentiable in all of its arguments, then we know from Young's Theorem that $U_{12}=U_{21}$.


The second-order conditions for a constrained maximisation problem in which there are $n$ choice variables (in the context of a budget-constrained utility maximisation problem, this would mean $n$ commodities) and $m<n$ constraints place restrictions of the signs of the last $(n-m)$ leading principle minors of the bordered Hessian matrix for the problem (when the borders are placed on the top rows and left columns of the bordered Hessian matrix).

Specifically, the last $(n-m)$ leading principal minors of the bordered Hessian matrix must alternate in sign, with the last leading principle minor having the same sign as $(-1)^{n}$.

In the two commodity version of a budget-constrained utility maximisation problem that is being considered here, there are $n=2$ choice variables (namely, $x_{1}$ and $x_{2}$ ) and there is $m=1$ constraint (namely, $b\left(x_{1}, x_{2}\right)=0$ ). Since $n-m=2-1=1$ in this case, the second-order condition for this problem requires that the determinant of the bordered Hessian matrix for the problem be strictly positive.



Note that

$$
\begin{aligned}
\operatorname{det}(H) &= \left|\begin{array}{ccc}
0 & -p_{1} & -p_{2} \\
-p_{1} & U_{11} & U_{12} \\
-p_{2} & U_{21} & U_{22}
\end{array}\right| \\[5pt]
&= 0+\left(-p_{1}\right)(-1)^{1+2}\left|\begin{array}{cc}
-p_{1} & U_{12} \\
-p_{2} & U_{22}
\end{array}\right| 
 +\left(-p_{2}\right)(-1)^{1+3}\left|\begin{array}{ll}
-p_{1} & U_{11} \\
-p_{2} & U_{21}
\end{array}\right| \\[5pt]
&= \left(-p_{1}\right)(-1)^{3}\left\{-p_{1} U_{22}-\left(-p_{2} U_{12}\right)\right\} 
 +\left(-p_{2}\right)(-1)^{4}\left\{-p_{1} U_{21}-\left(-p_{2} U_{11}\right)\right\} \\[5pt]
&= \left(-p_{1}\right)(-1)\left\{p_{2} U_{12}-p_{1} U_{22}\right\} 
 +\left(-p_{2}\right)(1)\left\{p_{2} U_{11}-p_{1} U_{21}\right\} \\[5pt]
&= p_{1}\left\{p_{2} U_{12}-p_{1} U_{22}\right\}-p_{2}\left\{p_{2} U_{11}-p_{1} U_{21}\right\} \\[5pt]
&= p_{1} p_{2} U_{12}-p_{1}^{2} U_{22}-p_{2}^{2} U_{11}+p_{1} p_{2} U_{21} \\[5pt]
&= p_{1} p_{2}\left(U_{12}+U_{21}\right)-\left(p_{1}^{2} U_{22}+p_{2}^{2} U_{11}\right)
\end{aligned}
$$


Recalling that $U_{21}=U_{12}$, this becomes

$$
\begin{aligned}
\operatorname{det}(H) & =p_{1} p_{2}\left(U_{12}+U_{21}\right)-\left(p_{1}^{2} U_{22}+p_{2}^{2} U_{11}\right) \\
& =2 p_{1} p_{2} U_{12}-\left(p_{1}^{2} U_{22}+p_{2}^{2} U_{11}\right)
\end{aligned}
$$


Thus the second-order condition for this two commodity budget-constrained utility maximisation problem is

$$
\begin{aligned}
\operatorname{det}(H) & >0 \\
\Longleftrightarrow 2 p_{1} p_{2} U_{12}-\left(p_{1}^{2} U_{22}+p_{2}^{2} U_{11}\right) & >0 \\
\Longleftrightarrow \quad 2 p_{1} p_{2} U_{12} & >p_{1}^{2} U_{22}+p_{2}^{2} U_{11} .
\end{aligned}
$$


The total differentials of $\mathcal{L}_{1}, \mathcal{L}_{2}$, and $\mathcal{L}_{\lambda}$ are

$$
\left\{\begin{aligned}
d \mathcal{L}_{1} & =U_{11} d x_{1}+U_{12} d x_{2}-p_{1} d \lambda-\lambda d p_{1}+0 d p_{2}+0 d y, \\[5pt]
d \mathcal{L}_{2} & =U_{21} d x_{1}+U_{22} d x_{2}-p_{2} d \lambda+0 d p_{1}-\lambda d p_{2}+0 d y, \\[5pt]
d \mathcal{L}_{\lambda} & =-p_{1} d x_{1}-p_{2} d x_{2}+0 d \lambda-x_{1} d p_{1}-x_{2} d p_{2}+1 d y,
\end{aligned}\right\}
$$

where $U_{i j}=\frac{\partial U_{i}}{\partial x_{j}}=\frac{\partial^{2} U}{\partial x_{j} \partial x_{i}}$ is a function of both $x_{1}$ and $x_{2}$.


The first-order-conditions that characterise the critical point for the Lagrangian must be satisfied throughout this comparative static thought experiment. This requires that $\mathcal{L}_{1}=\mathcal{L}_{2}=\mathcal{L}_{\lambda}=0$, which in turn requires that

$$
\left\{\begin{array}{l}
U_{11} d x_{1}+U_{12} d x_{2}-p_{1} d \lambda-\lambda d p_{1}+0 d p_{2}+0 d y=0, \\[5pt]
U_{21} d x_{1}+U_{22} d x_{2}-p_{2} d \lambda+0 d p_{1}-\lambda d p_{2}+0 d y=0, \\[5pt]
-p_{1} d x_{1}-p_{2} d x_{2}+0 d \lambda-x_{1} d p_{1}-x_{2} d p_{2}+1 d y=0
\end{array}\right\}
$$


Rearranging this system of simultaneous equations so that the $d x_{1}$, $d x_{2}$, and $d \lambda$ terms all appear on the left-hand-side of the equations; and the $d p_{1}, d p_{2}$, and $d y$ terms all appear on the right-hand-side of the equations; we obtain

$$
\left\{\begin{array}{rll}
U_{11} d x_{1}+U_{12} d x_{2}-p_{1} d \lambda & = & \lambda d p_{1}-0 d p_{2}-0 d y, \\[5pt]
U_{21} d x_{1}+U_{22} d x_{2}-p_{2} d \lambda & = & -0 d p_{1}+\lambda d p_{2}-0 d y, \\[5pt]
-p_{1} d x_{1}-p_{2} d x_{2}+0 d \lambda & = & x_{1} d p_{1}+x_{2} d p_{2}-1 d y,
\end{array}\right\}
$$

which can be written in a slightly less cluttered fashion as

$$
\left\{\begin{array}{clc}
U_{11} d x_{1}+U_{12} d x_{2}-p_{1} d \lambda & = & \lambda d p_{1}, \\[5pt]
U_{21} d x_{1}+U_{22} d x_{2}-p_{2} d \lambda & = & \lambda d p_{2}, \\[5pt]
-p_{1} d x_{1}-p_{2} d x_{2} & = & x_{1} d p_{1}+x_{2} d p_{2}-d y
\end{array}\right\}
$$


#### A small change in $p_{1}$ alone

Suppose that we want to examine the impact of a small change in the price of commodity one alone.  In other words, we want to consider an economic shock for which $d p_{1} \neq 0$ and $d p_{2}=d y=0$. The system of simultaneous equations that characterises the impacts of such a shock is given by

$$
\left\{\begin{array}{ccc}
U_{11} d x_{1}+U_{12} d x_{2}-p_{1} d \lambda & = & \lambda d p_{1}, \\[5pt]
U_{21} d x_{1}+U_{22} d x_{2}-p_{2} d \lambda & = & 0, \\[5pt]
-p_{1} d x_{1}-p_{2} d x_{2} & = & x_{1} d p_{1} .
\end{array}\right\}
$$


Since $d p_{1} \neq 0$, we can divide both sides of each of the equations in this system of simultaneous equations by $d p_{1}$ to obtain

$$
\left\{\begin{aligned}
U_{11}\left(\frac{d x_{1}}{d p_{1}}\right)+U_{12}\left(\frac{d x_{2}}{d p_{1}}\right)-p_{1}\left(\frac{d \lambda}{d p_{1}}\right) & =\lambda, \\[5pt]
U_{21}\left(\frac{d x_{1}}{d p_{1}}\right)+U_{22}\left(\frac{d x_{2}}{d p_{1}}\right)-p_{2}\left(\frac{d \lambda}{d p_{1}}\right) & =0, \\[5pt]
-p_{1}\left(\frac{d x_{1}}{d p_{1}}\right)-p_{2}\left(\frac{d x_{2}}{d p_{1}}\right) & =x_{1} .
\end{aligned}\right\}
$$


Upon taking the limit as $d p_{1} \longrightarrow 0$ of both sides of each of the equations in this system of simultaneous equations, we obtain

$$
\left\{\begin{array}{cc}
U_{11}\left(\frac{\partial x_{1}}{\partial p_{1}}\right)+U_{12}\left(\frac{\partial x_{2}}{\partial p_{1}}\right)-p_{1}\left(\frac{\partial \lambda}{\partial p_{1}}\right) & =\lambda, \\[5pt]
U_{21}\left(\frac{\partial x_{1}}{\partial p_{1}}\right)+U_{22}\left(\frac{\partial x_{2}}{\partial p_{1}}\right)-p_{2}\left(\frac{\partial \lambda}{\partial p_{1}}\right) & =0, \\[5pt]
-p_{1}\left(\frac{\partial x_{1}}{\partial p_{1}}\right)-p_{2}\left(\frac{\partial x_{2}}{\partial p_{1}}\right) & =x_{1} .
\end{array}\right\}
$$


The order of the equations in this system of simultaneous equations, and the order in which the relevant variables appear within each equation, can be rearranged to obtain

$$
\left\{\begin{array}{ccc}
0\left(\frac{\partial \lambda}{\partial p_{1}}\right)-p_{1}\left(\frac{\partial x_{1}}{\partial p_{1}}\right)-p_{2}\left(\frac{\partial x_{2}}{\partial p_{1}}\right) & =x_{1}, \\[5pt]
-p_{1}\left(\frac{\partial \lambda}{\partial p_{1}}\right)+U_{11}\left(\frac{\partial x_{1}}{\partial p_{1}}\right)+U_{12}\left(\frac{\partial x_{2}}{\partial p_{1}}\right) & =\lambda, \\[5pt]
-p_{2}\left(\frac{\partial \lambda}{\partial p_{1}}\right)+U_{21}\left(\frac{\partial x_{1}}{\partial p_{1}}\right)+U_{22}\left(\frac{\partial x_{2}}{\partial p_{1}}\right) & =0 .
\end{array}\right\}
$$


The matrix equation representation of this system of simultaneous equations is

$$
\left(\begin{array}{ccc}
0 & -p_{1} & -p_{2} \\
-p_{1} & U_{11} & U_{12} \\
-p_{2} & U_{21} & U_{22}
\end{array}\right)\left(\begin{array}{c}
\frac{\partial \lambda}{\partial p_{1}} \\
\frac{\partial x_{1}}{\partial p_{1}} \\
\frac{\partial x_{2}}{\partial p_{1}}
\end{array}\right)=\left(\begin{array}{c}
x_{1} \\
\lambda \\
0
\end{array}\right) .
$$


Note that the coefficient matrix component of the left-hand-side of this matrix equation is simply the bordered Hessian matrix from the two-commodity budget-constrained utility maximisation problem. If the appropriate second-order condition for that problem is satisfied, then we know that its determinant is strictly positive. Since its determinant is non-zero, we know that it is a non-singular matrix. This means that it is an invertible matrix.



Since the coefficient matrix on the left-hand-side of this matrix equation is invertible, we can conclude that the vector of comparative statics derivatives is given by

$$
\left(\begin{array}{c}
\frac{\partial \lambda}{\partial p_{1}} \\
\frac{\partial x_{1}}{\partial p_{1}} \\
\frac{\partial x_{2}}{\partial p_{1}}
\end{array}\right)=\left(\begin{array}{ccc}
0 & -p_{1} & -p_{2} \\
-p_{1} & U_{11} & U_{12} \\
-p_{2} & U_{21} & U_{22}
\end{array}\right)^{-1}\left(\begin{array}{c}
x_{1} \\
\lambda \\
0
\end{array}\right)
$$


### Some exercises

You might like to try the following two exercises for yourself.

1. Examine the impact of a small change in the price of commodity two alone. (In other words, conduct the above exercise for the case of an economic shock for which $d p_{2} \neq 0$ and $d p_{1}=d y=0$.)

2. Examine the impact of a small change in the individual's income alone. (In other words, conduct the above exercise for the case of an economic shock for which $d y \neq 0$ and $d p_{1}=d p_{2}=0$.)

