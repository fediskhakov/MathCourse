# Univariate differentiation

## Sources and reading guide

````{dropdown} Sources and reading guide

```{figure} _static/img/bibliography/shsc2016.png
:width: 100px
:align: left
```
{cite:ps}`sydsaeter2016`

Chapters 6, 7, and 8.

<div style="clear: both"></div>

Introductory level references: 
- {cite:ps}`bradley2008`: Chapter 6 (pp. 257–357).
- {cite:ps}`haeussler1987`: Chapters 10, 11, 12, and 13 (pp. 375–532).
- {cite:ps}`shannon1995`: Chapter 8 (pp. 356–407).


More advanced references:
- {cite:ps}`ayres2013`
- {cite:ps}`chiang2005`: Chapters 6–10 (pp. 124–290).
- {cite:ps}`kline1967`: Chapters 2, 4, 5, 6, 7, 12, 13, and 20.
- {cite:ps}`silverman1969`: Chapters 4, 5, and 6.
- {cite:ps}`simon1994`: Chapters 2–5 (pp. 10–103).
- {cite:ps}`spivak2006`: Chapters 3–12 (pp. 39–249).
````




## The setting

We will be working with the metric space known as Euclidean one-space.
This space is denoted by $(\mathbb{R}, d )$, where 

$$d (x, y ) = \sqrt{(y − x )^2} = |y − x |$$
is the Euclidean metric (or distance function) on \mathbb{R}.

We will be considering some of the properties of functions that take the form $f : X \rightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}$. In particular, we will be interested in limits, continuity and differentiability.


## What is a limit?
Consider a function, $f: \mathbb{R} \rightarrow \mathbb{R}$. This function can be written as $f(x)$. Note that $x \in \mathbb{R}$ here. Furthermore, for each point $x_0 \in \mathbb{R}$, we have $f(x_0) \in \mathbb{R}$ as well. In other words, the domain of $f$ is $\mathbb{R}$, while the range of $f$ is a (possibly improper) subset of $\mathbb{R}$.

{cite:ps}`spivak2006` (p. 90) provides the following informal definition of a limit: “The function $f$ approaches the limit $l$ near [the point $x =$] $a$,
if we can make $f(x)$ as close as we like to $l$ by requiring that $x$ be sufficiently close to, but unequal to, $a$.”

{cite:ps}`spivak2006` (p. 96) provides the following formal definition of a limit: “The function $f$ **approaches the limit $l$ near** [the point $x =$] $a$ means: for every $\epsilon > 0$, there is some $\delta > 0$ such that, for all $x$, if 
$0 < |x − a| < \delta$, then $|f (x ) − l | < \epsilon$.”. (Bold in the original.)

The formal definition provided by Spivak is known as an **epsilon-delta** ($\epsilon-\delta$) definition of the limit concept. It could also be written as follows:

The limit of $f$ as $x$ approaches $a$ is $l$ if, for each $\epsilon > 0$, there exists some $\delta_\epsilon > 0$ such that

$$ 0 < d (x, a) < \delta_\epsilon \Rightarrow d (f (x ) , l ) < \epsilon $$

Suppose that the limit of $f$ as $x$ approaches $a$ is $l$. We can write this as

$$ \lim_{x  \rightarrow a} f (x ) = l $$


```{figure} _static/img/lecture_06/epsilon_delta.png
:width: 80%
:align: center

An illustration of an epsilon-delta limit argument
```

## The structure of $\epsilon–\delta$ arguments

Suppose that we want to attempt to show that $\lim _{x \rightarrow a} f(x)=b$.

In order to do this, we need to show that, for any choice $\epsilon>0$, there exists some $\delta_{\epsilon}>0$ such that, whenever $|x-a|<\delta_{\epsilon}$, it is the case that $|f(x)-b|<\epsilon$.
- We write $\delta_{\epsilon}$ to indicate that the choice of $\delta$ is allowed to vary with the choice of $\epsilon$.

An often fruitful approach to the construction of a formal $\epsilon-\delta$ limit argument is to proceed as follows: 
1. Start with the end-point that we need to establish: $|f(x)-b|<\epsilon$.
2. Use appropriate algebraic rules to rearrange this "final" inequality into something of the form $|k(x)(x-a)|<\epsilon$.
3. This new version of the required inequality can be rewritten as $|k(x)||(x-a)|<\epsilon$.
4. If $k(x)=k$, a constant that does not vary with $x$, then this inequality becomes $|k||x-a|<\epsilon$. In such cases, we must have $|x-a|<\frac{\epsilon}{|k|}$, so that an appropriate choice of $\delta_{\epsilon}$ is $\delta_{\epsilon}=\frac{\epsilon}{|k|}$.
5. If $k(x)$ does vary with $x$, then we have to work a little bit harder.

Suppose that $k(x)$ does vary with $x$. How might we proceed in that case? One possibility is to see if we can find a restriction on the range of values for $\delta$ that we consider that will allow us to place an upper bound on the value taken by $|k(x)|$.

In other words, we try and find some restriction on $\delta$ that will ensure that $|k(x)|<K$ for some finite $K>0$. The type of restriction on the values of $\delta$ that you choose would ideally look something like $\delta<D$, for some fixed real number $D>0$. (The reason for this is that it is typically small deviations of $x$ from a that will cause us problems rather than large deviations of $x$ from a.)

If $0<|k(x)|<K$ whenever $0<\delta<D$, then we have

$$
|k(x)||x-a|<\epsilon \Longleftrightarrow K|x-a|<\epsilon \Longleftrightarrow|x-a|<\frac{\epsilon}{K} .
$$

In such cases, an appropriate choice of $\delta_{\epsilon}$ is $\delta_{\epsilon}=\min \left\{\frac{\epsilon}{K}, D\right\}$.


## Some useful rules about limits

- In practice, we would like to be able to find at least some limits without having to resort to the formal "epsilon-delta" arguments that define them. The following rules can sometimes assist us with this.
- Let $c \in \mathbb{R}$ be a fixed constant, $a \in \mathbb{R}, \alpha \in \mathbb{R}, \beta \in \mathbb{R}, n \in \mathbb{N}$, $f: \mathbb{R} \longrightarrow \mathbb{R}$ be a function for which $\lim _{x \rightarrow a} f(x)=\alpha$, and $g: \mathbb{R} \longrightarrow \mathbb{R}$ be a function for which $\lim _{x \rightarrow a} g(x)=\beta$.
- The following rules apply for limits.
- $\lim _{x \rightarrow a} c=c$ for any $a \in \mathbb{R}$.
- $\lim _{x \rightarrow a}(c f(x))=c\left(\lim _{x \rightarrow a} f(x)\right)=c \alpha$.
- $\lim _{x \rightarrow a}(f(x)+g(x))=\left(\lim _{x \rightarrow a} f(x)\right)+\left(\lim _{x \rightarrow a} g(x)\right)=\alpha+\beta$.
- $\lim _{x \rightarrow a}(f(x) g(x))=\left(\lim _{x \rightarrow a} f(x)\right)\left(\lim _{x \rightarrow a} g(x)\right)=\alpha \beta$.
- $\lim _{x \rightarrow a}\left(\frac{f(x)}{g(x)}\right)=\frac{\lim _{x \rightarrow a} f(x)}{\lim _{x \rightarrow a} g(x)}=\frac{\alpha}{\beta}$ whenever $\beta \neq 0$.
- $\lim _{x \rightarrow a} \sqrt[n]{f(x)}=\sqrt[n]{\lim _{x \rightarrow a} f(x)}=\sqrt[n]{\alpha}$ whenever $\sqrt[n]{\alpha}$ is defined.
- After we have learned about differentiation, we will learn about another useful rule for limits that is known as "L'Hopital's rule".


### Examples


```{dropdown} Example 1

- This is an example in which the chosen function will be very nicely behaved at all points $x \in \mathbb{R}$.
- Consider the function $f(x)=x$ and the point $x=2$. We will show formally that $\lim _{x \rightarrow 2} f(x)=2$.
- Note that

$$
d(f(x), 2)=|x-2|=d(x, 2) .
$$

- Let $\epsilon>0$. We have

$$
d(f(x), 2)<\epsilon \Longleftrightarrow|x-2|<\epsilon
$$

- Suppose that we let $\delta_{\epsilon}=\epsilon$. Since $\epsilon>0$, we must have $\delta_{\epsilon}>0$.
- Consider points in $\mathbb{R}$ that satisfy the condition

$$
0<d(x, 2)<\delta_{\epsilon} 
$$

- Consider points in $\mathbb{R}$ that satisfy the condition

$$
0<d(x, 2)<\delta_{\epsilon}
$$

- Note that

$$
\begin{aligned}
0<d(x, 2)<\delta_{\epsilon} & \Longleftrightarrow 0<|x-2|<\delta_{\epsilon} \\
& \Longleftrightarrow 0<d(f(x), 2)<\delta_{\epsilon}
\end{aligned}
$$

- Thus we have

$$
0<d(x, 2)<\delta_{\epsilon} \Longrightarrow d(f(x), 2)<\delta_{\epsilon} \text {. }
$$

- Since we chose $\epsilon>0$ arbitrarily, this must be true for all $\epsilon>0$. This means that $\lim _{x \rightarrow 2} f(x)=2$.
```



```{dropdown} Example 2

- This example is drawn from Willis Lauritz Peterson of the University of Utah.
- Consider the mapping $f: \mathbb{R} \longrightarrow \mathbb{R}$ defined by $f(x)=7 x-4$. We want to show that $\lim _{x \rightarrow 2} f(x)=10$.
- Note that $|f(x)-10|=|7 x-4-10|=|7 x-14|=|7(x-2)|=$ $|7||x-2|=7|x-2|$.
- We require $|f(x)-10|<\epsilon$. Note that

$$
|f(x)-10|<\epsilon \Longleftrightarrow 7|x-2|<\epsilon \Longleftrightarrow|x-2|<\frac{\epsilon}{7}
$$

- Thus, for any $\epsilon>0$, if $\delta_{\epsilon}=\frac{\epsilon}{7}$, then $|f(x)-10|<\epsilon$ whenever $|x-2|<\delta_{\epsilon}$.
```


```{dropdown} Example 3

- Consider the mapping $f: \mathbb{R} \longrightarrow \mathbb{R}$ defined by $f(x)=x^{2}$. We want to show that $\lim _{x \rightarrow 2} f(x)=4$.
- Note that $|f(x)-4|=\left|x^{2}-4\right|=|(x+2)(x-2)|=|x+2||x-2|$.
- Suppose that $|x-2|<\delta$, which in turn means that $(2-\delta)<x<(2+\delta)$. Thus we have $(4-\delta)<(x+2)<(4+\delta)$.
- Let us restrict attention to $\delta \in(0,1)$. This gives us $3<(x+2)<5$, so that $|x+2|<5$.
- Thus, when $|x-2|<\delta$ and $\delta \in(0,1)$, we have $|f(x)-4|=|x+2||x-2|<5 \delta$.
- We require $|f(x)-4|<\epsilon$. One way to ensure this is to set $\delta_{\epsilon}=\min \left(1, \frac{\epsilon}{5}\right)$.

```


```{dropdown} Example 4

This example is drawn from Willis Lauritz Peterson of the University of Utah.

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x) = x^2 − 3x + 1$.

We want to show that $lim_{x \rightarrow 2} f(x ) = −1$.

- Note that $|f(x) − (−1)| = |x^2 − 3x + 1 + 1| = |x^2 − 3x + 2| = |(x − 1)(x − 2)| = |x − 1||x − 2|$.
- Suppose that $|x − 2| < \delta$, which in turn means that $(2 − \delta) < x < (2 + \delta)$. Thus we have $(1 − \delta) < (x − 1) < (1 + \delta)$.
- Let us restrict attention to $\delta \in (0, 1)$. This gives us $0 < (x − 1) < 2$, so that $|x − 1| < 2$.
- Thus, when $|x − 2| < \delta$ and $\delta \in (0, 1)$, we have $|f (x ) − (−1)| = |x − 1||x − 2| < 2\delta$.
- We require $|f (x ) − (−1)| < \epsilon$. One way to ensure this is to set $\delta_\epsilon = \min(1, \frac{\epsilon}{2} )$.
```


```{dropdown} Example 5

Limits can sometimes exist even when the function being considered is not so well behaved. One such example is provided by {cite:ps}`spivak2006` (pp. 91–92). It involves the use of a trigonometric function.

As we will not be considering such functions elsewhere in this unit, this particular example will not assessed in this unit. However, the fact that a function might have a limit at a point where it is not defined is worth noting.

The example involves the function $f: \mathbb{R} \setminus 0 \rightarrow \mathbb{R}$ that is defined by $f (x ) = x sin ( \frac{1}{x})$.

Clearly this function is not defined when $x = 0$. Furthermore, it can be shown that $\lim_{x  \rightarrow 0} sin ( \frac{1}{x})$ does not exist. However, it can also be shown that $lim_{x  \rightarrow 0} f (x ) = 0$.

The reason for this is that $sin (\theta) \in [−1, 1]$ for all $\theta \in \mathbb{R}$. Thus $sin ( \frac{1}{x} )$ is bounded above and below by finite numbers as $x \rightarrow 0$. This allows the $x$ component of $x sin (\frac{1}{x})$ to dominate as $x \rightarrow 0$.
```


```{dropdown} Example 6

Limits do not always exist. In this example, we consider a case in which the limit of a function as $x$ approaches a particular point does not exist.

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by

$$
f(x ) = \begin{cases} 
1 \quad \text{ if } x > 5; \\
0 \quad \text{ if } x < 5
\end{cases}
$$

We want to show that $\lim_{x  \rightarrow 5} f (x )$ does not exist.

Suppose that the limit does exist. Denote the limit by $l$. Recall that $d (x, y ) = \{ (y − x )2 \}^{\frac{1}{2}} = |y − x |$. Let $\delta > 0$.

If $|5 − x | < \delta$, then $5 − \delta < x < 5 + \delta$, so that $x \in (5 − \delta, 5 + \delta)$. 

Note that $x \in (5 − \delta, 5 + \delta) = (5 − \delta, 5) \cup [5, 5 + \delta)$, where $(5 − \delta, 5) \ne \varnothing$ and $[5, 5 + \delta) \ne \varnothing$.

Thus we know the following:
1. There exist some $x \in (5 − \delta, 5) \subseteq (5 − \delta, 5 + \delta)$, so that $f (x ) = 0$ for some $x \in (5 − \delta, 5 + \delta)$.
2. There exist some $x \in [5, 5 + \delta) \subseteq (5 − \delta, 5 + \delta)$, so that $f (x ) = 1$ for some $x \in (5 − \delta, 5 + \delta)$.
3. The image set under $f$ for $(5 − \delta, 5 + \delta)$ is $f ((5 − \delta, 5 + \delta)) = \{ f (x ) : x \in (5 − \delta, 5 + \delta) \} = \{ 0, 1 \} \subseteq [0, 1]$.
4. Note that for any choice of $\delta > 0$, $[0, 1]$ is the smallest connected interval that contains the image set $f ((5 − \delta, 5 + \delta)) = \{ 0, 1 \}$.

Hence, in order for the limit to exist, we need $[0, 1] \subseteq (l − \epsilon, l + \epsilon)$
for all $\epsilon > 0$.  But for $\epsilon \in (0, \frac{1}{2})$, there is no $l \in \mathbb{R}$ for which this is the case.  Thus we can conclude that $\lim_{x  \rightarrow 5} f (x )$ does not exist.
```



## Continuity

Our focus here is on continuous real-valued functions of a single real variable. In other words, functions of the form $f : X \rightarrow \mathbb{R}$ where $X \subseteq \mathbb{R}$.

Before looking at the picky technical details of the concept of continuity, it is worth thinking about that concept intuitively. Basically, a real-valued function of a single real-valued variable is **continuous** if you could potentially draw its graph without lifting your pen from the page. In other words, there are no holes in the graph of the function and there are no jumps in the graph of the function.

Now for the slightly picky technical details!
- A function is continuous _at the point_ $x_0 \in X$ if $\lim_{x \rightarrow x_0} f (x ) = f (x_0)$.
- A function is continuous on a connected interval $(a, b) \subseteq X$ for which $a < b$ if it is continuous at all points in that interval. In other words, a function is continuous on $(a, b) \subseteq X$ for which $a < b$ if $\lim_{x \rightarrow x_0} f (x ) = f (x_0)$ for all $x_0 \in (a, b)$.
- A function is **continuous** if it is continuous at every point in its domain. In other words, a function is continuous if $\lim_{x \rightarrow x_0} f (x ) = f (x_0)$ for all $x_0 \in X$.

Now for the really picky technical details!

Let $X \subseteq \mathbb{R}^n$, $d^n$ be the $n$–dimensional Euclidean distance and $d$ be the one–dimensional Euclidean distance.

Suppose that $f : X \rightarrow \mathbb{R}$.
The mapping $f$ is said to be **continuous** at the point $x_0 \in X$ if, given any $\epsilon > 0$, there exists a $\delta > 0$ such that $d(f (x ), f (x_0)) < \epsilon$ whenever $d^n(x, x_0) < \delta$.

This is a formal version of the statement that $\lim_{x \rightarrow x_0} f (x ) = f (x_0)$.

```{figure} _static/img/lecture_06/continuity.png
:width: 80%
:align: center

Epsilon-delta continuity diagram
```

```{figure} _static/img/lecture_06/discontinuity.png
:width: 80%
:align: center

Epsilon-delta discontinuity diagram
```

### Examples

```{dropdown} Example 1

Consider the mapping $f : \mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x ) = x_2$. We want to show that $f(x)$ is continuous at the point $x = 2$.

Recall that $d (x, y ) = \{ (y − x )2 \}^{\frac{1}{2}} = |y − x |$.

Note that $|f (2) − f (x )| = |f (x ) − f (2)| = |x^2 − 4| = |x − 2||x + 2|$.

Suppose that $|2 − x | < \delta$. This means that $|x − 2| < \delta$, which in turn means that $(2 − \delta) < x < (2 + \delta)$. Thus we have $(4 − \delta) < (x + 2) < (4 + \delta)$.

Let us restrict attention to $\delta \in (0, 1)$. This gives us $3 < (x + 2) < 5$, so that $|x + 2| < 5$.

We have $|f (2) − f (x )| = |f (x ) − f (2)| = |x − 2||x + 2| < (\delta)(5) = 5\delta$.

We require $|f (2) − f (x )| < \epsilon$. One way to ensure this is to set $\delta = min(1, \frac{\epsilon}{5})$.
```



```{dropdown} Example 2

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x ) = x$. We want to show that $f (x )$ is continuous for all $x \in X$.

Recall that $d (x, y ) = \{ (y − x )2 \}^{\frac{1}{2}} = |y − x |$.

Consider an arbitrary point $x = a$. Note that $|f (a) − f (x )| = |a − x |$.

Suppose that $|a − x | < \delta$.

Note that if we set $\delta = \epsilon$, we have $|f (a) − f (x )| < \epsilon$. 

Thus we know that $f$ is continuous at the point $x = a$. Since $a$ was chosen arbitrarily, we now that this is true for all $a \in \mathbb{R}$. This means that $f$ is continuous for all $x \in \mathbb{R}$.
```



```{dropdown} Example 3 (discontinuity)

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by 

$$
f(x ) =
\begin{cases} 
1 \quad \text{ if } x > 5; \\
0 \quad \text{ if } x < 5
\end{cases}
$$

We have already shown that $\lim_{x  \rightarrow 5} f (x )$ does not exist. This means that it is impossible for $lim_{x  \rightarrow 5} f (x ) = f (5)$. As such, we know that $f (x )$ is not continuous at the point $x = 5$.

This means that f (x ) is not a continuous function. 

However, it can be shown that (i) $f(x )$ is continuous on the interval $(−\infty, 5)$, and that (ii) $f (x )$ is continuous on the interval $(5, \infty)$. Can
you explain why this is the case?
```


```{dropdown} Example 4 (discontinuity)

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by 

$$
f (x ) = \begin{cases}
\frac{1}{x} \quad \text{ if } x \ne 0; \\
0 \quad \text{ if } x = 0.
\end{cases}
$$
This function is a rectangular hyperbola when $x \ne 0$, but it takes on the value $0$ when $x = 0$. Recall that the rectangular hyperbola part of this function is not defined at the point $x = 0$.

This function is discontinuous at the point $x = 0$. *Illustrate the graph of this function on the whiteboard.*
```


```{dropdown} Example 5 (discontinuity)

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by

$$
f (x ) = \begin{cases} 
1 \quad \text{ if } x \in Q; \\
0 \quad \text{ if } x \notin Q
\end{cases}
$$
This function is sometimes known as Dirichlet’s discontinuous function. It is discontinuous at every point in its domain.
```



## Derivatives

If it exists, the derivative of a function tells us the direction and magnitude of the change in the dependent variable that will be induced by a very small increase in the independent variable.

If it exists, the derivative of a function at a point is equal to the slope of the straight line that is tangent to the function at that point. 

You should try and guess now what the derivative of a linear function of the form $f (x ) = ax + b$ will be.

### Definition

Consider a function $f : X \rightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}$.
Let $(x_0, f (x_0))$ and $(x_0 + h, f (x_0 + h))$ be two points that lie on the graph of this function.

Draw a straight line between these two points. The slope of this line is 

$$
\begin{align*}
\frac{\text{rise}}{\text{run}} &= \frac{y_2 − y_1}{x_2 − x_1} \\
&= \frac{f (x_0 + h) − f (x_0)}{x_0 + h − x_0} \\
&= \frac{f (x_0 + h) − f (x_0)}{h}
\end{align*}
$$

What happens to this slope as $h \rightarrow 0$ (that is, as the second point
gets very close to the first point?


The **derivative** of the function $f(x)$ at the point $x=x_{0}$, if it exists, is defined to be

$$
f^{\prime}\left(x_{0}\right)
=\left.\frac{d f}{d x}\right|_{x=x_{0}}
=\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h} .
$$

This is simply the slope of the straight line that is tangent to the function $f(x)$ at the point $x=x_{0}$.

We will now proceed to use this definition to find the derivative of some simple functions. This is sometimes called "finding the derivative of a function from first principles".

```{admonition} Example: the (first) derivative of a constant function
:class: tip

Consider the function $f: X \longrightarrow \mathbb{R}$ defined by $f(x)=b$, where $b \in \mathbb{R}$ is a constant.
Clearly we have $f\left(x_{0}\right)=f\left(x_{0}+h\right)=b$ for all choices of $x_{0}$ and $h$.

Thus we have

$$
\frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\frac{b-b}{h}=\frac{0}{h}=0
$$

for all choices of $x_{0}$ and $h$.

This means that

$$
\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\lim _{h \rightarrow 0} 0=0
$$

As such, we can conclude that

$$
f^{\prime}(x)=\frac{d f}{d x}=0
$$

```

```{admonition} Example: the (first) derivative of a linear function
:class: tip

Consider the function $f: X \longrightarrow \mathbb{R}$ defined by $f(x)=a x+b$.  Note that

$$
\begin{aligned}
f(x+h) & =a(x+h)+b \\
& =a x+a h+b \\
& =a x+b+a h \\
& =f(x)+a h .
\end{aligned}
$$

Thus we have

$$
\frac{f(x+h)-f(x)}{h}=\frac{f(x)+a h-f(x)}{h}=\frac{a h}{h}=a .
$$

This means that $\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\lim _{h \rightarrow 0} a=a$.

As such, we can conclude that $f^{\prime}(x)=\frac{d f}{d x}=a$.
```

```{admonition} Example: the (first) derivative of a quadratic power function
:class: tip

Consider the function $f: X \longrightarrow \mathbb{R}$ defined by $f(x)=x^{2}$.  Note that

$$
\begin{aligned}
f(x+h) & =(x+h)^{2} \\
& =x^{2}+2 x h+h^{2} \\
& =f(x)+2 x h+h^{2}
\end{aligned}
$$

Thus we have

$$
\begin{aligned}
\frac{f(x+h)-f(x)}{h} & =\frac{f(x)+2 x h+h^{2}-f(x)}{h} \\
& =\frac{2 x h+h^{2}}{h} \\
& =2 x+h .
\end{aligned}
$$

This means that $\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\lim _{h \rightarrow 0}(2 x+h)=2 x$.

As such, we can conclude that $f^{\prime}(x)=\frac{d f}{d x}=2 x$
```


```{admonition} Example: the (first) derivative of a quadratic polynomial function
:class: tip

Consider the function $f: X \longrightarrow \mathbb{R}$ defined by $f(x)=a x^{2}+b x+c$. Note that

$$
\begin{aligned}
f(x+h) & =a(x+h)^{2}+b(x+h)+c \\
& =a\left(x^{2}+2 x h+h^{2}\right)+b x+b h+c \\
& =a x^{2}+2 a x h+a h^{2}+b x+b h+c \\
& =\left(a x^{2}+b x+c\right)+(2 a x+b) h+a h^{2} \\
& =f(x)+(2 a x+b) h+a h^{2} .
\end{aligned}
$$

Thus we have

$$
\begin{aligned}
\frac{f(x+h)-f(x)}{h} & =\frac{f(x)+(2 a x+b) h+a h^{2}-f(x)}{h} \\
& =\frac{(2 a x+b) h+a h^{2}}{h} \\
& =(2 a x+b)+a h \\
& =2 a x+b+a h .
\end{aligned}
$$

This means that

$$
\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\lim _{h \rightarrow 0}(2 a x+b+a h)=2 a x+b
$$

As such, we can conclude that $f^{\prime}(x)=\frac{d f}{d x}=2 a x+b$.

```


```{admonition} Example: the (first) derivative of a positive integer power function
:class: tip

Consider the function $f: \mathbb{R} \longrightarrow \mathbb{R}$ defined by $f(x)=x^{n}$, where $n \in \mathbb{N}=\mathbb{Z}_{++}$. We know from the binomial theorem that

$$
\begin{aligned}
f(x+h) & =(x+h)^{n} \\
& =\sum_{r=0}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r} \\
& =\left(\begin{array}{l}
n \\
0
\end{array}\right) x^{n} h^{0}+\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r} \\
& =1 x^{n}(1)+\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r} \\
& =x^{n}+\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r} .
\end{aligned}
$$

Thus we have

$$
\begin{aligned}
\frac{f(x+h)-f(x)}{h} & =f(x)+\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r} \\
& =\frac{f(x)+\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r}-f(x)}{h} \\
& =\frac{\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r}}{h} \\
& =\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r-1} \\
& =\left(\begin{array}{l}
n \\
1
\end{array}\right) x^{n-1} h^{0}+\sum_{r=2}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r-1} \\
& =n x^{n-1}(1)+\sum_{r=2}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r-1}
\end{aligned}
$$

$$
\begin{aligned}
\frac{f(x+h)-f(x)}{h} & =n x^{n-1}+\sum_{r=2}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r-1} \\
& =n x^{n-1}+\sum_{r=2}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r-1}
\end{aligned}
$$

This means that

$$
\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\lim _{h \rightarrow 0}\left(n x^{n-1}+\sum_{r=2}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r-1}\right)=n x^{n-1}
$$

As such, we can conclude that

$$
f^{\prime}(x)=\frac{d f}{d x}=n x^{n-1}
$$

```


### The (first) derivatives of some commonly encountered functions

- If $f(x)=a$, where $a \in \mathbb{R}$ is a constant, then $f^{\prime}(x)=0$.
- If $f(x)=a x+b$, then $f^{\prime}(x)=a$.
- If $f(x)=a x^{2}+b x+c$, then $f^{\prime}(x)=2 a x+b$.
- If $f(x)=x^{n}$, where $n \in \mathbb{N}$, then $f^{\prime}(x)=n x^{n-1}$.
- If $f(x)=\frac{1}{x^{n}}=x^{-n}$, where $n \in \mathbb{N}$, then $f^{\prime}(x)=-n x^{-n-1}=-n x^{-(n+1)}=\frac{-n}{x^{n+1}}$. (Note that we need to assume that $x \neq 0$.)
- If $f(x)=e^{x}=\exp (x)$, then $f^{\prime}(x)=e^{x}=\exp (x)$.
- If $f(x)=\ln (x)$, then $f^{\prime}(x)=\frac{1}{x}$. (Recall that $\ln (x)$ is only defined for $x>0$, so we need to assume that $x>0$ here.)


### Some useful differentiation rules

- Scalar Multiplication Rule: If $f(x)=c g(x)$ where $c \in \mathbb{R}$ is a constant, then $f^{\prime}(x)=c g^{\prime}(x)$.
- Summation Rule: If $f(x)=g(x)+h(x)$, then $f^{\prime}(x)=g^{\prime}(x)+h^{\prime}(x)$.
- Product Rule: If $f(x)=g(x) h(x)$, then $f^{\prime}(x)=g^{\prime}(x) h(x)+h^{\prime}(x) g(x)$.
- Quotient Rule: If $f(x)=\frac{g(x)}{h(x)}$, then $f^{\prime}(x)=\frac{g^{\prime}(x) h(x)-h^{\prime}(x) g(x)}{[h(x)]^{2}}$.
- Chain Rule: If $f(x)=g(h(x))$, then $f^{\prime}(x)=g^{\prime}(h(x)) h^{\prime}(x)$.
- The Inverse Function Rule: Suppose that the function $y=f(x)$ has a well defined inverse function $x=f^{-1}(y)$. If appropriate regularity conditions hold, then

$$
\frac{d x}{d y}=\frac{1}{\left(\frac{d y}{d x}\right)}
$$

### The quotient rule is redundant

In a sense, the quotient rule is redundant. The reason for this is that it can be obtained from a combination of the product rule and the chain rule.

Suppose that $f(x)=\frac{g(x)}{h(x)}$. Note that

$$
f(x)=\frac{g(x)}{h(x)}=g(x)[h(x)]^{-1}
$$

Let $[h(x)]^{-1}=k(x)$.  We know from the chain rule that

$$
k^{\prime}(x)=(-1)[h(x)]^{-2} h^{\prime}(x)=\frac{-h^{\prime}(x)}{[h(x)]^{2}}
$$

Note that
$$
f(x)=\frac{g(x)}{h(x)}=g(x)[h(x)]^{-1}=g(x) k(x) .
$$

We know from the product rule that

$$
\begin{aligned}
f^{\prime}(x) & =g^{\prime}(x) k(x)+k^{\prime}(x) g(x) \\
& =g^{\prime}(x)[h(x)]^{-1}+\left(\frac{-h^{\prime}(x)}{[h(x)]^{2}}\right) g(x) \\
& =\frac{g^{\prime}(x)}{h(x)}-\frac{h^{\prime}(x) g(x)}{[h(x)]^{2}} \\
& =\frac{g^{\prime}(x) h(x)}{[h(x)]^{2}}-\frac{h^{\prime}(x) g(x)}{[h(x)]^{2}} \\
& =\frac{g^{\prime}(x) h(x)-h^{\prime}(x) g(x)}{[h(x)]^{2}} .
\end{aligned}
$$

This is simply the quotient rule!

```{admonition} Example: the (first) derivative of an exponential function
:class: tip

Suppose that $f(x)=a^{x}$, where $a \in \mathbb{R}_{++}=(0, \infty)$ and $x \neq 0$. Note that

$$
\ln (f(x))=\ln \left(a^{x}\right)=x \ln (a) .
$$

We know from the chain rule that

$$
\frac{d f(x)}{d x}=\left(\frac{d f(x)}{d \ln (f(x))}\right)\left(\frac{d \ln (f(x))}{d x}\right)
$$

We know from the inverse function rule that

$$
\frac{d f(x)}{d \ln (f(x))}=\frac{1}{\left(\frac{d \ln (f(x))}{d f(x)}\right)}=\frac{1}{\left(\frac{1}{a^{x}}\right)}=a^{x}
$$

We know from the scalar multiplication rule that

$$
\begin{aligned}
\frac{d \ln (f(x))}{d x} & =\frac{d x \ln (a)}{d x} \\
& =\ln (a)\left(\frac{d x}{d x}\right) \\
& =\ln (a)(1) \\
& =\ln (a) .
\end{aligned}
$$

Thus we have

$$
\frac{d a^{x}}{d x}=a^{x} \ln (a)
$$

```

```{admonition} Example: the (first) derivative of a logarithmic function
:class: tip

Suppose that $f(x)=\log _{a}(x)$, where $a \in \mathbb{R}_{++}=(0, \infty)$ and $x>0$.  Note that

$$
a^{f(x)}=a^{\log _{a}(x)}=x .
$$

We know from the chain rule that

$$
\frac{d f(x)}{d x}=\left(\frac{d f(x)}{d a^{f(x)}}\right)\left(\frac{d a^{f(x)}}{d x}\right)
$$

We know from the inverse function rule that

$$
\frac{d f(x)}{d a^{f(x)}}=\frac{1}{\left(\frac{d a^{f(x)}}{d f(x)}\right)}=\frac{1}{\left(a^{f(x)} \ln (a)\right)}=\frac{1}{x \ln (a)}
$$

Thus we have

$$
\begin{aligned}
\frac{d a^{x}}{d x} & =\left(\frac{1}{x \ln (a)}\right)\left(\frac{d x}{d x}\right) \\
& =\left(\frac{1}{x \ln (a)}\right)(1) \\
& =\frac{1}{x \ln (a)} .
\end{aligned}
$$

```

```{admonition} Example: product rule
:class: tip

Consider the function $f(x)=(a x+b)(c x+d)$.

Differentiation Approach One:
Note that

$$
\begin{aligned}
f(x) & =(a x+b)(c x+d) \\
& =a c x^{2}+a d x+b c x+b d \\
& =a c x^{2}+(a d+b c) x+b d
\end{aligned}
$$

Thus we have

$$
\begin{aligned}
f^{\prime}(x) & =2 a c x+(a d+b c) \\
& =2 a c x+a d+b c
\end{aligned}
$$


Differentiation Approach Two:
Note that $f(x)=g(x) h(x)$ where $g(x)=a x+b$ and $h(x)=c x+d$.
This means that $g^{\prime}(x)=a$ and $g^{\prime}(x)=c$.
Thus we know, from the product rule, that

$$
\begin{aligned}
f^{\prime}(x) & =g^{\prime}(x) h(x)+h^{\prime}(x) g(x) \\
& =a(c x+d)+c(a x+b) \\
& =a c x+a d+a c x+b c \\
& =2 a c x+a d+b c .
\end{aligned}
$$
```



## The relationship between continuity and differentiability

- Continuity is a necessary, but not sufficient, condition for differentiability.
    - Being a necessary condition means that "not continuous" implies "not differentiable", which means that differentiable implies continuous.
    - Not being a sufficient condition means that continuous does NOT imply differentiable.
- Differentiability is a sufficient, but not necessary, condition for continuity.
    - Being a sufficient condition means that differentiable implies continuous.
    - Not being a necessary condition means that "not differentiable" does NOT imply "not continuous", which means that continuous does NOT imply differentiable.


### Continuity does NOT imply differentiability

Consider the function

$$
f(x)=\left\{\begin{array}{cc}
2 x & \text { if } x \leq 1 \\
\frac{1}{2} x+\frac{3}{2} & \text { if } x \geq 1
\end{array}\right.
$$

(There is no problem with this double definition at the point $x=1$ because the two parts of the function are equal at that point.)

This function is continuous at $x=1$ because

$$
\lim _{x \rightarrow 1} 2 x=2=\lim _{x \rightarrow 1}\left(\frac{1}{2} x+\frac{3}{2}\right)
$$

and

$$
f(1)=2
$$


However, this function is not differentiable at $x=1$ because

$$
\lim _{h \uparrow 1} \frac{f(1+h)-f(1)}{h}=2
$$

and

$$
\lim _{h \downarrow 1} \frac{f(1+h)-f(1)}{h}=\frac{1}{2}
$$

Since

$$
\lim _{h \uparrow 1} \frac{f(1+h)-f(1)}{h} \neq \lim _{h \downarrow 1} \frac{f(1+h)-f(1)}{h}
$$

we know that

$$
\lim _{h \rightarrow 1} \frac{f(1+h)-f(1)}{h}
$$

does not exist.

### Differentiability implies continuity

Consider a function $f: X \longrightarrow \mathbb{R}$ where $X \subseteq \mathbb{R}$.
Suppose that

$$
\lim _{h \rightarrow 0}\left(\frac{f(a+h)-f(a)}{h}\right)
$$

exists.

We want to show that this implies that $f(x)$ is continuous at the point $a \in X$.  The following proof of this proposition is drawn from {cite:ps}`ayres2013` (Chapter 8, Solved Problem 2).


First, note that

$$
\begin{gathered}
\lim _{h \rightarrow 0}(f(a+h)-f(a))=\lim _{h \rightarrow 0}\left\{\left(\frac{h}{h}\right)(f(a+h)-f(a))\right\} \\
=\lim _{h \rightarrow 0}\left\{h\left(\frac{f(a+h)-f(a)}{h}\right)\right\} \\
=\lim _{h \rightarrow 0}(h) \lim _{h \rightarrow 0}\left(\frac{f(a+h)-f(a)}{h}\right) \\
=(0)\left(\lim _{h \rightarrow 0}\left(\frac{f(a+h)-f(a)}{h}\right)\right) \\
=0 .
\end{gathered}
$$

Thus we have

$$
\lim _{h \rightarrow 0}(f(a+h)-f(a))=0 .
$$

Now note that

$$
\begin{gathered}
\lim _{h \rightarrow 0}(f(a+h)-f(a))=\lim _{h \rightarrow 0} f(a+h)-\lim _{h \rightarrow 0} f(a) \\
=\left(\lim _{h \rightarrow 0} f(a+h)\right)-f(a)
\end{gathered}
$$

Upon combining these two results, we obtain

$$
\left(\lim _{h \rightarrow 0} f(a+h)\right)-f(a)=0 \Longleftrightarrow \lim _{h \rightarrow 0} f(a+h)=f(a) .
$$

Finally, note that

$$
\lim _{x \rightarrow a} f(x)=\lim _{h \rightarrow 0} f(a+h) .
$$

Thus we have

$$
\lim _{x \rightarrow a} f(x)=f(a)
$$

This means that $f(x)$ is continuous at the point $x=a$.



## Higher-order derivatives

Suppose that $f: X \longrightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}$, is an $n$-times continuously differentiable function for some $n \geqslant 2$.

We can view the first derivative of this function as a function in its own right. This can be seen by letting $g(x)=f^{\prime}(x)$.

The second derivative of $f(x)$ with respect to $x$ twice is simply the first derivative of $g(x)$ with respect to $x$.

In other words,

$$
f^{\prime \prime}(x)=g^{\prime}(x)
$$

or, if you prefer,

$$
\frac{d^{2} f}{d x^{2}}=\frac{d g}{d x}
$$

Thus we have

$$
\frac{d^{2} f}{d x^{2}}=\frac{d}{d x}\left(\frac{d f}{d x}\right)
$$


The same approach can be used for defining third and higher order derivative.
Thus we have

$$
f^{(k)}(x)=\frac{d^{k} f}{d x^{k}}=\frac{d}{d x}\left(\frac{d^{k-1} f}{d x^{k-1}}\right)
$$

for all $k \in\{1,2, \cdots, n\}$, where we define

$$
f^{(0)}(x)=f(x)
$$


```{admonition} Example: higher-order derivatives

- Let $f(x)=x^{n}$
- Then we have:
- $f^{\prime}(x)=\frac{d f(x)}{d x}=n x^{n-1}$
- $f^{\prime \prime}(x)=\frac{d f^{\prime}(x)}{d x}=n(n-1) x^{n-2}$,
- $f^{\prime \prime \prime}(x)=\frac{d f^{\prime \prime}(x)}{d x}=n(n-1)(n-2) x^{n-3}$,
- and so on and so forth until
- $f^{(k)}(x)=\frac{d f^{(k-1)}(x)}{d x}=n(n-1)(n-2) \cdots(n-(k-1)) x^{n-k}$,
- and so on and so forth until
- $f^{(n)}(x)=\frac{d f^{(n-1)}(x)}{d x}=n(n-1)(n-2) \cdots(1) x^{0}$.
- Note that $n(n-1)(n-2) \cdots(1)=n$ ! and $x^{0}=1$ (asuming that $x \neq 0$ ).
- This means that $f^{(n)}(x)=n$ !, which is a constant.
- As such, we know that $f^{(n+1)}(x)=\frac{d f^{(n)}(x)}{d x}=0$.
- This means that $f^{(n+j)}(x)=0$ for all $j \in \mathbb{N}$.
```


### Interpreting first and second derivatives

- The first and second order derivatives of a function provide useful information about the shape of that function.
- The first derivative tells us about the slope of a function. Is it increasing, decreasing, or constant over some interval? At what point or points, if any, does the sign of the slope of the function change? (Such points are known as turning points.)
- The second derivative tells us about the curvature of the function. Is it convex or concave over some interval? At what point or points, if any, does the curvature of the function change? (Such points are known as inflexion points.)


## Increasing functions

- A function is said to be **increasing** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(x_{0}\right) \leqslant f\left(x_{1}\right)$ whenever $x_{0}<x_{1}$, where $x_{0} \in(a, b)$ and $x_{1} \in(a, b)$.
    - If $f^{\prime}(x) \geqslant 0$ for all $x \in(a, b)$, then the function $f(x)$ is increasing on the interval $(a, b)$.
- A function is said to be **strictly increasing** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(x_{0}\right)<f\left(x_{1}\right)$ whenever $x_{0}<x_{1}$, where $x_{0} \in(a, b)$ and $x_{1} \in(a, b)$.
    - If $f^{\prime}(x)>0$ for all $x \in(a, b)$, then the function $f(x)$ is strictly increasing on the interval $(a, b)$.


## Decreasing functions

- A function is said to be **decreasing** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(x_{0}\right) \geqslant f\left(x_{1}\right)$ whenever $x_{0}<x_{1}$, where $x_{0} \in(a, b)$ and $x_{1} \in(a, b)$.
    - If $f^{\prime}(x) \leqslant 0$ for all $x \in(a, b)$, then the function $f(x)$ is decreasing on the interval $(a, b)$.
- A function is said to be **strictly decreasing** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(x_{0}\right)>f\left(x_{1}\right)$ whenever $x_{0}<x_{1}$, where $x_{0} \in(a, b)$ and $x_{1} \in(a, b)$.
    - If $f^{\prime}(x)<0$ for all $x \in(a, b)$, then the function $f(x)$ is strictly decreasing on the interval $(a, b)$.


## Convex functions

- A function is said to be **convex** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(\lambda x_{0}+(1-\lambda) x_{1}\right) \leqslant \lambda f\left(x_{0}\right)+(1-\lambda) f\left(x_{1}\right)$ whenever $x_{0} \in(a, b), x_{1} \in(a, b)$ and $x_{0} \neq x_{1}$.
    - In other words, a function is convex on an interval if the graph of the straight line joining any two distinct points in that interval lies on or above the graph of the function between those two points.
    - If $f^{\prime \prime}(x) \geqslant 0$ for all $x \in(a, b)$, then the function $f(x)$ is convex on the interval $(a, b)$.
- A function is said to be **strictly convex** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(\lambda x_{0}+(1-\lambda) x_{1}\right)<\lambda f\left(x_{0}\right)+(1-\lambda) f\left(x_{1}\right)$ whenever $x_{0} \in(a, b), x_{1} \in(a, b)$ and $x_{0} \neq x_{1}$.
    - In other words, a function is strictly convex on an interval if the graph of the straight line joining any two distinct points in that interval lies strictly above the graph of the function between those two points.
    - If $f^{\prime \prime}(x)>0$ for all $x \in(a, b)$, then the function $f(x)$ is strictly convex on the interval $(a, b)$.


## Concave functions

- A function is said to be **concave** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(\lambda x_{0}+(1-\lambda) x_{1}\right) \geqslant \lambda f\left(x_{0}\right)+(1-\lambda) f\left(x_{1}\right)$ whenever $x_{0} \in(a, b), x_{1} \in(a, b)$ and $x_{0} \neq x_{1}$.
    - In other words, a function is concave on an interval if the graph of the straight line joining any two distinct points in that interval lies on or below the graph of the function between those two points.
    - If $f^{\prime \prime}(x) \leqslant 0$ for all $x \in(a, b)$, then the function $f(x)$ is convex on the interval $(a, b)$.
- A function is said to be **strictly concave** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(\lambda x_{0}+(1-\lambda) x_{1}\right)>\lambda f\left(x_{0}\right)+(1-\lambda) f\left(x_{1}\right)$ whenever $x_{0} \in(a, b), x_{1} \in(a, b)$ and $x_{0} \neq x_{1}$.
    - In other words, a function is strictly concave on an interval if the graph of the straight line joining any two distinct points in that interval lies strictly below the graph of the function between those two points.
    - If $f^{\prime \prime}(x)<0$ for all $x \in(a, b)$, then the function $f(x)$ is strictly convex on the interval $(a, b)$.


## Peak (local maximum) turning points

- The point $c$ yields a **local maximum** of the function $f(x)$ if there exists some non-empty interval $(a, b)$ that is a subset of the domain of the function such that $c \in(a, b)$ and $f(x) \leqslant f(c)$ for all $x \in(a, b)$.
- If (i) $f^{\prime}(c)=0$, (ii) $f^{\prime}(x) \geqslant 0$ for all $x \in(a, c)$ where $a<c$, and (iii) $f^{\prime}(x) \leqslant 0$ for all $x \in(c, b)$ where $c<b$, then the point $x=c$ yields a local maximum of the function $f$ over the non-empty interval $(a, b)$.
- If (i) $f^{\prime}(c)=0$ and (ii) $f^{\prime \prime}(c)<0$, then the point $x=c$ yields a strict local maximum of the function $f$ over some non-empty interval $(a, b)$ such that $c \in(a, b)$.
    - Note that we need $f^{\prime \prime}(c)<0$ here. If $f^{\prime \prime}(c)=0$, then this theorem does not apply. In such a case, a different approach must be used to determine whether or not the point $x=c$ yields a local maximum of the function.


## Trough (local minimum) turning points

- The point $c$ yields a **local minimum** of the function $f(x)$ if there exists some non-empty interval $(a, b)$ that is a subset of the domain of the function such that $c \in(a, b)$ and $f(x) \geqslant f(c)$ for all $x \in(a, b)$.
- If (i) $f^{\prime}(c)=0$, (ii) $f^{\prime}(x) \leqslant 0$ for all $x \in(a, c)$ where $a<c$, and (iii) $f^{\prime}(x) \geqslant 0$ for all $x \in(c, b)$ where $c<b$, then the point $x=c$ yields a local minimum of the function $f$ over the non-empty interval $(a, b)$.
- If (i) $f^{\prime}(c)=0$ and (ii) $f^{\prime \prime}(c)>0$, then the point $x=c$ yields a strict local minimum of the function $f$ over some non-empty interval $(a, b)$ such that $c \in(a, b)$.
    - Note that we need $f^{\prime \prime}(c)>0$ here. If $f^{\prime \prime}(c)=0$, then this theorem does not apply. In such a case, a different approach must be used to determine whether or not the point $x=c$ yields a local minimum of the function.


## Inflection points

- An **inflection point** of a function is a point at which the curvature of the function changes from either convex to concave or concave to convex.
- The point $x=c$ is an inflection point of the twice differentiable function $f(x)$ if there exists some interval $(a, b)$ that is a subset of the domain of $f(x)$ such that $c \in(a, b)$ and either (a) both (i) $f^{\prime \prime}(x) \geqslant 0$ for all $x \in(a, c)$ and (ii) $f^{\prime \prime}(x) \leqslant 0$ for all $x \in(c, b)$, or (b) both (i) $f^{\prime \prime}(x) \leqslant 0$ for all $x \in(a, c)$ and (ii) $f^{\prime \prime}(x) \geqslant 0$ for all $x \in(c, b)$.


- Suppose that the function $f(x)$ is twice continuously differentiable on the interval $(a, b)$, where that interval is a subset of the domain of the function. If $c \in(a, b)$ is an inflection point of the function $f(x)$, then $f^{\prime \prime}(c)=0$.
- Suppose that the function $f(x)$ is twice continuously differentiable on the interval $(a, b)$, where that interval is a subset of the domain of the function and $c \in(a, b)$. If both (i) $f^{\prime \prime}(c)=0$ and (ii) $f^{\prime \prime}(c)$ changes sign at the point $x=c$, then the point $x=c$ is an inflection point of the function $f(x)$.


### Some economic applications of derivatives

- Obtaining marginal revenue from total revenue and marginal cost from total cost.
- The relationship between the marginal cost curve and the average cost curve. (See separate handout.)
- Marginal revenue for a single-price monopolist that faces a linear demand curve. (See separate handout.)
- The calculation of elasticities, with a particular focus on the own-price elasticity of demand, cross-price elasticities of demand, the income elasticity of demand, and the own-price elasticity of supply.


#### Marginal revenue and marginal cost

- If a firm's total revenue as a function of output is given by the function $R(Q)$, then its marginal revenue is given by the first derivative of that function:

$$
MR(Q)=\frac{d R(Q)}{d Q}=R^{\prime}(Q)
$$

- If a firm's total cost as a function of output is given by the function $C(Q)$, then its marginal cost is given by the first derivative of that function:

$$
MC(Q)=\frac{d C(Q)}{d Q}=C^{\prime}(Q)
$$

#### Marginal revenue for a monopolist

Consider a single-priced monopolist that faces an inverse demand curve that is given by the function $P(Q)$. Since demand curves usually slope down, we will assume that $P^{\prime}(Q)<0$. The total revenue for this monopolist is given by $R(Q)=P(Q) Q$. As such, the marginal revenue for this monopolist is

$$
\begin{aligned}
M R(Q) & =R^{\prime}(Q) \\
& =P^{\prime}(Q) Q+(1) P(Q) \\
& =P^{\prime}(Q) Q+P(Q) \\
& =P(Q)+P^{\prime}(Q) Q \\
& <P(Q)
\end{aligned}
$$

Since $P(Q)$ will be the market price when the monopolist produces $Q>0$ units of output and $P^{\prime}(Q)<0$, we know that marginal revenue is less than the market price for a monopolist.


The two terms that make up marginal revenue for a monopolist can be given a very intuitive explanation. If the monopolist increases its output by a small amount, it receives the market price for the additional output (hence the $P(Q)$ term), but it loses some revenue due to the lower price that it will receive for all of the pre-existing output (the $P^{\prime}(Q) Q$ term).
- The additional output is known as the "marginal" unit of output.
- The pre-existing units of output are known as the "infra-marginal" units of output.

*Illustrate the relationship between the inverse market demand curve for a monopolist's product and the marginal revenue curve for the monopolist on the white-board.*



#### Point elasticities

Suppose that $y=f(x)$. The **elasticity of $y$ with respect to $x$** is defined to be the percentage change in $y$ that is induced by a small percentage change in $x$.

In calculus terms, this is given by the function

$$
\epsilon_{x}^{y}(x)=\left(\frac{x}{y}\right)\left(\frac{d y}{d x}\right)=\frac{x f^{\prime}(x)}{f(x)}
$$

It can also be calculated using calculus as

$$
\epsilon_{x}^{y}(x)=\frac{d \ln (y)}{d \ln (x)}
$$

We can use the chain rule and the inverse function rule to show that these two formulae for the elasticity of $y$ with respect to $x$ are equivalent. Note that

$$
\begin{aligned}
\frac{d \ln (y)}{d \ln (x)} & =\left(\frac{d \ln (y)}{d y}\right)\left(\frac{d y}{d x}\right)\left(\frac{d x}{d \ln (x)}\right) \\
& =\left(\frac{1}{y}\right)\left(\frac{d y}{d x}\right)\left(\frac{1}{\left(\frac{d \ln (x)}{d x}\right)}\right) \\
& =\left(\frac{1}{y}\right)\left(\frac{d y}{d x}\right)\left(\frac{1}{\left(\frac{1}{x}\right)}\right) \\
& =\left(\frac{x}{y}\right)\left(\frac{d y}{d x}\right) .
\end{aligned}
$$

- Suppose that the demand curve for a commodity is given by $Q=D(P)$. In this case, the own-price elasticity of demand for this commodity is given by

$$
\epsilon_{P}^{D}(P)=\left(\frac{P}{Q}\right)\left(\frac{d Q}{d P}\right)=\frac{P D^{\prime}(P)}{D(P)}
$$

- Suppose that the supply curve for a commodity is given by $Q=S(P)$. In this case, the own-price elasticity of supply for this commodity is given by

$$
\epsilon_{P}^{S}(P)=\left(\frac{P}{Q}\right)\left(\frac{d Q}{d P}\right)=\frac{P S^{\prime}(P)}{S(P)}
$$




## Unconstrained optimisation

Consider a function $f: X \longrightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}$. This function can be written as $f(x)$.

We are going to define the concepts of local maximum, local minimum, global maximum and global minimum for this function. We will then examine some techniques for finding such extrema.

Note that the open interval $(a, b)$ is the set

$$
\{x \in \mathbb{R}: a<x<b\} .
$$

If $(a, b) \subseteq X$, we can rewrite this set as

$$
\{x \in X: a<x<b\} .
$$

- The point $x_{0} \in X$ yields a **local maximum** of the function $f(x)$ if there exists some open interval $(a, b) \subset X$ that contains $x_{0}$ and for which $f\left(x_{0}\right) \geq f(x)$ for all $x \in(a, b)$.
- The point $x_{0} \in X$ yields a **local minimum** of the function $f(x)$ if there exists some open interval $(a, b) \subset X$ that contains $x_{0}$ and for which $f\left(x_{0}\right) \leq f(x)$ for all $x \in(a, b)$.
- The point $x_{0} \in X$ yields a **global maximum** of the function $f(x)$ if $f\left(x_{0}\right) \geq f(x)$ for all $x \in X$.
- The point $x_{0} \in X$ yields a **global minimum** of the function $f(x)$ if $f\left(x_{0}\right) \leq f(x)$ for all $x \in X$.


### Finding extrema of functions

There are a variety of rules that can be used to help identify the extrema of functions.  In this unit, we will mention two such rules.
- The first set of rules focus only on the first derivative of the function. We will call this approach the "first derivative approach".
- The second set of rules focus on both the first derivative of the function and the second derivative of the function. We will call this approach the "second derivative approach".


#### Some terminology

Consider the problem "Maximise the function $f(x)$ on the set $X$ ". This requires us to find a point $x_{0} \in X$ such that $f\left(x_{0}\right) \geq f(x)$ for all $x \in X$.

The solution to the problem is the maximum value obtained by the function over the set $X$. This is given by $f\left(x_{0}\right)$ and is known as a (maximum) value function. Formally, the value function for this problem is defined as

$$
f\left(x_{0}\right) \equiv \max \{f(x): x \in X\}
$$

The value (or possibly values) of $x$ that solve this optimisation problem are sometimes known as "arg max"s because they are the arguments that yield a maximum value of the objective function on the constraint set. Formally, if $f\left(x_{0}\right) \geq f(x)$ for all $x \in X$, then

$$
x_{0} \in \arg \max \{f(x): x \in X\}
$$

If there is a unique value of $x$ that solves the constrained maximisation problem, then we would have

$$
x_{0}=\arg \max \{f(x): x \in X\} .
$$


Similarly, consider the problem "Minimise the function $f(x)$ on the set $X$ ". This requires us to find a point $x_{0} \in X$ such that $f\left(x_{0}\right) \leq f(x)$ for all $x \in X$.

The solution to the problem is the minimum value obtained by the function over the set $X$. This is given by $f\left(x_{0}\right)$ and is known as a (minimum) value function. Formally, the value function for this problem is defined as

$$
f\left(x_{0}\right) \equiv \min \{f(x): x \in X\}
$$


The value (or possibly values) of $x$ that solve this optimisation problem are sometimes known as "arg min"s because they are the arguments that yield a minimum value of the objective function on the constraint set.  Formally, if $f\left(x_{0}\right) \leq f(x)$ for all $x \in X$, then

$$
x_{0} \in \arg \min \{f(x): x \in X\} .
$$

If there is a unique value of $x$ that solves the constrained minimisation problem, then we would have

$$
x_{0}=\arg \min \{f(x): x \in X\} .
$$


### Critical points of a function

Consider a function $f: X \longrightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}$. This function can be written as $f(x)$.

The point $x_{0} \in X$ is a critical point for the function $f(x)$ if either

$$
f^{\prime}(x)=0 \text { when } x=x_{0}
$$

or

$$f^{\prime}(x) \text{ is undefined when } x=x_{0}$$

Critical points are important because they are sometimes a local extremum of a function. Note, however, that a critical point of a function will not necessarily yield a local extremum for that function. In other words, being a critical point is a necessary, but not a sufficient, condition for that point to yield an extremum of the function.


#### Examples

```{dropdown} Example 1

Let $f(x)=x^{n}$, where $n \in \mathbb{N}$ and $n>1$. Then $f^{\prime}(x)=n x^{n-1}$. 

Clearly $n x^{n-1}$ is defined for all $x \in \mathbb{R}$. Note that $n x^{n-1}=0$ if and only if $x=0$. 

Thus the only critical point for this function occurs when $x=0$.
```

```{dropdown} Example 2

Let $f(x)=\ln (x)$, where $x \in(0, \infty)$. Then $f^{\prime}(x)=\frac{1}{x}$. Note that $\frac{1}{x}$ is not defined when $x=0$. 

However, the point $x=0$ is NOT a critical value of the function $\ln (x)$. The reason for this is that the point $x=0$ does not belong the domain of this function, which is the open interval $(0, \infty)$. 

Furthermore, we know that $\frac{1}{x} \neq 0$ for any choice of $x$. 

Thus we can conclude that there are no critical values for the function $\ln (x)$.
```


```{dropdown} Example 3

Consider the function

$$
f(x)=\left\{\begin{array}{cc}
2 x & \text { if } x \leq 1 \\
\frac{1}{2} x+\frac{3}{2} & \text { if } x \geq 1
\end{array}\right.
$$

- There is no problem with this double definition at the point $x=1$ because the two parts of the function are equal at that point.
- We have already shown that this function is continuous on $\mathbb{R}$ but that it is not differentiable at the point $x=1$.
- On the interval $(-\infty, 1)$, we have $f^{\prime}(x)=2 \neq 0$. Thus there are no critical points of this function on that interval.
- On the interval $(1, \infty)$, we have $f^{\prime}(x)=\frac{1}{2} \neq 0$. As such, there are no critical points of this function on that interval either.

However, $f^{\prime}(x)$ when $x=1$. Since the point $x=1$ belongs to the domain of this function, we know that it is a critical point of the function.  Thus we can conclude that the only critical point of this function occurs when $x=1$.
```



### The first derivative approach to optimisation

- Suppose that (i) $x_{0} \in(a, b) \subseteq X \subseteq \mathbb{R}$, (ii) $x_{0}$ is a critical point of $X$, (iii) $f^{\prime}(x)>0$ for all $x \in\left(a, x_{0}\right)$ and (iv) $f^{\prime}(x)<0$ for all $x \in\left(x_{0}, b\right)$. Then the point $x=x_{0}$ will yield a maximum of the function $f(x)$ on the interval $(a, b)$. If $(a, b) \subset X$, we can only conclude that $f\left(x_{0}\right)$ is a local maximum of the function. If $(a, b)=X$, we can conclude that $f\left(x_{0}\right)$ is a global maximum of the function.
- Suppose that (i) $x_{0} \in(a, b) \subseteq X \subseteq \mathbb{R}$, (ii) $x_{0}$ is a critical point of $X$, (iii) $f^{\prime}(x)<0$ for all $x \in\left(a, x_{0}\right)$ and (iv) $f^{\prime}(x)>0$ for all $x \in\left(x_{0}, b\right)$. Then the point $x=x_{0}$ will yield a minimum of the function $f(x)$ on the interval $(a, b)$. If $(a, b) \subset X$, we can only conclude that $f\left(x_{0}\right)$ is a local minimum of the function. If $(a, b)=X$, we can conclude that $f\left(x_{0}\right)$ is a global minimum of the function.


### The second derivative approach to optimisation

The second derivative approach to unconstrained optimisation problems makes use of both the first derivative of a function and the second derivative of a function. The first derivative component of this approach is known as the **first-order condition**. The second derivative component of this approach is known as the **second-order condition**.
- The point $x_{0} \in X \subseteq \mathbb{R}$ yields a local maximum of the function $f: X \longrightarrow \mathbb{R}$ if both (i) $f^{\prime}\left(x_{0}\right)=0$ and (ii) $f^{\prime \prime}\left(x_{0}\right)<0$.
- The point $x_{0} \in X \subseteq \mathbb{R}$ yields a local minimum of the function $f: X \longrightarrow \mathbb{R}$ if both (i) $f^{\prime}\left(x_{0}\right)=0$ and (ii) $f^{\prime \prime}\left(x_{0}\right)>0$.
- Note that if both (i) $f^{\prime}\left(x_{0}\right)=0$ and (ii) $f^{\prime \prime}\left(x_{0}\right)=0$, then we might not be able to use the second derivative approach to solve the problem.


### Example

This is an extension of Worked Example 4.9 in {cite:ps}`bradley2008` (pp. 162-163).
Consider a monopolist that produces a single good. Suppose that the demand function for this good is $Q=65-5 P$. The monopolist has a fixed cost of $\$ 30$ and a constant marginal cost of $\$ 2$ per unit of the good that is produced.  We have previously shown that the profit function for this monopolist (assuming that it cannot price discriminate) is

$$
\pi(Q)=-\left(\frac{1}{5}\right) Q^{2}+11 Q-30
$$

The first derivative of the profit function for this monopolist is

$$
\pi^{\prime}(Q)=-\left(\frac{2}{5}\right) Q+11
$$

The second derivative of the profit function for this monopolist is

$$
\pi^{\prime \prime}(Q)=-\left(\frac{2}{5}\right)
$$

The first-order condition for profit maximisation by this monopolist is

$$
\begin{aligned}
\pi^{\prime}(Q) & =0 \\
\Longleftrightarrow-\left(\frac{2}{5}\right) Q+11 & =0 \\
\Longleftrightarrow \quad 11 & =\left(\frac{2}{5}\right) Q \\
\Longleftrightarrow \quad 55 & =2 Q \\
\Longleftrightarrow \quad Q_{0} & =\frac{55}{2} .
\end{aligned}
$$

Note that

$$
\pi^{\prime \prime}(Q)=-\left(\frac{2}{5}\right) \text { for all } Q \in[0, \infty)
$$

Thus the second-order condition for profit maximisation by this monopolist is satisfied. Indeed, since the second order condition is satisfied for all possible values of $Q$ and not just for our candidate value of $Q$, we can conclude that $Q_{0}=\frac{55}{2}$ will yield a global profit maximum for the monopolist.


The maximum amount of profit that this monopolist can earn is

$$
\begin{aligned}
\pi^{*} & =\pi\left(Q_{0}\right) \\
& =\pi\left(\frac{55}{2}\right) \\
& =-\left(\frac{1}{5}\right) Q_{0}^{2}+11 Q_{0}-30 \\
& =-\left(\frac{1}{5}\right)\left(\frac{55}{2}\right)^{2}+11\left(\frac{55}{2}\right)-30 \\
& =-(0.2)(756.25)+11(27.5)-30 \\
& =151.25+302.5-30 \\
& =\$ 423.75 .
\end{aligned}
$$


### Profit maximisation in general

Suppose that the profits earned by a firm are given by the function

$$
\pi(Q)=R(Q)-C(Q)
$$

where $R(Q)$ is the firm's total revenue as a a function of output and $C(Q)$ is the firm's total cost as a function of output.

We want to consider the firm's profit maximisation problem. We will employ the first-derivative approach to solving this problem.


Suppose that profit as a function of output $(\pi(Q))$ is at least twice continuously differentiable. This means that any critical point for this function will involve its first derivative equalling zero.

Suppose that a unique critical point of this variety exists. It will involve $\pi^{\prime}(Q)=0$. Note that

$$
\begin{aligned}
\pi^{\prime}(Q) & =R^{\prime}(Q)-C^{\prime}(Q) \\
& =M R(Q)-M C(Q)
\end{aligned}
$$

Thus the critical point will occur when

$$
M R(Q)-M C(Q)=0
$$

which requires that

$$
M R(Q)=M C(Q)
$$

Denote this critical point by $Q_{0}$. A sufficient condition for $Q_{0}$ to generate the maximum possible profit for the firm is that both

$$
\text { (i) } \pi^{\prime}(Q)>0 \text { for all } Q<Q_{0}
$$

and

$$
\text { (ii) } \pi^{\prime}(Q)<0 \text { for all } Q>Q_{0} \text {. }
$$

Note that

$$
\begin{aligned}
\pi^{\prime}(Q)>0 & \Longleftrightarrow M R(Q)-M C(Q)>0 \\
& \Longleftrightarrow M R(Q)>M C(Q)
\end{aligned}
$$

and

$$
\begin{aligned}
\pi^{\prime}(Q)<0 & \Longleftrightarrow M R(Q)-M C(Q)<0 \\
& \Longleftrightarrow M R(Q)<M C(Q) .
\end{aligned}
$$

Thus a sufficient set of conditions for $Q_{0}$ to generate the maximum possible profit for the firm is that

1. $M R\left(Q_{0}\right)=M C\left(Q_{0}\right)$,
2. $M R(Q)>M C(Q)$ for all $Q<Q_{0}$, and
3. $M R(Q)<M C(Q)$ for all $Q>Q_{0}$.


## Using derivatives to approximate things

In situations where functions are rather complicated (especially when they are non-linear and non-separable in the components that make use of the independent variable), it is sometimes convenient to approximate the function, or the change in the value generated by the function, rather than calculate it directly. Obviously, we need to worry about the accuracy of the approximation when we do this.

Derivatives can be used to form some such approximations. In particular, they can be used to form the following approximations:
- Polynomial (Power Series) approximations of a function.
    - We will look at an example of this known as a Taylor series approximation of a function $f(x)$ around the point $x=x_{0}$.
    - The special case of a Taylor series approximation of the function $f(x)$ that takes place around the point $x=0$ is known as a McLauren series approximation of that function.
- We will also look at differential approximations to the change in the value taken by a function when there is a change in the value taken by the independent variable.



### Taylor series approximations of functions

Consider the function $f:(a, b) \longrightarrow \mathbb{R}$, where $-\infty<a<b<\infty$. Denote this function by $f(x)$.

Suppose that $f(x)$ is at least $n$-times differentiable with respect to $x$ on the interval $(a, b)$, and let $x_{0} \in(a, b)$.  The **$n$-th order Taylor series approximation** of $f(x)$ around the point $x=x_{0}$ is defined to be: 

$$
\begin{gathered}
f(x) \approx \sum_{k=0}^{n}\left(\frac{1}{k !}\right) f^{(k)}\left(x_{0}\right)\left(x-x_{0}\right)^{k} \\
\approx\left(\frac{1}{0 !}\right) f^{(0)}\left(x_{0}\right)\left(x-x_{0}\right)^{0}+\left(\frac{1}{1 !}\right) f^{(1)}\left(x_{0}\right)\left(x-x_{0}\right)^{1} \\
+\left(\frac{1}{2 !}\right) f^{(2)}\left(x_{0}\right)\left(x-x_{0}\right)^{2}+\cdots+\left(\frac{1}{n !}\right) f^{(n)}\left(x_{0}\right)\left(x-x_{0}\right)^{n} \\
\approx\left(\frac{1}{1}\right) f\left(x_{0}\right)+\left(\frac{1}{1}\right) f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\left(\frac{1}{2}\right) f^{\prime \prime}\left(x_{0}\right)\left(x-x_{0}\right)^{2} \\
+\cdots+\left(\frac{1}{n !}\right) f^{(n)}\left(x_{0}\right)\left(x-x_{0}\right)^{n} \\
\approx f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\left(\frac{1}{2}\right) f^{\prime \prime}\left(x_{0}\right)\left(x-x_{0}\right)^{2} \\
+\cdots+\left(\frac{1}{n !}\right) f^{(n)}\left(x_{0}\right)\left(x-x_{0}\right)^{n} 
\end{gathered}
$$

- A **linear (or first-order) Taylor series approximation** of a function would take the form

$$
f(x) \approx f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right) 
$$

- A **quadratic (or second-order) Taylor series approximation** of a function would take the form

$$
f(x) \approx f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\left(\frac{1}{2}\right) f^{\prime \prime}\left(x_{0}\right)\left(x-x_{0}\right)^{2}
$$

- If the function was differentiable an infinite number of times, then we could imagine forming the **$\infty$-order Taylor series approximation** of it around the point $x=x_{0}$. This would take the form

$$
f(x) \approx \sum_{k=0}^{\infty}\left(\frac{1}{k !}\right) f^{(k)}\left(x_{0}\right)\left(x-x_{0}\right)^{k}
$$



It is important to remember that Taylor series approximations of functions are just that: An approximation of the function.

The accuracy of the approximation will depend on a number of factors. These include the form of the underlying function, the closeness of the independent variable $(x)$ to the value around which the Taylor series approximation is centred $\left(x_{0}\right)$, and the order of the approximation.

In many cases, the Taylor series approximation of a function $f(x)$ that is centred on the point $x=x_{0}$ will be a reasonably accurate approximation of the function in a sufficiently small neighbourhood of the point $x=x_{0}$.

Nonetheless, it is important to recognise that we will usually have

$$
f(x) \approx\left(\sum_{k=0}^{n}\left(\frac{1}{k !}\right) f^{(k)}\left(x_{0}\right)\left(x-x_{0}\right)^{k}\right)+R_{n+1}(x)
$$

where $R_{n+1}(x)$ is a "remainder" term.


- The $n$ th-order Taylor series approximation of the function $f(x)$ around the point $x=x_{0}$ is given by

$$
f(x) \approx \sum_{k=0}^{n}\left(\frac{1}{k !}\right) f^{(k)}\left(x_{0}\right)\left(x-x_{0}\right)^{k}
$$


If the function being approximated is at least $(n+1)$-times differentiable on an interval that includes both the point $x=x_{0}$ and the point $x$, then Lagrange has shown that the remainder term that reconciles the above Taylor series approximation of $f(x)$ with the value taken by the function itself is exactly given by the expression

$$
R_{n+1}(x)=\left(\frac{1}{(n+1) !}\right) f^{(n+1)}(z)\left(x-x_{0}\right)^{n+1}
$$

for some $z$ between $x$ and $x_{0}$. (If $x<x_{0}$, then $z \in\left(x, x_{0}\right)$. If $x>x_{0}$, then $z \in\left(x_{0}, x\right)$.)


### Differential approximation to a change in a function's value

Consider a function $y=f(x)$ that is at least once differentiable on some non-empty interval $(a, b)$.  Suppose that the value of the independent variable changes from some initial value $x \in(a, b)$ to some final value $x+\Delta x \in(a, b)$.

The exact impact of this change on the value taken by the function is $\Delta y=f(x+\Delta x)-f(x)$.

We might think about calculating an approximation of this change by using a linear (first-order) Taylor series of the function $f(x+\Delta x)$ around the point $x$. This approximation takes the form

$$
f(x+\Delta x) \approx f(x)+f^{\prime}(x)((x+\Delta x)-x)=f(x)+f^{\prime}(x) \Delta x
$$

If we employ this approximation, then we obtain

$$
\Delta y \approx f(x)+f^{\prime}(x) \Delta x-f(x)=f^{\prime}(x) \Delta x
$$

Recall that in many cases, if $x+\Delta x$ is sufficiently close to $x$, then the linear Taylor series approximation for $f(x+\Delta x)$ around the point $x$ will be reasonably accurate. If this is the case, then the linear (or first-order) approximation to the change in the value of the function $f(x)$ when the independent variable moves from $x$ to $x+\Delta x$ that we derived above will also be reasonably accurate when $x+\Delta x$ is sufficiently close to $x$.

In other words, it will be reasonably accurate when $\Delta x$ is sufficiently small in absolute value terms (that is, close to zero). In such cases, we typically use $d x$ to denote $\Delta x$, and $d y$ to denote $\Delta y$. This means that if $f(x)$ is at least once differentiable and $d x$ bis sufficiently small, then

$$
d y \approx f^{\prime}(x) d x
$$

This expression is known as the linear (or first-order) **differential** of the function $f(x)$.


### An application of linear differential approximation

The change in the natural logarithm of a variable is approximately equal to the corresponding proprtionate change in that variable. As such, we can interpret the change in the natural logarithm of the price level in an economy as a good indicator of inflation in that economy (as long as the change in the price level is not too large). This can be seen by noting that if $p=\ln (P)$, where $P$ is the price level in an economy, then

$$
d p \approx\left(\frac{d \ln (P)}{d P}\right) d P \Longleftrightarrow d p \approx\left(\frac{1}{P}\right) d P \Longleftrightarrow d p \approx \frac{d P}{P}
$$


Ted Sieper places the natural log of the price level on the horizontal axis of a diagrammatic representation of a version of the "IS-LM" macroeconomic model, thereby enabling him to use the above result to interpret horizontal distances in that diagram as a measure of inflation (or expected inflation).  This is a very interesting and insightful paper that is well worth a read at some stage during your study of economics.

See Sieper, E (1987), "Policy irrelevance is not the rule", Unpublished working paper, ANU, Canberra, November. Available online at: https://files.brontecapital.com/ted_sieper_paper.pdf.



### L'Hopital's rule for limits of indeterminate form

Consider the function $f(x)=\frac{g(x)}{h(x)}$. Suppose that $g(x)$ and $h(x)$ are both differentiable at all points on a non-empty interval $(a, b)$ that contains the point $x=x_{0}$, with the possible exception that they might not be differentiable at the point $x_{0}$.

Suppose also that $h^{\prime}(x) \neq 0$ for all $x \in(a, b)$.  

- If $\lim _{x \rightarrow x_{0}} g(x)=\lim _{x \rightarrow x_{0}} h(x)=0$ and $\lim _{x \rightarrow x_{0}}\left(\frac{g^{\prime}(x)}{h^{\prime}(x)}\right)=L$, then according to L'Hopital's rule for " $\left(\frac{0}{0}\right)$ " indeterminate forms, we have

$$
\lim _{x \rightarrow x_{0}} f(x)=\lim _{x \rightarrow x_{0}}\left(\frac{g(x)}{h(x)}\right)=\lim _{x \rightarrow x_{0}}\left(\frac{g^{\prime}(x)}{h^{\prime}(x)}\right)=L
$$

- If $\lim _{x \rightarrow x_{0}} g(x)= \pm \infty$, $\lim _{x \rightarrow x_{0}} h(x)= \pm \infty$ and $\lim _{x \rightarrow x_{0}}\left(\frac{g^{\prime}(x)}{h^{\prime}(x)}\right)=L$, then according to L'Hopital's rule for " $\left(\frac{ \pm \infty}{ \pm \infty}\right)$ " indeterminate forms, we have

$$
\lim _{x \rightarrow x_{0}} f(x)=\lim _{x \rightarrow x_{0}}\left(\frac{g(x)}{h(x)}\right)=\lim _{x \rightarrow x_{0}}\left(\frac{g^{\prime}(x)}{h^{\prime}(x)}\right)=L
$$