# üìñ Univariate differentiation

<small>‚è± <span class="eta"></span> | <span class="words"></span> words</small>

## Sources and reading guide

````{dropdown} Sources and reading guide

```{figure} _static/img/bibliography/shsc2016.png
:width: 100px
:align: left
```
{cite:ps}`sydsaeter2016`

Chapters 6, 7, and 8.

<div style="clear: both"></div>

Introductory level references: 
- {cite:ps}`bradley2008`: Chapter 6 (pp. 257‚Äì357).
- {cite:ps}`haeussler1987`: Chapters 10, 11, 12, and 13 (pp. 375‚Äì532).
- {cite:ps}`shannon1995`: Chapter 8 (pp. 356‚Äì407).


More advanced references:
- {cite:ps}`ayres2013`
- {cite:ps}`chiang2005`: Chapters 6‚Äì10 (pp. 124‚Äì290).
- {cite:ps}`kline1967`: Chapters 2, 4, 5, 6, 7, 12, 13, and 20.
- {cite:ps}`silverman1969`: Chapters 4, 5, and 6.
- {cite:ps}`simon1994`: Chapters 2‚Äì5 (pp. 10‚Äì103).
- {cite:ps}`spivak2006`: Chapters 3‚Äì12 (pp. 39‚Äì249).
````




## Limit for functions

Consider a function, $f: \mathbb{R} \rightarrow \mathbb{R}$. This function can be written as $f(x)$. Note that $x \in \mathbb{R}$ here. Furthermore, for each point $x_0 \in \mathbb{R}$, we have $f(x_0) \in \mathbb{R}$ as well. In other words, the domain of $f$ is $\mathbb{R}$, while the range of $f$ is a (possibly improper) subset of $\mathbb{R}$.

{cite:ps}`spivak2006` (p. 90) provides the following informal definition of a limit: ‚ÄúThe function $f$ approaches the limit $l$ near [the point $x =$] $a$,
if we can make $f(x)$ as close as we like to $l$ by requiring that $x$ be sufficiently close to, but unequal to, $a$.‚Äù

{cite:ps}`spivak2006` (p. 96) provides the following formal definition of a limit: ‚ÄúThe function $f$ **approaches the limit $l$ near** [the point $x =$] $a$ means: for every $\epsilon > 0$, there is some $\delta > 0$ such that, for all $x$, if 
$0 < |x ‚àí a| < \delta$, then $|f (x ) ‚àí l | < \epsilon$.‚Äù. (Bold in the original.)

The formal definition provided by Spivak is known as an **epsilon-delta** ($\epsilon-\delta$) definition of the limit concept. It could also be written as follows:

The limit of $f$ as $x$ approaches $a$ is $l$ if, for each $\epsilon > 0$, there exists some $\delta_\epsilon > 0$ such that

$$ 0 < d (x, a) < \delta_\epsilon \Rightarrow d (f (x ) , l ) < \epsilon $$

Suppose that the limit of $f$ as $x$ approaches $a$ is $l$. We can write this as

$$ \lim_{x  \rightarrow a} f (x ) = l $$


```{figure} _static/img/lecture_06/epsilon_delta.png
:width: 80%
:align: center

An illustration of an epsilon-delta limit argument
```

### The structure of $\epsilon‚Äì\delta$ arguments

Suppose that we want to attempt to show that $\lim _{x \rightarrow a} f(x)=b$.

In order to do this, we need to show that, for any choice $\epsilon>0$, there exists some $\delta_{\epsilon}>0$ such that, whenever $|x-a|<\delta_{\epsilon}$, it is the case that $|f(x)-b|<\epsilon$.
- We write $\delta_{\epsilon}$ to indicate that the choice of $\delta$ is allowed to vary with the choice of $\epsilon$.

An often fruitful approach to the construction of a formal $\epsilon-\delta$ limit argument is to proceed as follows: 
1. Start with the end-point that we need to establish: $|f(x)-b|<\epsilon$.
2. Use appropriate algebraic rules to rearrange this "final" inequality into something of the form $|k(x)(x-a)|<\epsilon$.
3. This new version of the required inequality can be rewritten as $|k(x)||(x-a)|<\epsilon$.
4. If $k(x)=k$, a constant that does not vary with $x$, then this inequality becomes $|k||x-a|<\epsilon$. In such cases, we must have $|x-a|<\frac{\epsilon}{|k|}$, so that an appropriate choice of $\delta_{\epsilon}$ is $\delta_{\epsilon}=\frac{\epsilon}{|k|}$.
5. If $k(x)$ does vary with $x$, then we have to work a little bit harder.

Suppose that $k(x)$ does vary with $x$. How might we proceed in that case? One possibility is to see if we can find a restriction on the range of values for $\delta$ that we consider that will allow us to place an upper bound on the value taken by $|k(x)|$.

In other words, we try and find some restriction on $\delta$ that will ensure that $|k(x)|<K$ for some finite $K>0$. The type of restriction on the values of $\delta$ that you choose would ideally look something like $\delta<D$, for some fixed real number $D>0$. (The reason for this is that it is typically small deviations of $x$ from a that will cause us problems rather than large deviations of $x$ from a.)

If $0<|k(x)|<K$ whenever $0<\delta<D$, then we have

$$
|k(x)||x-a|<\epsilon \Longleftrightarrow K|x-a|<\epsilon \Longleftrightarrow|x-a|<\frac{\epsilon}{K} .
$$

In such cases, an appropriate choice of $\delta_{\epsilon}$ is $\delta_{\epsilon}=\min \left\{\frac{\epsilon}{K}, D\right\}$.


### Some useful rules about limits

- In practice, we would like to be able to find at least some limits without having to resort to the formal "epsilon-delta" arguments that define them. The following rules can sometimes assist us with this.
- Let $c \in \mathbb{R}$ be a fixed constant, $a \in \mathbb{R}, \alpha \in \mathbb{R}, \beta \in \mathbb{R}, n \in \mathbb{N}$, $f: \mathbb{R} \longrightarrow \mathbb{R}$ be a function for which $\lim _{x \rightarrow a} f(x)=\alpha$, and $g: \mathbb{R} \longrightarrow \mathbb{R}$ be a function for which $\lim _{x \rightarrow a} g(x)=\beta$.
- The following rules apply for limits.
- $\lim _{x \rightarrow a} c=c$ for any $a \in \mathbb{R}$.
- $\lim _{x \rightarrow a}(c f(x))=c\left(\lim _{x \rightarrow a} f(x)\right)=c \alpha$.
- $\lim _{x \rightarrow a}(f(x)+g(x))=\left(\lim _{x \rightarrow a} f(x)\right)+\left(\lim _{x \rightarrow a} g(x)\right)=\alpha+\beta$.
- $\lim _{x \rightarrow a}(f(x) g(x))=\left(\lim _{x \rightarrow a} f(x)\right)\left(\lim _{x \rightarrow a} g(x)\right)=\alpha \beta$.
- $\lim _{x \rightarrow a}\left(\frac{f(x)}{g(x)}\right)=\frac{\lim _{x \rightarrow a} f(x)}{\lim _{x \rightarrow a} g(x)}=\frac{\alpha}{\beta}$ whenever $\beta \neq 0$.
- $\lim _{x \rightarrow a} \sqrt[n]{f(x)}=\sqrt[n]{\lim _{x \rightarrow a} f(x)}=\sqrt[n]{\alpha}$ whenever $\sqrt[n]{\alpha}$ is defined.
- After we have learned about differentiation, we will learn about another useful rule for limits that is known as "L'Hopital's rule".


```{admonition} Example
:class: tip

- This is an example in which the chosen function will be very nicely behaved at all points $x \in \mathbb{R}$.
- Consider the function $f(x)=x$ and the point $x=2$. We will show formally that $\lim _{x \rightarrow 2} f(x)=2$.
- Note that

$$
d(f(x), 2)=|x-2|=d(x, 2) .
$$

- Let $\epsilon>0$. We have

$$
d(f(x), 2)<\epsilon \Longleftrightarrow|x-2|<\epsilon
$$

- Suppose that we let $\delta_{\epsilon}=\epsilon$. Since $\epsilon>0$, we must have $\delta_{\epsilon}>0$.
- Consider points in $\mathbb{R}$ that satisfy the condition

$$
0<d(x, 2)<\delta_{\epsilon} 
$$

- Consider points in $\mathbb{R}$ that satisfy the condition

$$
0<d(x, 2)<\delta_{\epsilon}
$$

- Note that

$$
\begin{aligned}
0<d(x, 2)<\delta_{\epsilon} & \Longleftrightarrow 0<|x-2|<\delta_{\epsilon} \\
& \Longleftrightarrow 0<d(f(x), 2)<\delta_{\epsilon}
\end{aligned}
$$

- Thus we have

$$
0<d(x, 2)<\delta_{\epsilon} \Longrightarrow d(f(x), 2)<\delta_{\epsilon} \text {. }
$$

- Since we chose $\epsilon>0$ arbitrarily, this must be true for all $\epsilon>0$. This means that $\lim _{x \rightarrow 2} f(x)=2$.
```



```{admonition} Example
:class: tip

- This example is drawn from Willis Lauritz Peterson of the University of Utah.
- Consider the mapping $f: \mathbb{R} \longrightarrow \mathbb{R}$ defined by $f(x)=7 x-4$. We want to show that $\lim _{x \rightarrow 2} f(x)=10$.
- Note that $|f(x)-10|=|7 x-4-10|=|7 x-14|=|7(x-2)|=$ $|7||x-2|=7|x-2|$.
- We require $|f(x)-10|<\epsilon$. Note that

$$
|f(x)-10|<\epsilon \Longleftrightarrow 7|x-2|<\epsilon \Longleftrightarrow|x-2|<\frac{\epsilon}{7}
$$

- Thus, for any $\epsilon>0$, if $\delta_{\epsilon}=\frac{\epsilon}{7}$, then $|f(x)-10|<\epsilon$ whenever $|x-2|<\delta_{\epsilon}$.
```


```{admonition} Example
:class: tip

- Consider the mapping $f: \mathbb{R} \longrightarrow \mathbb{R}$ defined by $f(x)=x^{2}$. We want to show that $\lim _{x \rightarrow 2} f(x)=4$.
- Note that $|f(x)-4|=\left|x^{2}-4\right|=|(x+2)(x-2)|=|x+2||x-2|$.
- Suppose that $|x-2|<\delta$, which in turn means that $(2-\delta)<x<(2+\delta)$. Thus we have $(4-\delta)<(x+2)<(4+\delta)$.
- Let us restrict attention to $\delta \in(0,1)$. This gives us $3<(x+2)<5$, so that $|x+2|<5$.
- Thus, when $|x-2|<\delta$ and $\delta \in(0,1)$, we have $|f(x)-4|=|x+2||x-2|<5 \delta$.
- We require $|f(x)-4|<\epsilon$. One way to ensure this is to set $\delta_{\epsilon}=\min \left(1, \frac{\epsilon}{5}\right)$.

```


```{admonition} Example
:class: tip

This example is drawn from Willis Lauritz Peterson of the University of Utah.

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x) = x^2 ‚àí 3x + 1$.

We want to show that $lim_{x \rightarrow 2} f(x ) = ‚àí1$.

- Note that $|f(x) ‚àí (‚àí1)| = |x^2 ‚àí 3x + 1 + 1| = |x^2 ‚àí 3x + 2| = |(x ‚àí 1)(x ‚àí 2)| = |x ‚àí 1||x ‚àí 2|$.
- Suppose that $|x ‚àí 2| < \delta$, which in turn means that $(2 ‚àí \delta) < x < (2 + \delta)$. Thus we have $(1 ‚àí \delta) < (x ‚àí 1) < (1 + \delta)$.
- Let us restrict attention to $\delta \in (0, 1)$. This gives us $0 < (x ‚àí 1) < 2$, so that $|x ‚àí 1| < 2$.
- Thus, when $|x ‚àí 2| < \delta$ and $\delta \in (0, 1)$, we have $|f (x ) ‚àí (‚àí1)| = |x ‚àí 1||x ‚àí 2| < 2\delta$.
- We require $|f (x ) ‚àí (‚àí1)| < \epsilon$. One way to ensure this is to set $\delta_\epsilon = \min(1, \frac{\epsilon}{2} )$.
```


```{admonition} Example
:class: tip

Limits can sometimes exist even when the function being considered is not so well behaved. One such example is provided by {cite:ps}`spivak2006` (pp. 91‚Äì92). It involves the use of a trigonometric function.

As we will not be considering such functions elsewhere in this unit, this particular example will not assessed in this unit. However, the fact that a function might have a limit at a point where it is not defined is worth noting.

The example involves the function $f: \mathbb{R} \setminus 0 \rightarrow \mathbb{R}$ that is defined by $f (x ) = x sin ( \frac{1}{x})$.

Clearly this function is not defined when $x = 0$. Furthermore, it can be shown that $\lim_{x  \rightarrow 0} sin ( \frac{1}{x})$ does not exist. However, it can also be shown that $lim_{x  \rightarrow 0} f (x ) = 0$.

The reason for this is that $sin (\theta) \in [‚àí1, 1]$ for all $\theta \in \mathbb{R}$. Thus $sin ( \frac{1}{x} )$ is bounded above and below by finite numbers as $x \rightarrow 0$. This allows the $x$ component of $x sin (\frac{1}{x})$ to dominate as $x \rightarrow 0$.
```

```{admonition} Example
:class: tip

Limits do not always exist. In this example, we consider a case in which the limit of a function as $x$ approaches a particular point does not exist.

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by

$$
f(x ) = \begin{cases} 
1 \quad \text{ if } x > 5; \\
0 \quad \text{ if } x < 5
\end{cases}
$$

We want to show that $\lim_{x  \rightarrow 5} f (x )$ does not exist.

Suppose that the limit does exist. Denote the limit by $l$. Recall that $d (x, y ) = \{ (y ‚àí x )2 \}^{\frac{1}{2}} = |y ‚àí x |$. Let $\delta > 0$.

If $|5 ‚àí x | < \delta$, then $5 ‚àí \delta < x < 5 + \delta$, so that $x \in (5 ‚àí \delta, 5 + \delta)$. 

Note that $x \in (5 ‚àí \delta, 5 + \delta) = (5 ‚àí \delta, 5) \cup [5, 5 + \delta)$, where $(5 ‚àí \delta, 5) \ne \varnothing$ and $[5, 5 + \delta) \ne \varnothing$.

Thus we know the following:
1. There exist some $x \in (5 ‚àí \delta, 5) \subseteq (5 ‚àí \delta, 5 + \delta)$, so that $f (x ) = 0$ for some $x \in (5 ‚àí \delta, 5 + \delta)$.
2. There exist some $x \in [5, 5 + \delta) \subseteq (5 ‚àí \delta, 5 + \delta)$, so that $f (x ) = 1$ for some $x \in (5 ‚àí \delta, 5 + \delta)$.
3. The image set under $f$ for $(5 ‚àí \delta, 5 + \delta)$ is $f ((5 ‚àí \delta, 5 + \delta)) = \{ f (x ) : x \in (5 ‚àí \delta, 5 + \delta) \} = \{ 0, 1 \} \subseteq [0, 1]$.
4. Note that for any choice of $\delta > 0$, $[0, 1]$ is the smallest connected interval that contains the image set $f ((5 ‚àí \delta, 5 + \delta)) = \{ 0, 1 \}$.

Hence, in order for the limit to exist, we need $[0, 1] \subseteq (l ‚àí \epsilon, l + \epsilon)$
for all $\epsilon > 0$.  But for $\epsilon \in (0, \frac{1}{2})$, there is no $l \in \mathbb{R}$ for which this is the case.  Thus we can conclude that $\lim_{x  \rightarrow 5} f (x )$ does not exist.
```



## Continuity

Our focus here is on continuous real-valued functions of a single real variable. In other words, functions of the form $f : X \rightarrow \mathbb{R}$ where $X \subseteq \mathbb{R}$.

Before looking at the picky technical details of the concept of continuity, it is worth thinking about that concept intuitively. Basically, a real-valued function of a single real-valued variable is **continuous** if you could potentially draw its graph without lifting your pen from the page. In other words, there are no holes in the graph of the function and there are no jumps in the graph of the function.

Now for the slightly picky technical details!
- A function is continuous _at the point_ $x_0 \in X$ if $\lim_{x \rightarrow x_0} f (x ) = f (x_0)$.
- A function is continuous on a connected interval $(a, b) \subseteq X$ for which $a < b$ if it is continuous at all points in that interval. In other words, a function is continuous on $(a, b) \subseteq X$ for which $a < b$ if $\lim_{x \rightarrow x_0} f (x ) = f (x_0)$ for all $x_0 \in (a, b)$.
- A function is **continuous** if it is continuous at every point in its domain. In other words, a function is continuous if $\lim_{x \rightarrow x_0} f (x ) = f (x_0)$ for all $x_0 \in X$.


```{admonition} Definition
:class: caution

$f: A \to \mathbb{R}$ is called **continuous** if it is continuous at every $x \in
A$
```

```{figure} _static/img/lecture_06/cont_func.png
:name: discont_func
:width: 60%

Continuous function
```
```{admonition} Example
:class: tip

Function $f(x) = \exp(x)$ is continuous at $x=0$

**Proof:**

Consider any sequence $\{x_n\}$ which converges to $0$

We want to show that for any $\epsilon>0$ there exists $N$ such that $n \geq N \implies |f(x_n) - f(0)| < \epsilon$.  We have
:::{math}
\begin{array}{l}
|f(x_n) - f(0)| = |\exp(x_n) - 1| < \epsilon \quad \iff \\
\exp(x_n) - 1 < \epsilon \; \text{ and } \; \exp(x_n) - 1 > -\epsilon  \quad \iff \\
x_n < \ln(1+\epsilon) \; \text{ and } \; x_n > \ln(1-\epsilon) \quad \Longleftarrow \\
| x_n - 0 | < \min \big(\ln(1+\epsilon),\ln(1-\epsilon) \big) = \ln(1-\epsilon)
\end{array}
:::
Because due to $x_n \to x$ for any $\epsilon' = \ln(1-\epsilon)$ there exists $N$ such that $n \geq N \implies |x_n - 0| < \epsilon'$, we have $f(x_n) \to f(x)$ by definition.  Thus, $f$ is continuous at $x=0$.
```

```{admonition} Fact
:class: important

Some functions known to be continuous on their domains:
- $f: x \mapsto x^\alpha$
- $f: x \mapsto |x|$
- $f: x \mapsto \log(x)$
- $f: x \mapsto \exp(x)$
- $f: x \mapsto \sin(x)$
- $f: x \mapsto \cos(x)$

```

### Types of discontinuities

```{figure} _static/img/lecture_06/4-types-of-discontinuity.png
:name: cont_func
:width: 100%

4 common types of discontinuity
```

```{admonition} Example
:class: tip

The indicator function $x \mapsto \mathbb{1}\{x > 0\}$ has a jump discontinuity at $0$.

```


```{admonition} Fact
:class: important

Let $f$ and $g$ be functions and let $\alpha \in \mathbb{R}$

%
1. If $f$ and $g$ are continuous at $x$ then so is $f + g$,
where
:::{math}
(f + g)(x) := f(x) + g(x)
:::

2. If $f$ is continuous at $x$ then so is $\alpha f$, where
:::{math}
(\alpha f)(x) := \alpha f(x)
:::

3. If $f$ and $g$ are continuous at $x$ and real valued then so is
$f \circ g$, where
:::{math}
(f \circ g)(x) := f(x) \cdot g(x)
:::
In the latter case, if in addition $g(x) \ne 0$, then $f/g$ is also continuous.

```

````{admonition} Proof
:class: dropdown

Just repeatedly apply the properties of the limits

Let's just check that 
:::{math}
\text{$f$ and $g$ continuous at $x$}
\implies 
\text{$f + g$ continuous at $x$}
:::

Let $f$ and $g$ be continuous at $x$

Pick any $x_n \to x$

We claim that 
$f(x_n) + g(x_n) \to f(x) + g(x)$

By assumption, $f(x_n) \to f(x)$ and $g(x_n) \to g(x)$

From this and the triangle inequality we get
:::{math}
\| f(x_n) + g(x_n) - (f(x) + g(x)) \|
\leq 
:::
:::{math}
\leq 
\| f(x_n) - f(x) \|
+
\| g(x_n) - g(x) \|
\to 0
:::
````

As a result, set of continuous functions is "closed" under elementary
arithmetic operations

```{admonition} Example
:class: tip

The function $f \colon \mathbb{R} \to \mathbb{R}$ defined by
:::{math}
f(x) = \frac{\exp(x) + \sin(x)}{2 + \cos(x)} + \frac{x^4}{2}
- \frac{\cos^3(x)}{8!}
:::
is continuous (we just have to be careful to ensure that denominator is not zero -- which it is not for all $x\in\mathbb{R}$)
```

```{admonition} Example
:class: tip

An example of oscillating discontinuity is the function $f(x) = \sin(1/x)$ which is discontinuous at $x=0$.

[Video to illustrate this function](https://youtu.be/BayeWDEnXm8?si=255RAsYqZzY9mGvO)

```


**EDIT EDIT **


Now for the really picky technical details!

Let $X \subseteq \mathbb{R}^n$, $d^n$ be the $n$‚Äìdimensional Euclidean distance and $d$ be the one‚Äìdimensional Euclidean distance.

Suppose that $f : X \rightarrow \mathbb{R}$.
The mapping $f$ is said to be **continuous** at the point $x_0 \in X$ if, given any $\epsilon > 0$, there exists a $\delta > 0$ such that $d(f (x ), f (x_0)) < \epsilon$ whenever $d^n(x, x_0) < \delta$.

This is a formal version of the statement that $\lim_{x \rightarrow x_0} f (x ) = f (x_0)$.

```{figure} _static/img/lecture_06/continuity.png
:width: 80%
:align: center

Epsilon-delta continuity diagram
```

```{figure} _static/img/lecture_06/discontinuity.png
:width: 80%
:align: center

Epsilon-delta discontinuity diagram
```

```{admonition} Example
:class: tip

Consider the mapping $f : \mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x ) = x_2$. We want to show that $f(x)$ is continuous at the point $x = 2$.

Recall that $d (x, y ) = \{ (y ‚àí x )2 \}^{\frac{1}{2}} = |y ‚àí x |$.

Note that $|f (2) ‚àí f (x )| = |f (x ) ‚àí f (2)| = |x^2 ‚àí 4| = |x ‚àí 2||x + 2|$.

Suppose that $|2 ‚àí x | < \delta$. This means that $|x ‚àí 2| < \delta$, which in turn means that $(2 ‚àí \delta) < x < (2 + \delta)$. Thus we have $(4 ‚àí \delta) < (x + 2) < (4 + \delta)$.

Let us restrict attention to $\delta \in (0, 1)$. This gives us $3 < (x + 2) < 5$, so that $|x + 2| < 5$.

We have $|f (2) ‚àí f (x )| = |f (x ) ‚àí f (2)| = |x ‚àí 2||x + 2| < (\delta)(5) = 5\delta$.

We require $|f (2) ‚àí f (x )| < \epsilon$. One way to ensure this is to set $\delta = min(1, \frac{\epsilon}{5})$.
```

```{admonition} Example
:class: tip

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x ) = x$. We want to show that $f (x )$ is continuous for all $x \in X$.

Recall that $d (x, y ) = \{ (y ‚àí x )2 \}^{\frac{1}{2}} = |y ‚àí x |$.

Consider an arbitrary point $x = a$. Note that $|f (a) ‚àí f (x )| = |a ‚àí x |$.

Suppose that $|a ‚àí x | < \delta$.

Note that if we set $\delta = \epsilon$, we have $|f (a) ‚àí f (x )| < \epsilon$. 

Thus we know that $f$ is continuous at the point $x = a$. Since $a$ was chosen arbitrarily, we now that this is true for all $a \in \mathbb{R}$. This means that $f$ is continuous for all $x \in \mathbb{R}$.
```


```{admonition} Example
:class: tip

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by 

$$
f(x ) =
\begin{cases} 
1 \quad \text{ if } x > 5; \\
0 \quad \text{ if } x < 5
\end{cases}
$$

We have already shown that $\lim_{x  \rightarrow 5} f (x )$ does not exist. This means that it is impossible for $lim_{x  \rightarrow 5} f (x ) = f (5)$. As such, we know that $f (x )$ is not continuous at the point $x = 5$.

This means that f (x ) is not a continuous function. 

However, it can be shown that (i) $f(x )$ is continuous on the interval $(‚àí\infty, 5)$, and that (ii) $f (x )$ is continuous on the interval $(5, \infty)$. Can
you explain why this is the case?
```


```{admonition} Example
:class: tip

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by 

$$
f (x ) = \begin{cases}
\frac{1}{x} \quad \text{ if } x \ne 0; \\
0 \quad \text{ if } x = 0.
\end{cases}
$$
This function is a rectangular hyperbola when $x \ne 0$, but it takes on the value $0$ when $x = 0$. Recall that the rectangular hyperbola part of this function is not defined at the point $x = 0$.

This function is discontinuous at the point $x = 0$. *Illustrate the graph of this function on the whiteboard.*
```


```{admonition} Example
:class: tip

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by

$$
f (x ) = \begin{cases} 
1 \quad \text{ if } x \in Q; \\
0 \quad \text{ if } x \notin Q
\end{cases}
$$
This function is sometimes known as Dirichlet‚Äôs discontinuous function. It is discontinuous at every point in its domain.
```





## Derivatives

If it exists, the derivative of a function tells us the direction and magnitude of the change in the dependent variable that will be induced by a very small increase in the independent variable.

If it exists, the derivative of a function at a point is equal to the slope of the straight line that is tangent to the function at that point. 

You should try and guess now what the derivative of a linear function of the form $f (x ) = ax + b$ will be.

### Definition

Consider a function $f : X \rightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}$.
Let $(x_0, f (x_0))$ and $(x_0 + h, f (x_0 + h))$ be two points that lie on the graph of this function.

Draw a straight line between these two points. The slope of this line is 

$$
\begin{align*}
\frac{\text{rise}}{\text{run}} &= \frac{y_2 ‚àí y_1}{x_2 ‚àí x_1} \\
&= \frac{f (x_0 + h) ‚àí f (x_0)}{x_0 + h ‚àí x_0} \\
&= \frac{f (x_0 + h) ‚àí f (x_0)}{h}
\end{align*}
$$

What happens to this slope as $h \rightarrow 0$ (that is, as the second point
gets very close to the first point?


The **derivative** of the function $f(x)$ at the point $x=x_{0}$, if it exists, is defined to be

$$
f^{\prime}\left(x_{0}\right)
=\left.\frac{d f}{d x}\right|_{x=x_{0}}
=\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h} .
$$

This is simply the slope of the straight line that is tangent to the function $f(x)$ at the point $x=x_{0}$.

We will now proceed to use this definition to find the derivative of some simple functions. This is sometimes called "finding the derivative of a function from first principles".

```{admonition} Example: the (first) derivative of a constant function
:class: tip

Consider the function $f: X \longrightarrow \mathbb{R}$ defined by $f(x)=b$, where $b \in \mathbb{R}$ is a constant.
Clearly we have $f\left(x_{0}\right)=f\left(x_{0}+h\right)=b$ for all choices of $x_{0}$ and $h$.

Thus we have

$$
\frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\frac{b-b}{h}=\frac{0}{h}=0
$$

for all choices of $x_{0}$ and $h$.

This means that

$$
\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\lim _{h \rightarrow 0} 0=0
$$

As such, we can conclude that

$$
f^{\prime}(x)=\frac{d f}{d x}=0
$$

```

```{admonition} Example: the (first) derivative of a linear function
:class: tip

Consider the function $f: X \longrightarrow \mathbb{R}$ defined by $f(x)=a x+b$.  Note that

$$
\begin{aligned}
f(x+h) & =a(x+h)+b \\
& =a x+a h+b \\
& =a x+b+a h \\
& =f(x)+a h .
\end{aligned}
$$

Thus we have

$$
\frac{f(x+h)-f(x)}{h}=\frac{f(x)+a h-f(x)}{h}=\frac{a h}{h}=a .
$$

This means that $\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\lim _{h \rightarrow 0} a=a$.

As such, we can conclude that $f^{\prime}(x)=\frac{d f}{d x}=a$.
```

```{admonition} Example: the (first) derivative of a quadratic power function
:class: tip

Consider the function $f: X \longrightarrow \mathbb{R}$ defined by $f(x)=x^{2}$.  Note that

$$
\begin{aligned}
f(x+h) & =(x+h)^{2} \\
& =x^{2}+2 x h+h^{2} \\
& =f(x)+2 x h+h^{2}
\end{aligned}
$$

Thus we have

$$
\begin{aligned}
\frac{f(x+h)-f(x)}{h} & =\frac{f(x)+2 x h+h^{2}-f(x)}{h} \\
& =\frac{2 x h+h^{2}}{h} \\
& =2 x+h .
\end{aligned}
$$

This means that $\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\lim _{h \rightarrow 0}(2 x+h)=2 x$.

As such, we can conclude that $f^{\prime}(x)=\frac{d f}{d x}=2 x$
```


```{admonition} Example: the (first) derivative of a quadratic polynomial function
:class: tip

Consider the function $f: X \longrightarrow \mathbb{R}$ defined by $f(x)=a x^{2}+b x+c$. Note that

$$
\begin{aligned}
f(x+h) & =a(x+h)^{2}+b(x+h)+c \\
& =a\left(x^{2}+2 x h+h^{2}\right)+b x+b h+c \\
& =a x^{2}+2 a x h+a h^{2}+b x+b h+c \\
& =\left(a x^{2}+b x+c\right)+(2 a x+b) h+a h^{2} \\
& =f(x)+(2 a x+b) h+a h^{2} .
\end{aligned}
$$

Thus we have

$$
\begin{aligned}
\frac{f(x+h)-f(x)}{h} & =\frac{f(x)+(2 a x+b) h+a h^{2}-f(x)}{h} \\
& =\frac{(2 a x+b) h+a h^{2}}{h} \\
& =(2 a x+b)+a h \\
& =2 a x+b+a h .
\end{aligned}
$$

This means that

$$
\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\lim _{h \rightarrow 0}(2 a x+b+a h)=2 a x+b
$$

As such, we can conclude that $f^{\prime}(x)=\frac{d f}{d x}=2 a x+b$.

```


```{admonition} Example: the (first) derivative of a positive integer power function
:class: tip

Consider the function $f: \mathbb{R} \longrightarrow \mathbb{R}$ defined by $f(x)=x^{n}$, where $n \in \mathbb{N}=\mathbb{Z}_{++}$. We know from the binomial theorem that

$$
\begin{aligned}
f(x+h) & =(x+h)^{n} \\
& =\sum_{r=0}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r} \\
& =\left(\begin{array}{l}
n \\
0
\end{array}\right) x^{n} h^{0}+\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r} \\
& =1 x^{n}(1)+\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r} \\
& =x^{n}+\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r} .
\end{aligned}
$$

Thus we have

$$
\begin{aligned}
\frac{f(x+h)-f(x)}{h} & =f(x)+\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r} \\
& =\frac{f(x)+\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r}-f(x)}{h} \\
& =\frac{\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r}}{h} \\
& =\sum_{r=1}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r-1} \\
& =\left(\begin{array}{l}
n \\
1
\end{array}\right) x^{n-1} h^{0}+\sum_{r=2}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r-1} \\
& =n x^{n-1}(1)+\sum_{r=2}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r-1}
\end{aligned}
$$

$$
\begin{aligned}
\frac{f(x+h)-f(x)}{h} & =n x^{n-1}+\sum_{r=2}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r-1} \\
& =n x^{n-1}+\sum_{r=2}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r-1}
\end{aligned}
$$

This means that

$$
\lim _{h \rightarrow 0} \frac{f\left(x_{0}+h\right)-f\left(x_{0}\right)}{h}=\lim _{h \rightarrow 0}\left(n x^{n-1}+\sum_{r=2}^{n}\left(\begin{array}{l}
n \\
r
\end{array}\right) x^{n-r} h^{r-1}\right)=n x^{n-1}
$$

As such, we can conclude that

$$
f^{\prime}(x)=\frac{d f}{d x}=n x^{n-1}
$$

```


### The (first) derivatives of some commonly encountered functions

- If $f(x)=a$, where $a \in \mathbb{R}$ is a constant, then $f^{\prime}(x)=0$.
- If $f(x)=a x+b$, then $f^{\prime}(x)=a$.
- If $f(x)=a x^{2}+b x+c$, then $f^{\prime}(x)=2 a x+b$.
- If $f(x)=x^{n}$, where $n \in \mathbb{N}$, then $f^{\prime}(x)=n x^{n-1}$.
- If $f(x)=\frac{1}{x^{n}}=x^{-n}$, where $n \in \mathbb{N}$, then $f^{\prime}(x)=-n x^{-n-1}=-n x^{-(n+1)}=\frac{-n}{x^{n+1}}$. (Note that we need to assume that $x \neq 0$.)
- If $f(x)=e^{x}=\exp (x)$, then $f^{\prime}(x)=e^{x}=\exp (x)$.
- If $f(x)=\ln (x)$, then $f^{\prime}(x)=\frac{1}{x}$. (Recall that $\ln (x)$ is only defined for $x>0$, so we need to assume that $x>0$ here.)


### Some useful differentiation rules

- Scalar Multiplication Rule: If $f(x)=c g(x)$ where $c \in \mathbb{R}$ is a constant, then $f^{\prime}(x)=c g^{\prime}(x)$.
- Summation Rule: If $f(x)=g(x)+h(x)$, then $f^{\prime}(x)=g^{\prime}(x)+h^{\prime}(x)$.
- Product Rule: If $f(x)=g(x) h(x)$, then $f^{\prime}(x)=g^{\prime}(x) h(x)+h^{\prime}(x) g(x)$.
- Quotient Rule: If $f(x)=\frac{g(x)}{h(x)}$, then $f^{\prime}(x)=\frac{g^{\prime}(x) h(x)-h^{\prime}(x) g(x)}{[h(x)]^{2}}$.
- Chain Rule: If $f(x)=g(h(x))$, then $f^{\prime}(x)=g^{\prime}(h(x)) h^{\prime}(x)$.
- The Inverse Function Rule: Suppose that the function $y=f(x)$ has a well defined inverse function $x=f^{-1}(y)$. If appropriate regularity conditions hold, then

$$
\frac{d x}{d y}=\frac{1}{\left(\frac{d y}{d x}\right)}
$$

### The quotient rule is redundant

In a sense, the quotient rule is redundant. The reason for this is that it can be obtained from a combination of the product rule and the chain rule.

Suppose that $f(x)=\frac{g(x)}{h(x)}$. Note that

$$
f(x)=\frac{g(x)}{h(x)}=g(x)[h(x)]^{-1}
$$

Let $[h(x)]^{-1}=k(x)$.  We know from the chain rule that

$$
k^{\prime}(x)=(-1)[h(x)]^{-2} h^{\prime}(x)=\frac{-h^{\prime}(x)}{[h(x)]^{2}}
$$

Note that
$$
f(x)=\frac{g(x)}{h(x)}=g(x)[h(x)]^{-1}=g(x) k(x) .
$$

We know from the product rule that

$$
\begin{aligned}
f^{\prime}(x) & =g^{\prime}(x) k(x)+k^{\prime}(x) g(x) \\
& =g^{\prime}(x)[h(x)]^{-1}+\left(\frac{-h^{\prime}(x)}{[h(x)]^{2}}\right) g(x) \\
& =\frac{g^{\prime}(x)}{h(x)}-\frac{h^{\prime}(x) g(x)}{[h(x)]^{2}} \\
& =\frac{g^{\prime}(x) h(x)}{[h(x)]^{2}}-\frac{h^{\prime}(x) g(x)}{[h(x)]^{2}} \\
& =\frac{g^{\prime}(x) h(x)-h^{\prime}(x) g(x)}{[h(x)]^{2}} .
\end{aligned}
$$

This is simply the quotient rule!

```{admonition} Example: the (first) derivative of an exponential function
:class: tip

Suppose that $f(x)=a^{x}$, where $a \in \mathbb{R}_{++}=(0, \infty)$ and $x \neq 0$. Note that

$$
\ln (f(x))=\ln \left(a^{x}\right)=x \ln (a) .
$$

We know from the chain rule that

$$
\frac{d f(x)}{d x}=\left(\frac{d f(x)}{d \ln (f(x))}\right)\left(\frac{d \ln (f(x))}{d x}\right)
$$

We know from the inverse function rule that

$$
\frac{d f(x)}{d \ln (f(x))}=\frac{1}{\left(\frac{d \ln (f(x))}{d f(x)}\right)}=\frac{1}{\left(\frac{1}{a^{x}}\right)}=a^{x}
$$

We know from the scalar multiplication rule that

$$
\begin{aligned}
\frac{d \ln (f(x))}{d x} & =\frac{d x \ln (a)}{d x} \\
& =\ln (a)\left(\frac{d x}{d x}\right) \\
& =\ln (a)(1) \\
& =\ln (a) .
\end{aligned}
$$

Thus we have

$$
\frac{d a^{x}}{d x}=a^{x} \ln (a)
$$

```

```{admonition} Example: the (first) derivative of a logarithmic function
:class: tip

Suppose that $f(x)=\log _{a}(x)$, where $a \in \mathbb{R}_{++}=(0, \infty)$ and $x>0$.  Note that

$$
a^{f(x)}=a^{\log _{a}(x)}=x .
$$

We know from the chain rule that

$$
\frac{d f(x)}{d x}=\left(\frac{d f(x)}{d a^{f(x)}}\right)\left(\frac{d a^{f(x)}}{d x}\right)
$$

We know from the inverse function rule that

$$
\frac{d f(x)}{d a^{f(x)}}=\frac{1}{\left(\frac{d a^{f(x)}}{d f(x)}\right)}=\frac{1}{\left(a^{f(x)} \ln (a)\right)}=\frac{1}{x \ln (a)}
$$

Thus we have

$$
\begin{aligned}
\frac{d a^{x}}{d x} & =\left(\frac{1}{x \ln (a)}\right)\left(\frac{d x}{d x}\right) \\
& =\left(\frac{1}{x \ln (a)}\right)(1) \\
& =\frac{1}{x \ln (a)} .
\end{aligned}
$$

```

```{admonition} Example: product rule
:class: tip

Consider the function $f(x)=(a x+b)(c x+d)$.

Differentiation Approach One:
Note that

$$
\begin{aligned}
f(x) & =(a x+b)(c x+d) \\
& =a c x^{2}+a d x+b c x+b d \\
& =a c x^{2}+(a d+b c) x+b d
\end{aligned}
$$

Thus we have

$$
\begin{aligned}
f^{\prime}(x) & =2 a c x+(a d+b c) \\
& =2 a c x+a d+b c
\end{aligned}
$$


Differentiation Approach Two:
Note that $f(x)=g(x) h(x)$ where $g(x)=a x+b$ and $h(x)=c x+d$.
This means that $g^{\prime}(x)=a$ and $g^{\prime}(x)=c$.
Thus we know, from the product rule, that

$$
\begin{aligned}
f^{\prime}(x) & =g^{\prime}(x) h(x)+h^{\prime}(x) g(x) \\
& =a(c x+d)+c(a x+b) \\
& =a c x+a d+a c x+b c \\
& =2 a c x+a d+b c .
\end{aligned}
$$
```



## The relationship between continuity and differentiability

- Continuity is a necessary, but not sufficient, condition for differentiability.
    - Being a necessary condition means that "not continuous" implies "not differentiable", which means that differentiable implies continuous.
    - Not being a sufficient condition means that continuous does NOT imply differentiable.
- Differentiability is a sufficient, but not necessary, condition for continuity.
    - Being a sufficient condition means that differentiable implies continuous.
    - Not being a necessary condition means that "not differentiable" does NOT imply "not continuous", which means that continuous does NOT imply differentiable.


### Continuity does NOT imply differentiability

Consider the function

$$
f(x)=\left\{\begin{array}{cc}
2 x & \text { if } x \leq 1 \\
\frac{1}{2} x+\frac{3}{2} & \text { if } x \geq 1
\end{array}\right.
$$

(There is no problem with this double definition at the point $x=1$ because the two parts of the function are equal at that point.)

This function is continuous at $x=1$ because

$$
\lim _{x \rightarrow 1} 2 x=2=\lim _{x \rightarrow 1}\left(\frac{1}{2} x+\frac{3}{2}\right)
$$

and

$$
f(1)=2
$$


However, this function is not differentiable at $x=1$ because

$$
\lim _{h \uparrow 1} \frac{f(1+h)-f(1)}{h}=2
$$

and

$$
\lim _{h \downarrow 1} \frac{f(1+h)-f(1)}{h}=\frac{1}{2}
$$

Since

$$
\lim _{h \uparrow 1} \frac{f(1+h)-f(1)}{h} \neq \lim _{h \downarrow 1} \frac{f(1+h)-f(1)}{h}
$$

we know that

$$
\lim _{h \rightarrow 1} \frac{f(1+h)-f(1)}{h}
$$

does not exist.

### Differentiability implies continuity

Consider a function $f: X \longrightarrow \mathbb{R}$ where $X \subseteq \mathbb{R}$.
Suppose that

$$
\lim _{h \rightarrow 0}\left(\frac{f(a+h)-f(a)}{h}\right)
$$

exists.

We want to show that this implies that $f(x)$ is continuous at the point $a \in X$.  The following proof of this proposition is drawn from {cite:ps}`ayres2013` (Chapter 8, Solved Problem 2).


First, note that

$$
\begin{gathered}
\lim _{h \rightarrow 0}(f(a+h)-f(a))=\lim _{h \rightarrow 0}\left\{\left(\frac{h}{h}\right)(f(a+h)-f(a))\right\} \\
=\lim _{h \rightarrow 0}\left\{h\left(\frac{f(a+h)-f(a)}{h}\right)\right\} \\
=\lim _{h \rightarrow 0}(h) \lim _{h \rightarrow 0}\left(\frac{f(a+h)-f(a)}{h}\right) \\
=(0)\left(\lim _{h \rightarrow 0}\left(\frac{f(a+h)-f(a)}{h}\right)\right) \\
=0 .
\end{gathered}
$$

Thus we have

$$
\lim _{h \rightarrow 0}(f(a+h)-f(a))=0 .
$$

Now note that

$$
\begin{gathered}
\lim _{h \rightarrow 0}(f(a+h)-f(a))=\lim _{h \rightarrow 0} f(a+h)-\lim _{h \rightarrow 0} f(a) \\
=\left(\lim _{h \rightarrow 0} f(a+h)\right)-f(a)
\end{gathered}
$$

Upon combining these two results, we obtain

$$
\left(\lim _{h \rightarrow 0} f(a+h)\right)-f(a)=0 \Longleftrightarrow \lim _{h \rightarrow 0} f(a+h)=f(a) .
$$

Finally, note that

$$
\lim _{x \rightarrow a} f(x)=\lim _{h \rightarrow 0} f(a+h) .
$$

Thus we have

$$
\lim _{x \rightarrow a} f(x)=f(a)
$$

This means that $f(x)$ is continuous at the point $x=a$.



## Higher-order derivatives

Suppose that $f: X \longrightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}$, is an $n$-times continuously differentiable function for some $n \geqslant 2$.

We can view the first derivative of this function as a function in its own right. This can be seen by letting $g(x)=f^{\prime}(x)$.

The second derivative of $f(x)$ with respect to $x$ twice is simply the first derivative of $g(x)$ with respect to $x$.

In other words,

$$
f^{\prime \prime}(x)=g^{\prime}(x)
$$

or, if you prefer,

$$
\frac{d^{2} f}{d x^{2}}=\frac{d g}{d x}
$$

Thus we have

$$
\frac{d^{2} f}{d x^{2}}=\frac{d}{d x}\left(\frac{d f}{d x}\right)
$$


The same approach can be used for defining third and higher order derivative.
Thus we have

$$
f^{(k)}(x)=\frac{d^{k} f}{d x^{k}}=\frac{d}{d x}\left(\frac{d^{k-1} f}{d x^{k-1}}\right)
$$

for all $k \in\{1,2, \cdots, n\}$, where we define

$$
f^{(0)}(x)=f(x)
$$


```{admonition} Example: higher-order derivatives

- Let $f(x)=x^{n}$
- Then we have:
- $f^{\prime}(x)=\frac{d f(x)}{d x}=n x^{n-1}$
- $f^{\prime \prime}(x)=\frac{d f^{\prime}(x)}{d x}=n(n-1) x^{n-2}$,
- $f^{\prime \prime \prime}(x)=\frac{d f^{\prime \prime}(x)}{d x}=n(n-1)(n-2) x^{n-3}$,
- and so on and so forth until
- $f^{(k)}(x)=\frac{d f^{(k-1)}(x)}{d x}=n(n-1)(n-2) \cdots(n-(k-1)) x^{n-k}$,
- and so on and so forth until
- $f^{(n)}(x)=\frac{d f^{(n-1)}(x)}{d x}=n(n-1)(n-2) \cdots(1) x^{0}$.
- Note that $n(n-1)(n-2) \cdots(1)=n$ ! and $x^{0}=1$ (asuming that $x \neq 0$ ).
- This means that $f^{(n)}(x)=n$ !, which is a constant.
- As such, we know that $f^{(n+1)}(x)=\frac{d f^{(n)}(x)}{d x}=0$.
- This means that $f^{(n+j)}(x)=0$ for all $j \in \mathbb{N}$.
```


### Interpreting first and second derivatives

- The first and second order derivatives of a function provide useful information about the shape of that function.
- The first derivative tells us about the slope of a function. Is it increasing, decreasing, or constant over some interval? At what point or points, if any, does the sign of the slope of the function change? (Such points are known as turning points.)
- The second derivative tells us about the curvature of the function. Is it convex or concave over some interval? At what point or points, if any, does the curvature of the function change? (Such points are known as inflexion points.)


## Increasing functions

- A function is said to be **increasing** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(x_{0}\right) \leqslant f\left(x_{1}\right)$ whenever $x_{0}<x_{1}$, where $x_{0} \in(a, b)$ and $x_{1} \in(a, b)$.
    - If $f^{\prime}(x) \geqslant 0$ for all $x \in(a, b)$, then the function $f(x)$ is increasing on the interval $(a, b)$.
- A function is said to be **strictly increasing** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(x_{0}\right)<f\left(x_{1}\right)$ whenever $x_{0}<x_{1}$, where $x_{0} \in(a, b)$ and $x_{1} \in(a, b)$.
    - If $f^{\prime}(x)>0$ for all $x \in(a, b)$, then the function $f(x)$ is strictly increasing on the interval $(a, b)$.


## Decreasing functions

- A function is said to be **decreasing** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(x_{0}\right) \geqslant f\left(x_{1}\right)$ whenever $x_{0}<x_{1}$, where $x_{0} \in(a, b)$ and $x_{1} \in(a, b)$.
    - If $f^{\prime}(x) \leqslant 0$ for all $x \in(a, b)$, then the function $f(x)$ is decreasing on the interval $(a, b)$.
- A function is said to be **strictly decreasing** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(x_{0}\right)>f\left(x_{1}\right)$ whenever $x_{0}<x_{1}$, where $x_{0} \in(a, b)$ and $x_{1} \in(a, b)$.
    - If $f^{\prime}(x)<0$ for all $x \in(a, b)$, then the function $f(x)$ is strictly decreasing on the interval $(a, b)$.


## Convex functions

- A function is said to be **convex** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(\lambda x_{0}+(1-\lambda) x_{1}\right) \leqslant \lambda f\left(x_{0}\right)+(1-\lambda) f\left(x_{1}\right)$ whenever $x_{0} \in(a, b), x_{1} \in(a, b)$ and $x_{0} \neq x_{1}$.
    - In other words, a function is convex on an interval if the graph of the straight line joining any two distinct points in that interval lies on or above the graph of the function between those two points.
    - If $f^{\prime \prime}(x) \geqslant 0$ for all $x \in(a, b)$, then the function $f(x)$ is convex on the interval $(a, b)$.
- A function is said to be **strictly convex** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(\lambda x_{0}+(1-\lambda) x_{1}\right)<\lambda f\left(x_{0}\right)+(1-\lambda) f\left(x_{1}\right)$ whenever $x_{0} \in(a, b), x_{1} \in(a, b)$ and $x_{0} \neq x_{1}$.
    - In other words, a function is strictly convex on an interval if the graph of the straight line joining any two distinct points in that interval lies strictly above the graph of the function between those two points.
    - If $f^{\prime \prime}(x)>0$ for all $x \in(a, b)$, then the function $f(x)$ is strictly convex on the interval $(a, b)$.


## Concave functions

- A function is said to be **concave** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(\lambda x_{0}+(1-\lambda) x_{1}\right) \geqslant \lambda f\left(x_{0}\right)+(1-\lambda) f\left(x_{1}\right)$ whenever $x_{0} \in(a, b), x_{1} \in(a, b)$ and $x_{0} \neq x_{1}$.
    - In other words, a function is concave on an interval if the graph of the straight line joining any two distinct points in that interval lies on or below the graph of the function between those two points.
    - If $f^{\prime \prime}(x) \leqslant 0$ for all $x \in(a, b)$, then the function $f(x)$ is convex on the interval $(a, b)$.
- A function is said to be **strictly concave** on the non-empty interval $(a, b) \subseteq \mathbb{R}$ if $f\left(\lambda x_{0}+(1-\lambda) x_{1}\right)>\lambda f\left(x_{0}\right)+(1-\lambda) f\left(x_{1}\right)$ whenever $x_{0} \in(a, b), x_{1} \in(a, b)$ and $x_{0} \neq x_{1}$.
    - In other words, a function is strictly concave on an interval if the graph of the straight line joining any two distinct points in that interval lies strictly below the graph of the function between those two points.
    - If $f^{\prime \prime}(x)<0$ for all $x \in(a, b)$, then the function $f(x)$ is strictly convex on the interval $(a, b)$.


## Peak (local maximum) turning points

- The point $c$ yields a **local maximum** of the function $f(x)$ if there exists some non-empty interval $(a, b)$ that is a subset of the domain of the function such that $c \in(a, b)$ and $f(x) \leqslant f(c)$ for all $x \in(a, b)$.
- If (i) $f^{\prime}(c)=0$, (ii) $f^{\prime}(x) \geqslant 0$ for all $x \in(a, c)$ where $a<c$, and (iii) $f^{\prime}(x) \leqslant 0$ for all $x \in(c, b)$ where $c<b$, then the point $x=c$ yields a local maximum of the function $f$ over the non-empty interval $(a, b)$.
- If (i) $f^{\prime}(c)=0$ and (ii) $f^{\prime \prime}(c)<0$, then the point $x=c$ yields a strict local maximum of the function $f$ over some non-empty interval $(a, b)$ such that $c \in(a, b)$.
    - Note that we need $f^{\prime \prime}(c)<0$ here. If $f^{\prime \prime}(c)=0$, then this theorem does not apply. In such a case, a different approach must be used to determine whether or not the point $x=c$ yields a local maximum of the function.


## Trough (local minimum) turning points

- The point $c$ yields a **local minimum** of the function $f(x)$ if there exists some non-empty interval $(a, b)$ that is a subset of the domain of the function such that $c \in(a, b)$ and $f(x) \geqslant f(c)$ for all $x \in(a, b)$.
- If (i) $f^{\prime}(c)=0$, (ii) $f^{\prime}(x) \leqslant 0$ for all $x \in(a, c)$ where $a<c$, and (iii) $f^{\prime}(x) \geqslant 0$ for all $x \in(c, b)$ where $c<b$, then the point $x=c$ yields a local minimum of the function $f$ over the non-empty interval $(a, b)$.
- If (i) $f^{\prime}(c)=0$ and (ii) $f^{\prime \prime}(c)>0$, then the point $x=c$ yields a strict local minimum of the function $f$ over some non-empty interval $(a, b)$ such that $c \in(a, b)$.
    - Note that we need $f^{\prime \prime}(c)>0$ here. If $f^{\prime \prime}(c)=0$, then this theorem does not apply. In such a case, a different approach must be used to determine whether or not the point $x=c$ yields a local minimum of the function.


## Inflection points

- An **inflection point** of a function is a point at which the curvature of the function changes from either convex to concave or concave to convex.
- The point $x=c$ is an inflection point of the twice differentiable function $f(x)$ if there exists some interval $(a, b)$ that is a subset of the domain of $f(x)$ such that $c \in(a, b)$ and either (a) both (i) $f^{\prime \prime}(x) \geqslant 0$ for all $x \in(a, c)$ and (ii) $f^{\prime \prime}(x) \leqslant 0$ for all $x \in(c, b)$, or (b) both (i) $f^{\prime \prime}(x) \leqslant 0$ for all $x \in(a, c)$ and (ii) $f^{\prime \prime}(x) \geqslant 0$ for all $x \in(c, b)$.


- Suppose that the function $f(x)$ is twice continuously differentiable on the interval $(a, b)$, where that interval is a subset of the domain of the function. If $c \in(a, b)$ is an inflection point of the function $f(x)$, then $f^{\prime \prime}(c)=0$.
- Suppose that the function $f(x)$ is twice continuously differentiable on the interval $(a, b)$, where that interval is a subset of the domain of the function and $c \in(a, b)$. If both (i) $f^{\prime \prime}(c)=0$ and (ii) $f^{\prime \prime}(c)$ changes sign at the point $x=c$, then the point $x=c$ is an inflection point of the function $f(x)$.


### Some economic applications of derivatives

- Obtaining marginal revenue from total revenue and marginal cost from total cost.
- The relationship between the marginal cost curve and the average cost curve. (See separate handout.)
- Marginal revenue for a single-price monopolist that faces a linear demand curve. (See separate handout.)
- The calculation of elasticities, with a particular focus on the own-price elasticity of demand, cross-price elasticities of demand, the income elasticity of demand, and the own-price elasticity of supply.


#### Marginal revenue and marginal cost

- If a firm's total revenue as a function of output is given by the function $R(Q)$, then its marginal revenue is given by the first derivative of that function:

$$
MR(Q)=\frac{d R(Q)}{d Q}=R^{\prime}(Q)
$$

- If a firm's total cost as a function of output is given by the function $C(Q)$, then its marginal cost is given by the first derivative of that function:

$$
MC(Q)=\frac{d C(Q)}{d Q}=C^{\prime}(Q)
$$

#### Marginal revenue for a monopolist

Consider a single-priced monopolist that faces an inverse demand curve that is given by the function $P(Q)$. Since demand curves usually slope down, we will assume that $P^{\prime}(Q)<0$. The total revenue for this monopolist is given by $R(Q)=P(Q) Q$. As such, the marginal revenue for this monopolist is

$$
\begin{aligned}
M R(Q) & =R^{\prime}(Q) \\
& =P^{\prime}(Q) Q+(1) P(Q) \\
& =P^{\prime}(Q) Q+P(Q) \\
& =P(Q)+P^{\prime}(Q) Q \\
& <P(Q)
\end{aligned}
$$

Since $P(Q)$ will be the market price when the monopolist produces $Q>0$ units of output and $P^{\prime}(Q)<0$, we know that marginal revenue is less than the market price for a monopolist.


The two terms that make up marginal revenue for a monopolist can be given a very intuitive explanation. If the monopolist increases its output by a small amount, it receives the market price for the additional output (hence the $P(Q)$ term), but it loses some revenue due to the lower price that it will receive for all of the pre-existing output (the $P^{\prime}(Q) Q$ term).
- The additional output is known as the "marginal" unit of output.
- The pre-existing units of output are known as the "infra-marginal" units of output.

*Illustrate the relationship between the inverse market demand curve for a monopolist's product and the marginal revenue curve for the monopolist on the white-board.*



#### Point elasticities


FROM LINEAR DEMAND

The ‚Äúpoint‚Äù own-price elasticity of demand is given by the formula

$$
\epsilon^D_P (P) = \left( \frac{P}{Q^D (P)} \right) (\text{slope}(Q^D (P)))
$$
Suppose that we have a linear demand curve of the form

$$
Q = a ‚àí bP
$$
where $a > 0$ and $b > 0$
The slope of this demand curve is equal to $(‚àíb)$
As such, the point own-price elasticity of demand in this case is given by

$$
\begin{align*}
\epsilon^D_P (P) &= \left( \frac{P}{a - bP} \right) (-b) \\
&= \left( \frac{-bP}{a - bP} \right)
\end{align*}
$$

Note that 

$$
\begin{align*}
| \epsilon^D_P (P) | &= 1 \\
\iff | \frac{-bP}{a - bP} | &= 1 \\
\iff \frac{bP}{a - bP} &= 1 \\
\iff bP &= a - bP \\
\iff 2bP &= a \\
\iff P &= \frac{a}{2b} \\
\end{align*}
$$

More generally, we can show that, for the linear demand cuve above, we have

$$
| \epsilon^D_P (P) | =
\begin{cases}
\in (0, 1) \;\; \text{ if } P < \frac{a}{2b} \\
= 1 \;\; \text{ if } P = \frac{a}{2b} \\
\in (1, \infty) \;\; \text{ if } P > \frac{a}{2b} \\
\end{cases}
$$

Since the demand curve above is downward sloping, this means that

$$
\epsilon^D_P (P) =
\begin{cases}
\in (-1, 0) \;\; \text{ if } P < \frac{a}{2b} \\
= -1 \;\; \text{ if } P = \frac{a}{2b} \\
\in (-\infty, -1) \;\; \text{ if } P > \frac{a}{2b} \\
\end{cases}
$$

Illustrate this on the whiteboard


---



Suppose that $y=f(x)$. The **elasticity of $y$ with respect to $x$** is defined to be the percentage change in $y$ that is induced by a small percentage change in $x$.

In calculus terms, this is given by the function

$$
\epsilon_{x}^{y}(x)=\left(\frac{x}{y}\right)\left(\frac{d y}{d x}\right)=\frac{x f^{\prime}(x)}{f(x)}
$$

It can also be calculated using calculus as

$$
\epsilon_{x}^{y}(x)=\frac{d \ln (y)}{d \ln (x)}
$$

We can use the chain rule and the inverse function rule to show that these two formulae for the elasticity of $y$ with respect to $x$ are equivalent. Note that

$$
\begin{aligned}
\frac{d \ln (y)}{d \ln (x)} & =\left(\frac{d \ln (y)}{d y}\right)\left(\frac{d y}{d x}\right)\left(\frac{d x}{d \ln (x)}\right) \\
& =\left(\frac{1}{y}\right)\left(\frac{d y}{d x}\right)\left(\frac{1}{\left(\frac{d \ln (x)}{d x}\right)}\right) \\
& =\left(\frac{1}{y}\right)\left(\frac{d y}{d x}\right)\left(\frac{1}{\left(\frac{1}{x}\right)}\right) \\
& =\left(\frac{x}{y}\right)\left(\frac{d y}{d x}\right) .
\end{aligned}
$$

- Suppose that the demand curve for a commodity is given by $Q=D(P)$. In this case, the own-price elasticity of demand for this commodity is given by

$$
\epsilon_{P}^{D}(P)=\left(\frac{P}{Q}\right)\left(\frac{d Q}{d P}\right)=\frac{P D^{\prime}(P)}{D(P)}
$$

- Suppose that the supply curve for a commodity is given by $Q=S(P)$. In this case, the own-price elasticity of supply for this commodity is given by

$$
\epsilon_{P}^{S}(P)=\left(\frac{P}{Q}\right)\left(\frac{d Q}{d P}\right)=\frac{P S^{\prime}(P)}{S(P)}
$$




## Using derivatives to approximate things

In situations where functions are rather complicated (especially when they are non-linear and non-separable in the components that make use of the independent variable), it is sometimes convenient to approximate the function, or the change in the value generated by the function, rather than calculate it directly. Obviously, we need to worry about the accuracy of the approximation when we do this.

Derivatives can be used to form some such approximations. In particular, they can be used to form the following approximations:
- Polynomial (Power Series) approximations of a function.
    - We will look at an example of this known as a Taylor series approximation of a function $f(x)$ around the point $x=x_{0}$.
    - The special case of a Taylor series approximation of the function $f(x)$ that takes place around the point $x=0$ is known as a McLauren series approximation of that function.
- We will also look at differential approximations to the change in the value taken by a function when there is a change in the value taken by the independent variable.



### Taylor series approximations of functions

Consider the function $f:(a, b) \longrightarrow \mathbb{R}$, where $-\infty<a<b<\infty$. Denote this function by $f(x)$.

Suppose that $f(x)$ is at least $n$-times differentiable with respect to $x$ on the interval $(a, b)$, and let $x_{0} \in(a, b)$.  The **$n$-th order Taylor series approximation** of $f(x)$ around the point $x=x_{0}$ is defined to be: 

$$
\begin{gathered}
f(x) \approx \sum_{k=0}^{n}\left(\frac{1}{k !}\right) f^{(k)}\left(x_{0}\right)\left(x-x_{0}\right)^{k} \\
\approx\left(\frac{1}{0 !}\right) f^{(0)}\left(x_{0}\right)\left(x-x_{0}\right)^{0}+\left(\frac{1}{1 !}\right) f^{(1)}\left(x_{0}\right)\left(x-x_{0}\right)^{1} \\
+\left(\frac{1}{2 !}\right) f^{(2)}\left(x_{0}\right)\left(x-x_{0}\right)^{2}+\cdots+\left(\frac{1}{n !}\right) f^{(n)}\left(x_{0}\right)\left(x-x_{0}\right)^{n} \\
\approx\left(\frac{1}{1}\right) f\left(x_{0}\right)+\left(\frac{1}{1}\right) f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\left(\frac{1}{2}\right) f^{\prime \prime}\left(x_{0}\right)\left(x-x_{0}\right)^{2} \\
+\cdots+\left(\frac{1}{n !}\right) f^{(n)}\left(x_{0}\right)\left(x-x_{0}\right)^{n} \\
\approx f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\left(\frac{1}{2}\right) f^{\prime \prime}\left(x_{0}\right)\left(x-x_{0}\right)^{2} \\
+\cdots+\left(\frac{1}{n !}\right) f^{(n)}\left(x_{0}\right)\left(x-x_{0}\right)^{n} 
\end{gathered}
$$

- A **linear (or first-order) Taylor series approximation** of a function would take the form

$$
f(x) \approx f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right) 
$$

- A **quadratic (or second-order) Taylor series approximation** of a function would take the form

$$
f(x) \approx f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\left(\frac{1}{2}\right) f^{\prime \prime}\left(x_{0}\right)\left(x-x_{0}\right)^{2}
$$

- If the function was differentiable an infinite number of times, then we could imagine forming the **$\infty$-order Taylor series approximation** of it around the point $x=x_{0}$. This would take the form

$$
f(x) \approx \sum_{k=0}^{\infty}\left(\frac{1}{k !}\right) f^{(k)}\left(x_{0}\right)\left(x-x_{0}\right)^{k}
$$



It is important to remember that Taylor series approximations of functions are just that: An approximation of the function.

The accuracy of the approximation will depend on a number of factors. These include the form of the underlying function, the closeness of the independent variable $(x)$ to the value around which the Taylor series approximation is centred $\left(x_{0}\right)$, and the order of the approximation.

In many cases, the Taylor series approximation of a function $f(x)$ that is centred on the point $x=x_{0}$ will be a reasonably accurate approximation of the function in a sufficiently small neighbourhood of the point $x=x_{0}$.

Nonetheless, it is important to recognise that we will usually have

$$
f(x) \approx\left(\sum_{k=0}^{n}\left(\frac{1}{k !}\right) f^{(k)}\left(x_{0}\right)\left(x-x_{0}\right)^{k}\right)+R_{n+1}(x)
$$

where $R_{n+1}(x)$ is a "remainder" term.


- The $n$ th-order Taylor series approximation of the function $f(x)$ around the point $x=x_{0}$ is given by

$$
f(x) \approx \sum_{k=0}^{n}\left(\frac{1}{k !}\right) f^{(k)}\left(x_{0}\right)\left(x-x_{0}\right)^{k}
$$


If the function being approximated is at least $(n+1)$-times differentiable on an interval that includes both the point $x=x_{0}$ and the point $x$, then Lagrange has shown that the remainder term that reconciles the above Taylor series approximation of $f(x)$ with the value taken by the function itself is exactly given by the expression

$$
R_{n+1}(x)=\left(\frac{1}{(n+1) !}\right) f^{(n+1)}(z)\left(x-x_{0}\right)^{n+1}
$$

for some $z$ between $x$ and $x_{0}$. (If $x<x_{0}$, then $z \in\left(x, x_{0}\right)$. If $x>x_{0}$, then $z \in\left(x_{0}, x\right)$.)


### Differential approximation to a change in a function's value

Consider a function $y=f(x)$ that is at least once differentiable on some non-empty interval $(a, b)$.  Suppose that the value of the independent variable changes from some initial value $x \in(a, b)$ to some final value $x+\Delta x \in(a, b)$.

The exact impact of this change on the value taken by the function is $\Delta y=f(x+\Delta x)-f(x)$.

We might think about calculating an approximation of this change by using a linear (first-order) Taylor series of the function $f(x+\Delta x)$ around the point $x$. This approximation takes the form

$$
f(x+\Delta x) \approx f(x)+f^{\prime}(x)((x+\Delta x)-x)=f(x)+f^{\prime}(x) \Delta x
$$

If we employ this approximation, then we obtain

$$
\Delta y \approx f(x)+f^{\prime}(x) \Delta x-f(x)=f^{\prime}(x) \Delta x
$$

Recall that in many cases, if $x+\Delta x$ is sufficiently close to $x$, then the linear Taylor series approximation for $f(x+\Delta x)$ around the point $x$ will be reasonably accurate. If this is the case, then the linear (or first-order) approximation to the change in the value of the function $f(x)$ when the independent variable moves from $x$ to $x+\Delta x$ that we derived above will also be reasonably accurate when $x+\Delta x$ is sufficiently close to $x$.

In other words, it will be reasonably accurate when $\Delta x$ is sufficiently small in absolute value terms (that is, close to zero). In such cases, we typically use $d x$ to denote $\Delta x$, and $d y$ to denote $\Delta y$. This means that if $f(x)$ is at least once differentiable and $d x$ bis sufficiently small, then

$$
d y \approx f^{\prime}(x) d x
$$

This expression is known as the linear (or first-order) **differential** of the function $f(x)$.


### An application of linear differential approximation

The change in the natural logarithm of a variable is approximately equal to the corresponding proprtionate change in that variable. As such, we can interpret the change in the natural logarithm of the price level in an economy as a good indicator of inflation in that economy (as long as the change in the price level is not too large). This can be seen by noting that if $p=\ln (P)$, where $P$ is the price level in an economy, then

$$
d p \approx\left(\frac{d \ln (P)}{d P}\right) d P \Longleftrightarrow d p \approx\left(\frac{1}{P}\right) d P \Longleftrightarrow d p \approx \frac{d P}{P}
$$


Ted Sieper places the natural log of the price level on the horizontal axis of a diagrammatic representation of a version of the "IS-LM" macroeconomic model, thereby enabling him to use the above result to interpret horizontal distances in that diagram as a measure of inflation (or expected inflation).  This is a very interesting and insightful paper that is well worth a read at some stage during your study of economics.

See Sieper, E (1987), "Policy irrelevance is not the rule", Unpublished working paper, ANU, Canberra, November. Available online at: https://files.brontecapital.com/ted_sieper_paper.pdf.



### L'Hopital's rule for limits of indeterminate form

Consider the function $f(x)=\frac{g(x)}{h(x)}$. Suppose that $g(x)$ and $h(x)$ are both differentiable at all points on a non-empty interval $(a, b)$ that contains the point $x=x_{0}$, with the possible exception that they might not be differentiable at the point $x_{0}$.

Suppose also that $h^{\prime}(x) \neq 0$ for all $x \in(a, b)$.  

- If $\lim _{x \rightarrow x_{0}} g(x)=\lim _{x \rightarrow x_{0}} h(x)=0$ and $\lim _{x \rightarrow x_{0}}\left(\frac{g^{\prime}(x)}{h^{\prime}(x)}\right)=L$, then according to L'Hopital's rule for " $\left(\frac{0}{0}\right)$ " indeterminate forms, we have

$$
\lim _{x \rightarrow x_{0}} f(x)=\lim _{x \rightarrow x_{0}}\left(\frac{g(x)}{h(x)}\right)=\lim _{x \rightarrow x_{0}}\left(\frac{g^{\prime}(x)}{h^{\prime}(x)}\right)=L
$$

- If $\lim _{x \rightarrow x_{0}} g(x)= \pm \infty$, $\lim _{x \rightarrow x_{0}} h(x)= \pm \infty$ and $\lim _{x \rightarrow x_{0}}\left(\frac{g^{\prime}(x)}{h^{\prime}(x)}\right)=L$, then according to L'Hopital's rule for " $\left(\frac{ \pm \infty}{ \pm \infty}\right)$ " indeterminate forms, we have

$$
\lim _{x \rightarrow x_{0}} f(x)=\lim _{x \rightarrow x_{0}}\left(\frac{g(x)}{h(x)}\right)=\lim _{x \rightarrow x_{0}}\left(\frac{g^{\prime}(x)}{h^{\prime}(x)}\right)=L
$$