# Linear algebra

# Vector and matrix arithmetic 


## References and reading guide

- {cite:ps}`anton1987`: Chapters 1-3 (pp. 1-142).
- {cite:ps}`asano2013`: Appendix A (pp. 218-242).
- {cite:ps}`basilevsky1983`: Chapters 1-4 (pp. 1-187).
- {cite:ps}`bradley2008`: Chapter 9 (pp. 475-536).
- {cite:ps}`chiang2005`: Chapters 4 and 5.
- {cite:ps}`haeussler1987`: Chapter 8 (pp. 238-309).
- {cite:ps}`shannon1995`: Chapters 4 and 5 (pp. 121-230).
- {cite:ps}`sydsaeter2006`: Chapter 15 (pp. 549-590).
- {cite:ps}`simon1994`: Chapters 6-11 and 26-28 (pp. 107-250 and 719-799).





## What is a matrix?

A **matrix** is an array of numbers or variables. It is organised into rows and columns. These form its **dimensions**.

An $(n \times m)$ matrix has $n$ rows and $m$ columns. Note that, while it is possible that $n=m$, it is also possible that $n \neq m$. When $n=m$, we say that the matrix is a **square matrix**.

An $(n \times m)$ matrix takes the following form:

$$
A=\left(\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 m} \\
a_{21} & a_{22} & a_{23} & \cdots & a_{2 m} \\
a_{31} & a_{32} & a_{33} & \cdots & a_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & a_{n 3} & \cdots & a_{n m}
\end{array}\right)
$$


### Scalars and vectors

A **scalar** is a real number $(a \in \mathbb{R})$.

A **column vector** is an $(n \times 1)$ matrix of the form

$$
A=\left(\begin{array}{c}
a_{11} \\
a_{21} \\
a_{31} \\
\vdots \\
a_{n 1}
\end{array}\right)
$$

A **row vector** is a $(1 \times m)$ matrix of the form

$$
A=\left(\begin{array}{lllll}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 m}
\end{array}\right) .
$$


### Economic examples of matrices and vectors

- The $(L \times 1)$ price vector for an economy there are $L$ commodities:

$$
p=\left(\begin{array}{c}
p_{1} \\
p_{2} \\
p_{3} \\
\vdots \\
p_{L}
\end{array}\right)
$$

- The $(L \times 1)$ Marshallian demand vector for a consumer whose preferences are defined over bundles of $L$ commodities:

$$
x(p, y)=\left(\begin{array}{c}
x_{1}(p, y) \\
x_{2}(p, y) \\
x_{3}(p, y) \\
\vdots \\
x_{L}(p, y)
\end{array}\right)
$$


- The $(n \times k)$ "design matrix" (that is, matrix of independent variables) in a multivariate linear regression model of the form

$$
y_{i}=\beta_{0}+\sum_{j=1}^{k-1} \beta_{j} x_{i, j}+\epsilon_{i}
$$

where there are observations of all of the observable variables for each of $n$ sample points.

- This matrix takes the form

$$
X=\left(\begin{array}{ccccc}
1 & x_{1,2} & x_{1,3} & \cdots & x_{1,(k-1)} \\
1 & x_{2,2} & x_{2,3} & \cdots & x_{2,(k-1)} \\
1 & x_{3,2} & x_{3,3} & \cdots & x_{3,(k-1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n, 2} & x_{n, 3} & \cdots & x_{n,(k-1)}
\end{array}\right)
$$



## An overview of matrix arithmetic

- Scalar multiplication of a matrix
- Matrix addition
- Matrix subtraction
- Matrix multiplication: The inner, or dot, product
- The transpose of a matrix and matrix symmetry
- The additive inverse of a matrix and the null matrix
- The multiplicative inverse of a matrix and the identity matrix
- Idempotent matrices
- Vector inequalities



### Scalar multiplication

Suppose that $c \in \mathbb{R}$ and $A$ is an $(n \times m)$ matrix takes the following form:

$$
A=\left(\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 m} \\
a_{21} & a_{22} & a_{23} & \cdots & a_{2 m} \\
a_{31} & a_{32} & a_{33} & \cdots & a_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & a_{n 3} & \cdots & a_{n m}
\end{array}\right)
$$

We will assume that $a_{i j} \in \mathbb{R}$ for all

$$
(i, j) \in\{1,2, \cdots, n\} \times\{1,2, \cdots, m\} .
$$


The scalar pre-product of this constant with this matrix is given by

$$
\begin{aligned}
c A & =c\left(\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 m} \\
a_{21} & a_{22} & a_{23} & \cdots & a_{2 m} \\
a_{31} & a_{32} & a_{33} & \cdots & a_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & a_{n 3} & \cdots & a_{n m}
\end{array}\right) \\[20pt]
& =\left(\begin{array}{ccccc}
c a_{11} & c a_{12} & c a_{13} & \cdots & c a_{1 m} \\
c a_{21} & c a_{22} & c a_{23} & \cdots & c a_{2 m} \\
c a_{31} & c a_{32} & c a_{33} & \cdots & c a_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
c a_{n 1} & c a_{n 2} & c a_{n 3} & \cdots & c a_{n m}
\end{array}\right)
\end{aligned}
$$


The scalar post-product of the matrix with constant is given by

$$
\begin{aligned}
A c & =\left(\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 m} \\
a_{21} & a_{22} & a_{23} & \cdots & a_{2 m} \\
a_{31} & a_{32} & a_{33} & \cdots & a_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & a_{n 3} & \cdots & a_{n m}
\end{array}\right) c \\[20pt]
& =\left(\begin{array}{ccccc}
c a_{11} & c a_{12} & c a_{13} & \cdots & c a_{1 m} \\
c a_{21} & c a_{22} & c a_{23} & \cdots & c a_{2 m} \\
c a_{31} & c a_{32} & c a_{33} & \cdots & c a_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
c a_{n 1} & c a_{n 2} & c a_{n 3} & \cdots & c a_{n m}
\end{array}\right)
\end{aligned}
$$

Note that $c A=A c$. As such, we can just talk about the **scalar product** of a constant with a matrix, without specifying the order in which the multiplication takes place.


### Some examples of scalar multiplication of a matrix

The following examples come from {cite:ps}`asano2013` (pp. 222-224).

```{dropdown} Example 1

$$
2 X=2\left(\begin{array}{ll}
1 & 1 \\
1 & 2
\end{array}\right)=\left(\begin{array}{ll}
2(1) & 2(1) \\
2(1) & 2(2)
\end{array}\right)=\left(\begin{array}{ll}
2 & 2 \\
2 & 4
\end{array}\right)
$$
```

```{dropdown} Example 2

$$
3 Y=3\left(\begin{array}{ll}
2 & 1 \\
2 & 4
\end{array}\right)=\left(\begin{array}{ll}
3(2) & 3(1) \\
3(2) & 3(4)
\end{array}\right)=\left(\begin{array}{cc}
6 & 3 \\
6 & 12
\end{array}\right)
$$
```

```{dropdown} Example 3

$$
2 Z=2\left(\begin{array}{cc}
-1 & -2 \\
3 & 1
\end{array}\right)=\left(\begin{array}{cc}
2(-1) & 2(-2) \\
2(3) & 2(1)
\end{array}\right)=\left(\begin{array}{cc}
-2 & -4 \\
6 & 2
\end{array}\right)
$$
```


The following examples come from {cite:ps}`sydsaeter2006` (pp. 555-556).

```{dropdown} Example 4

$$
\begin{gathered}
3 A=2\left(\begin{array}{ccc}
1 & 2 & 0 \\
4 & -3 & 1
\end{array}\right)=\left(\begin{array}{ccc}
3(1) & 3(2) & 3(0) \\
3(4) & 3-(3) & 3(1)
\end{array}\right) \\
=\left(\begin{array}{ccc}
3 & 6 & 0 \\
12 & -9 & 3
\end{array}\right)
\end{gathered}
$$
```

```{dropdown} Example 5

$$
\begin{aligned}
& \left(\frac{-1}{2}\right) B=2\left(\begin{array}{lll}
0 & 1 & 2 \\
1 & 0 & 2
\end{array}\right)=\left(\begin{array}{ccc}
\left(\frac{-1}{2}\right)(0) & \left(\frac{-1}{2}\right)(1) & \left(\frac{-1}{2}\right)(2) \\
\left(\frac{-1}{2}\right)(1) & \left(\frac{-1}{2}\right)(0) & \left(\frac{-1}{2}\right)(2)
\end{array}\right) \\
& =\left(\begin{array}{ccc}
0 & \frac{-1}{2} & -1 \\
\frac{-1}{2} & 0 & -1
\end{array}\right)
\end{aligned}
$$
```


### Matrix addition

The sum of two matrices is only defined if the two matrices have exactly the same dimensions.  

Suppose that $A$ is an $(n \times m)$ matrix that takes the following form:

$$
A=\left(\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 m} \\
a_{21} & a_{22} & a_{23} & \cdots & a_{2 m} \\
a_{31} & a_{32} & a_{33} & \cdots & a_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & a_{n 3} & \cdots & a_{n m}
\end{array}\right)
$$

Suppose that $B$ is an $(n \times m)$ matrix that takes the following form:

$$
B=\left(\begin{array}{ccccc}
b_{11} & b_{12} & b_{13} & \cdots & b_{1 m} \\
b_{21} & b_{22} & b_{23} & \cdots & b_{2 m} \\
b_{31} & b_{32} & b_{33} & \cdots & b_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
b_{n 1} & b_{n 2} & b_{n 3} & \cdots & b_{n m}
\end{array}\right)
$$


The **matrix sum** $(A+B)$ is an $(n \times m)$ matrix that takes the following form:

$$
\begin{aligned}
A+B
&=\left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 m} \\
a_{21} & a_{22} & \cdots & a_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & \cdots & a_{n m}
\end{array}\right)+\left(\begin{array}{cccc}
b_{11} & b_{12} & \cdots & b_{1 m} \\
b_{21} & b_{22} & \cdots & b_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
b_{n 1} & b_{n 2} & \cdots & b_{n m}
\end{array}\right) \\[20pt]
&=\left(\begin{array}{cccc}
a_{11}+b_{11} & a_{12}+b_{12} & \cdots & a_{1 m}+b_{1 m} \\
a_{21}+b_{21} & a_{22}+b_{22} & \cdots & a_{2 m}+b_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1}+b_{n 1} & a_{n 2}+b_{n 2} & \cdots & a_{n m}+b_{n m}
\end{array}\right)
\end{aligned}
$$

Note that $A+B=B+A$. (Exercise: Convince yourself of the validity of this claim.)


### Some examples of matrix addition

- Suppose that $A$ is an $(m \times n)$ matrix, $B$ is an $(n \times m)$ matrix and $C$ is an $(n \times p)$ matrix, where $m \neq n, m \neq p$ and $n \neq p$.
- Example 1: Neither the matrix sum $A+B$ nor the matrix sum $B+A$ are defined.
- Example 2: Neither the matrix sum $A+C$ nor the matrix sum $C+A$ are defined.
- Example 3: Neither the matrix sum $B+C$ nor the matrix sum $C+B$ are defined.


The following examples come from {cite:ps}`asano2013` (pp. 222-224).

```{dropdown} Example 4

$$
X+Y=\left(\begin{array}{ll}
1 & 1 \\
1 & 2
\end{array}\right)+\left(\begin{array}{ll}
1 & 0 \\
1 & 2
\end{array}\right)=\left(\begin{array}{ll}
1+1 & 1+0 \\
1+1 & 2+2
\end{array}\right)=\left(\begin{array}{ll}
2 & 1 \\
2 & 4
\end{array}\right)
$$
```

```{dropdown} Example 5

$$
\begin{aligned}
& X+Z=\left(\begin{array}{ll}
1 & 1 \\
1 & 2
\end{array}\right)+\left(\begin{array}{ll}
-1 & 3 \\
-2 & 1
\end{array}\right)=\left(\begin{array}{ll}
1+(-1) & 1+3 \\
1+(-2) & 2+1
\end{array}\right) 
 =\left(\begin{array}{cc}
0 & 4 \\
-1 & 3
\end{array}\right)
\end{aligned}
$$
```

The following example comes from {cite:ps}`sydsaeter2006` (pp. 555-556).
```{dropdown} Example 6

$$
\begin{gathered}
M+N=\left(\begin{array}{ccc}
1 & 2 & 0 \\
4 & -3 & -1
\end{array}\right)+\left(\begin{array}{lll}
0 & 1 & 2 \\
1 & 0 & 2
\end{array}\right) 
=\left(\begin{array}{ccc}
1+0 & 2+1 & 0+2 \\
4+1 & -3+0 & -1+2
\end{array}\right) 
=\left(\begin{array}{ccc}
1 & 3 & 2 \\
5 & -3 & 1
\end{array}\right)
\end{gathered}
$$
```


### An economic example of matrix addition

- Suppose that there are $I$ consumers in an economy, each of whom has preferences defined over bundles of the same $L$ commodities.
- Suppose that consumer $i$'s Marshallian demand vector is given by the column vector

$$
x^{i}\left(p, y^{i}\right)=\left(\begin{array}{c}
x_{1}^{i}\left(p, y^{i}\right) \\
x_{2}^{i}\left(p, y^{i}\right) \\
x_{3}^{i}\left(p, y^{i}\right) \\
\vdots \\
x_{L}^{i}\left(p, y^{i}\right)
\end{array}\right)
$$

- The aggegate vector of Marshallian demands (or, if you prefer, the vector of market demands) for this economy is given by

$$
\begin{aligned}
x\left(p, y^{1}, y^{2}, \cdots, y^{\prime}\right)
&=\sum_{i=1}^{I} x^{i}\left(p, y^{i}\right) \\[20pt]
&=\sum_{i=1}^{I}\left(\begin{array}{c}
x_{1}^{i}\left(p, y^{i}\right) \\
x_{2}^{i}\left(p, y^{i}\right) \\
x_{3}^{i}\left(p, y^{i}\right) \\
\vdots \\
x_{L}^{i}\left(p, y^{i}\right)
\end{array}\right) \\[20pt]
&= \left(\begin{array}{c}
\sum_{i=1}^{I} x_{1}^{i}\left(p, y^{i}\right) \\
\sum_{i=1}^{I} x_{2}^{i}\left(p, y^{i}\right) \\
\sum_{i=1}^{I} x_{3}^{i}\left(p, y^{i}\right) \\
\vdots \\
\sum_{i=1}^{I} x_{L}^{i}\left(p, y^{i}\right)
\end{array}\right)
\end{aligned}
$$


### Matrix subtraction

Matrix subtraction involves a combination of (i) scalar multiplication of a matrix, and (ii) matrix addition.

As with matrix addition, the difference of two matrices is only defined if the two matrices have exactly the same dimensions.

Suppose that $A$ and $B$ are both $(n \times m)$ matrices. The difference between $A$ and $B$ is defined to be

$$
A-B=A+(-1) B
$$

Since $A$ and $B$ are both $(n \times m)$ matrices, the matrix difference $(A-B)$ is an $(n \times m)$ matrix that takes the following form:

$$
\begin{aligned}
A-B &= A+(-1) B \\[20pt]
&= \left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 m} \\
a_{21} & a_{22} & \cdots & a_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & \cdots & a_{n m}
\end{array}\right)+(-1)\left(\begin{array}{cccc}
b_{11} & b_{12} & \cdots & b_{1 m} \\
b_{21} & b_{22} & \cdots & b_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
b_{n 1} & b_{n 2} & \cdots & b_{n m}
\end{array}\right) \\[20pt]
&= \left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 m} \\
a_{21} & a_{22} & \cdots & a_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & \cdots & a_{n m}
\end{array}\right)+\left(\begin{array}{cccc}
-b_{11} & -b_{12} & \cdots & -b_{1 m} \\
-b_{21} & -b_{22} & \cdots & -b_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
-b_{n 1} & -b_{n 2} & \cdots & -b_{n m}
\end{array}\right) \\[20pt]
&= \left(\begin{array}{cccc}
a_{11}-b_{11} & a_{12}-b_{12} & \cdots & a_{1 m}-b_{1 m} \\
a_{21}-b_{21} & a_{22}-b_{22} & \cdots & a_{2 m}-b_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1}-b_{n 1} & a_{n 2}-b_{n 2} & \cdots & a_{n m}-b_{n m}
\end{array}\right)
\end{aligned}
$$

In general, $A-B \neq B-A$. Thus matrix subtraction does not share all of the properties of matrix addition.

Exercise: Under what circumstances will $A-B=B-A$ ?


```{dropdown} Example
The following example comes from {cite:ps}`asano2013` (pp. 222-224).

$$
\begin{aligned}
X-Y &=\left(\begin{array}{cc}
6 & 3 \\
6 & 12
\end{array}\right)-\left(\begin{array}{cc}
-2 & -4 \\
6 & 2
\end{array}\right) \\
& =\left(\begin{array}{cc}
6 & 3 \\
6 & 12
\end{array}\right)+(-1)\left(\begin{array}{cc}
-2 & -4 \\
6 & 2
\end{array}\right) \\
& =\left(\begin{array}{cc}
6 & 3 \\
6 & 12
\end{array}\right)+\left(\begin{array}{cc}
(-1)(-2) & (-1)(-4) \\
(-1)(6) & (-1)(2)
\end{array}\right) \\
& =\left(\begin{array}{cc}
6 & 3 \\
6 & 12
\end{array}\right)+\left(\begin{array}{cc}
2 & 4 \\
-6 & -2
\end{array}\right) \\
& =\left(\begin{array}{cc}
6+2 & 3+4 \\
6+(-6) & 12+(-2)
\end{array}\right) \\
& =\left(\begin{array}{cc}
8 & 7 \\
0 & 10
\end{array}\right)
\end{aligned}
$$

```


### Matrix multiplication

- The standard matrix product is the dot, or inner, product of two matrices.
- The dot product of two matrices is only defined for cases in which the number of columns of the first listed matrix is identical to the number of rows of the second listed matrix.
- If the dot product is defined, the solution matrix will have the same number of rows as the first listed matrix and the same number of columns as the second listed matrix.


Suppose that $X$ is an $(m \times n)$ matrix, $Y$ is an $(n \times m)$ matrix and $Z$ is an $(n \times p)$ matrix, where $m \neq n, m \neq p$ and $n \neq p$.
- The matrix product $X Y$ is defined and will be an $(m \times m)$ matrix.
- The matrix product $Y X$ is defined and will be an $(n \times n)$ matrix.
- The matrix product $X Z$ is defined and will be an $(m \times p)$ matrix.
- The matrix products $Z X, Y Z$ and $Z Y$ are not defined.


Suppose that $A$ is an $(n \times m)$ matrix that takes the following form:

$$
A=\left(\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 m} \\
a_{21} & a_{22} & a_{23} & \cdots & a_{2 m} \\
a_{31} & a_{32} & a_{33} & \cdots & a_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & a_{n 3} & \cdots & a_{n m}
\end{array}\right)
$$

Suppose that $B$ is an $(m \times p)$ matrix that takes the following form:

$$
B=\left(\begin{array}{ccccc}
b_{11} & b_{12} & b_{13} & \cdots & b_{1 p} \\
b_{21} & b_{22} & b_{23} & \cdots & b_{2 p} \\
b_{31} & b_{32} & b_{33} & \cdots & b_{3 p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
b_{m 1} & b_{m 2} & b_{m 3} & \cdots & b_{m p}
\end{array}\right)
$$


The **matrix product** $A B$ is defined and will be an $(n \times p)$ matrix. The solution matrix is given by

$$
\begin{aligned}
AB
&=\left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 m} \\
a_{21} & a_{22} & \cdots & a_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & \cdots & a_{n m}
\end{array}\right) \cdot\left(\begin{array}{cccc}
b_{11} & b_{12} & \cdots & b_{1 p} \\
b_{21} & b_{22} & \cdots & b_{2 p} \\
\vdots & \vdots & \ddots & \vdots \\
b_{m 1} & b_{m 2} & \cdots & b_{m p}
\end{array}\right) \\[20pt]
&=\left(\begin{array}{ccccc}
\sum_{k=1}^{m} a_{1 k} b_{k 1} & \sum_{k=1}^{m} a_{1 k} b_{k 2} & \cdots & \sum_{k=1}^{m} a_{1 k} b_{k p} \\
\vdots & a_{2 k} b_{k 1} & \sum_{k=1}^{m} a_{2 k} b_{k 2} & \cdots & \sum_{k=1}^{m} a_{2 k} b_{k p} \\
\sum_{k=1}^{m} a_{n k} b_{k 1} & \sum_{k=1}^{m} a_{n k} b_{k 2} & \cdots & \sum_{k=1}^{m} a_{n k} b_{k p}
\end{array}\right)
\end{aligned}
$$

Note that, while it is possible for $A B=B A$ in some cases, in general we will have $A B \neq B A$. There are three reasons for this:
- First, $B A$ will not necessarily be defined even if $A B$ is defined.
- Second, even when both $A B$ and $B A$ are defined, they might have different dimensions.
- Third, even when both $A B$ and $B A$ are defined and have the same dimensions, they might have one or more different entries.


These examples come from {cite:ps}`bradley2008` (pp. 490-492).

```{dropdown} Example 1

$$
\begin{aligned}
AB & =\left(\begin{array}{cc}
1 & 2 \\
-2 & 4
\end{array}\right) \cdot\left(\begin{array}{lll}
0 & 2 & 2 \\
1 & 0 & 5
\end{array}\right) \\
&=\left(\begin{array}{ccc}
(1)(0)+(2)(1) & (1)(2)+(2)(0) & (1)(2)+(2)(5) \\
(-2)(0)+(4)(1) & (-2)(2)+(4)(0) & (-2)(2)+(4)(5)
\end{array}\right) \\
&=\left(\begin{array}{ccc}
0+2 & 2+0 & 2+10 \\
0+4 & -4+0 & -4+20
\end{array}\right) \\
&=\left(\begin{array}{ccc}
2 & 2 & 12 \\
4 & -4 & 16
\end{array}\right)
\end{aligned}
$$
```

```{dropdown} Example 2
The matrix product $B A$ is undefined because the number of columns in $B$ (which is three) does not equal the number of rows in $A$ (which is two).

Note that $A B \neq B A$.

```

```{dropdown} Example 3

$$
\begin{aligned}
A C &=\left(\begin{array}{cc}
1 & 2 \\
-2 & 4
\end{array}\right) \cdot\left(\begin{array}{cc}
3 & -2 \\
5 & 0
\end{array}\right) \\
&=\left(\begin{array}{cc}
(1)(3)+(2)(5) & (1)(-2)+(2)(0) \\
(-2)(3)+(4)(5) & (-2)(-2)+(4)(0)
\end{array}\right) \\
&=\left(\begin{array}{cc}
3+10 & -2+0 \\
-6+20 & 4+0
\end{array}\right) \\
&=\left(\begin{array}{cc}
13 & -2 \\
14 & 4
\end{array}\right)
\end{aligned}
$$
```

```{dropdown} Example 4

$$
\begin{aligned}
C A &=\left(\begin{array}{cc}
3 & -2 \\
5 & 0
\end{array}\right) \cdot\left(\begin{array}{cc}
1 & 2 \\
-2 & 4
\end{array}\right) \\
&=\left(\begin{array}{cc}
(3)(1)+(-2)(-2) & (3)(2)+(-2)(4) \\
(5)(1)+(0)(-2) & (5)(2)+(0)(4)
\end{array}\right) \\
&=\left(\begin{array}{cc}
3+4 & 6+(-8) \\
5+0 & 10+0
\end{array}\right) \\
&=\left(\begin{array}{cc}
7 & -2 \\
5 & 10
\end{array}\right)
\end{aligned}
$$

Note that $A C \neq C A$.
```


#### An economic example of matrix multiplication

Suppose that a consumer whose preferences are defined over bundles of $L$ commodities faces a price vector given by the row vector $p=\left(p_{1}, p_{2}, \cdots, p_{L}\right)$ and chooses to purchase the quantities of each commodity that are given by the column vector

$$
q=\left(\begin{array}{c}
q_{1} \\
q_{2} \\
q_{3} \\
\vdots \\
q_{L}
\end{array}\right)
$$

The consumer's total expenditure will be equal to

$$
\begin{gathered}
p q=\left(p_{1}, p_{2}, \cdots, p_{L}\right)\left(\begin{array}{c}
q_{1} \\
q_{2} \\
q_{3} \\
\vdots \\
q_{L}
\end{array}\right) \\
=p_{1} q_{1}+p_{2} q_{2}+\cdots+p_{L} q_{L} \\
=\sum_{I=1}^{L} p_{I} q_{I} .
\end{gathered}
$$


### Matrix transposition

Suppose that $A$ is an $(n \times m)$ matrix. The **transpose** of the matrix $A$, which is denoted by $A^{T}$, is the $(m \times n)$ matrix that is formed by taking the rows of $A$ and turning them into columns, without changing their order. In other words, the $i$ th column of $A^{T}$ is the ith row of $A$. This also means that the jth row of $A^{T}$ is the $j$ th column of $A$.


Suppose that $A$ is the $(n \times m)$ matrix that takes the following form:

$$
A=\left(\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 m} \\
a_{21} & a_{22} & a_{23} & \cdots & a_{2 m} \\
a_{31} & a_{32} & a_{33} & \cdots & a_{3 m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & a_{n 3} & \cdots & a_{n m}
\end{array}\right)
$$

The **transpose** of the matrix $A$ is the $(m \times n)$ matrix that takes the following form:

$$
A^{T}=\left(\begin{array}{ccccc}
a_{11} & a_{21} & a_{31} & \cdots & a_{n 1} \\
a_{12} & a_{22} & a_{32} & \cdots & a_{n 2} \\
a_{13} & a_{23} & a_{33} & \cdots & a_{n 3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{1 m} & a_{2 m} & a_{3 m} & \cdots & a_{n m}
\end{array}\right)
$$




```{dropdown} Examples
These examples come from {cite:ps}`shannon1995` (p. 139, Example 8).

- Example 1: If $
x=\left(\begin{array}{l}
1 \\
3 \\
5
\end{array}\right)
$ then $X^{T}=(1,3,5)$.  
<br></br>

- Example 2: If $
Y=\left(\begin{array}{ll}
2 & 3 \\
5 & 9 \\
7 & 6
\end{array}\right)
$ then $
Y^{T}=\left(\begin{array}{lll}
2 & 5 & 7 \\
3 & 9 & 6
\end{array}\right)
$.  
<br></br>

- Example 3: If $
Z=\left(\begin{array}{ccc}
1 & 3 & 7 \\
4 & 5 & 11 \\
6 & 8 & 10
\end{array}\right)
$ then $
Z^{T}=\left(\begin{array}{ccc}
1 & 4 & 6 \\
3 & 5 & 8 \\
7 & 11 & 10
\end{array}\right)
$.  
```


### Symmetric matrices

In general, $A^{T} \neq A$. There are two reasons for this:
- First, unless $A$ is a square matrix (that is, unless it has the same number of rows and columns), the dimensions of the matrix $A^{T}$ will be different to the dimensions of the matrix $A$.
- Second, even if $A$ is a square matrix, in general the $i$th row of $A$ will not be identical to the $i$th column of $A$. As such, in general we will have $A^{T} \neq A$ even for square matrices.

If it is the case that $A^{T}=A$, then we say that $A$ is a **symmetric matrix**.


```{dropdown} Examples
- Example 1:
$
A=\left(\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right)=A^{T}
$
<br></br>

- Example 2:
$
B=\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right)=B^{T} 
$
<br></br>

- Example 3:
$
C=\left(\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right)=C^{T} 
$
<br></br>

- Example 4:
$
D=\left(\begin{array}{ll}
1 & 0 \\
0 & 0
\end{array}\right)=D^{T} 
$
```


### Null matrices

A **null matrix** (or vector) is a matrix that consists solely of zeroes. For example, the $(2 \times 2)$ null matrix is

$$
0=\left(\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right)
$$

The $(n \times m)$ null matrix is the ADDITIVE identity matrix for the space of all $(n \times m)$ null matrices. This means that if $A$ is an $(n \times m)$ matrix, then $A+0=0+A=A$.


### Additive inverses

Suppose that $A$ is an $(n \times m)$ matrix and 0 is the $(n \times m)$ null matrix.
The $(n \times m)$ matrix $B$ is the **additive inverse** of $A$ if and only if $A+B=B+A=0$.
Suppose that

$$
A=\left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 m} \\
a_{21} & a_{22} & \cdots & a_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & \cdots & a_{n m}
\end{array}\right)
$$

The additive inverse of $A$ is

$$
B=-A=\left(\begin{array}{cccc}
-a_{11} & -a_{12} & \cdots & -a_{1 m} \\
-a_{21} & -a_{22} & \cdots & -a_{2 m} \\
\vdots & \vdots & \ddots & \vdots \\
-a_{n 1} & -a_{n 2} & \cdots & -a_{n m}
\end{array}\right)
$$

#### An example of an additive inverse matrix

Note that

$$
\begin{aligned}
A+B &=\left(\begin{array}{ll}
1 & 2 \\
4 & 3
\end{array}\right)+\left(\begin{array}{ll}
-1 & -2 \\
-4 & -3
\end{array}\right) \\[20pt]
&=\left(\begin{array}{ll}
1+(-1) & 2+(-2) \\
4+(-4) & 3+(-3)
\end{array}\right) \\[20pt]
&=\left(\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right)
=0_{(2 \times 2)}
\end{aligned}
$$

Note also that

$$
\begin{aligned}
B+A &=\left(\begin{array}{ll}
-1 & -2 \\
-4 & -3
\end{array}\right)+\left(\begin{array}{ll}
1 & 2 \\
4 & 3
\end{array}\right) \\[20pt]
&=\left(\begin{array}{ll}
-1+1 & -2+2 \\
-4+4 & -3+3
\end{array}\right)\\[20pt]
&=\left(\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right)=0_{(2 \times 2)}
\end{aligned}
$$

Since $A+B=B+A=0_{(2 \times 2)}$, we can conclude that $B$ is the additive inverse for $A$.


### Identity matrices

An **identity matrix** is a square matrix that has ones on the main (north-west to south-east) diagonal and zeros everywhere else. For example, the $(2 \times 2)$ identity matrix is

$$
I=\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right)
$$

The $(n \times n)$ identity matrix is the MULTIPLICATIVE identity matrix for any relevant space of matrices:
- If $A$ is an $(n \times n)$ matrix, then $A I=I A=A$.
- If $A$ is an $(m \times n)$ matrix, then $A I=A$.
- If $A$ is an $(n \times m)$ matrix, then $I A=A$.


### Multiplicative inverses

Only square matrices have any chance of having a multiplicative inverse. Some, but not all, square matrices will have a multiplicative inverse. Suppose that $A$ is an $(n \times n)$ matrix and $I$ is the $(n \times n)$ identity matrix.

The $(n \times n)$ matrix $B$ is the **multiplicative inverse** (usually just referred to as the inverse) of $A$ if and only if $A B=B A=I$.

- A square matrix that has an inverse is said to be **non-singular**.
- A square matrix that does not have an inverse is said to be **singular**.
- We will talk about methods for determining whether or not a matrix is non-singular later in this unit.
- We will talk about methods for finding an inverse matrix, if it exists, later in this unit.
- Useful fact: "The transpose of the inverse is equal to the inverse of the transpose".
    - If $A$ is a non-singular square matrix whose multiplicative inverse is $A^{-1}$, then we have $\left(A^{-1}\right)^{T}=\left(A^{T}\right)^{-1}$.


#### An example of a multiplicative inverse matrix

This example comes from {cite:ps}`haeussler1987` (p. 278, Example 1).
- Note that

$$
\begin{aligned}
A B &=\left(\begin{array}{ll}
1 & 2 \\
3 & 7
\end{array}\right) \cdot\left(\begin{array}{cc}
7 & -2 \\
-3 & 1
\end{array}\right) \\[10pt]
&=\left(\begin{array}{cc}
(1)(7)+(2)(-3) & (1)(-2)+(2)(1) \\
(3)(7)+(7)(-3) & (3)(-2)+(7)(1)
\end{array}\right) \\[10pt]
&=\left(\begin{array}{cc}
7+(-6) & -2+2 \\
21+(-21) & -6+7
\end{array}\right) \\[10pt]
&=\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right) \\[10pt]
&= I
\end{aligned}
$$

Note that

$$
\begin{aligned}
B A &=\left(\begin{array}{cc}
7 & -2 \\
-3 & 1
\end{array}\right) \cdot\left(\begin{array}{cc}
1 & 2 \\
3 & 7
\end{array}\right) \\[10pt]
&=\left(\begin{array}{cc}
(7)(1)+(-2)(3) & (7)(2)+(-2)(7) \\
(-3)(1)+(1)(3) & (-3)(2)+(1)(7)
\end{array}\right) \\[10pt]
&=\left(\begin{array}{cc}
7+(-6) & 14+(-14) \\
-3+3 & -6+7
\end{array}\right) \\[10pt]
&=\left(\begin{array}{cc}
1 & 0 \\
0 & 1
\end{array}\right) \\[10pt]
&= I
\end{aligned}
$$

Since $A B=B A=I$, we can conclude that $A^{-1}=B$.


### Idempotent matrices

- A matrix $A$ is said to be **idempotent** if and only if $A A=A$.
- Clearly a NECESSARY condition for matrix to be idempotent is that $A$ be a square matrix. (Exercise: Explain why this is the case.)
- However, this is NOT a SUFFICIENT condition for a matrix to be idempotent. In general, $A A \neq A$, even for square matrices.
- Two examples of idempotent matrices that you have already encountered are square null matrices and identity matrices.
- We will shortly encounter two more examples. These are the Hat matrix $(P)$ and the residual-making matrix $(M=I-P)$ from statistics and econometrics.


#### Some examples of idempotent matrices

- The $(2 \times 2)$ identity matrix:

$$
\begin{aligned}
l_{2} \cdot I_{2}
&=\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right) \cdot\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right) \\
&=\left(\begin{array}{ll}
(1)(1)+(0)(0) & (1)(0)+(0)(1) \\
(0)(1)+(1)(0) & (0)(0)+(1)(1)
\end{array}\right) \\
&=\left(\begin{array}{ll}
1+0 & 0+0 \\
0+0 & 0+1
\end{array}\right) \\
&=\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right) \\
&=I_{2}
\end{aligned}
$$


- The $(2 \times 2)$ null matrix:

$$
\begin{aligned}
0_{(2 \times 2)} \cdot 0_{(2 \times 2)}
&=\left(\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right) \cdot\left(\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right) \\
&=\left(\begin{array}{cc}
(0)(0)+(0)(0) & (0)(0)+(0)(0) \\
(0)(0)+(0)(0) & (0)(0)+(0)(0)
\end{array}\right) \\
&=\left(\begin{array}{ll}
0+0 & 0+0 \\
0+0 & 0+0
\end{array}\right) \\
&=\left(\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right) \\
&=0_{(2 \times 2)}
\end{aligned}
$$



### Econometric example: the classical linear regression model

One of simplest models that you will encounter in statistics and econometrics is the classical linear regression model (CLRM). This model takes the form

$$
Y=X \beta+\epsilon
$$

where $Y$ is an $(n \times 1)$ vector of $n$ observations on a single dependent variable, $X$ is an $(n \times k)$ matrix of $n$ observations on $k$ independent variables, $\beta$ is a $(k \times 1)$ vector of unknown parameters and $\epsilon$ is an $(n \times 1)$ vector of random disturbances.

In the CLRM, the joint distribution of the random disturbances, conditional on $X$, is given by

$$
\epsilon \mid X \sim N\left(0, \sigma^{2} I\right)
$$

where 0 is an $(n \times 1)$ null vector, $l$ is an $(n \times n)$ identity matrix and $\sigma^{2}$ is an unknown parameter.

#### Matrices associated with the CLRM

- The ordinary least squares estimator (and, in the case of the CLRM, maximum likelihood estimator) of the parameter vector $\beta$ in the CLRM is given by

$$
b=\left(X^{T} X\right)^{-1} X^{T} Y
$$

- The hat matrix for the CLRM is given by

$$
P=X\left(X^{T} X\right)^{-1} X^{T}
$$

- The residual-making matrix for the CLRM is given by

$$
\begin{aligned}
M & =I-P \\
& =I-X\left(X^{T} X\right)^{-1} X^{T}
\end{aligned}
$$

#### The hat matrix is symmetric

$$
\begin{aligned}
P^{T} & =\left(X\left(X^{T} X\right)^{-1} X^{T}\right)^{T} \\
& =\left(X^{T}\right)^{T}\left(\left(X^{T} X\right)^{-1}\right)^{T}(X)^{T} \\
& =X\left(\left(X^{T} X\right)^{T}\right)^{-1} X^{T} \\
& =X\left((X)^{T}\left(X^{T}\right)^{T}\right)^{-1} X^{T} \\
& =X\left(X^{T} X\right)^{-1} X^{T} \\
& =P
\end{aligned}
$$

#### The hat matrix is idempotent

$$
\begin{aligned}
P P & =\left(x\left(x^{T} x\right)^{-1} x^{T}\right)\left(x\left(x^{T} x\right)^{-1} x^{T}\right) \\
& =x\left(x^{T} x\right)^{-1} x^{T} x\left(x^{T} x\right)^{-1} x^{T} \\
& =x\left(x^{T} x\right)^{-1} I X^{T} \\
& =x\left(x^{T} x\right)^{-1} x^{T} \\
& =P
\end{aligned}
$$

#### The residual-making matrix is symmetric

$$
\begin{aligned}
M^{T} & =(I-P)^{T} \\
& =I^{T}-P^{T} \\
& =I-P \\
& =M
\end{aligned}
$$

#### The residual-making matrix is idempotent

$$
\begin{aligned}
M M & =(I-P)(I-P) \\
& =I I-I P-P I+P P \\
& =I-P-P+P \\
& =I-P \\
& =M
\end{aligned}
$$


## Vector inequalities in Euclidean $n$-space

- Suppose that $x \in \mathbb{R}^{n}$ and $y \in \mathbb{R}^{n}$.
    - This means that $x=\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ where $x_{i} \in \mathbb{R}$ for all $i \in\{1,2, \cdots, n\}$.
    - It also means that means that $y=\left(y_{1}, y_{2}, \cdots, y_{n}\right)$ where $y_{i} \in \mathbb{R}$ for all $i \in\{1,2, \cdots, n\}$.
- We say that $x \geqslant y$ if $x_{i} \geqslant y_{i}$ for all $i \in\{1,2, \cdots, n\}$.
- We say that $x>y$ if $x \geqslant y$ and $x_{k}>y_{k}$ for at least one $k \in\{1,2, \cdots, n\}$.
- We say that $x \gg y$ if $x_{i}>y_{i}$ for all $i \in\{1,2, \cdots, n\}$.
- Note that there are some pairs of vectors that cannot be ranked using these vector inequalities.
- These vector inequalities are illustrated for the case of $\mathbb{R}^{2}$ in the following diagrams.


```{figure} _static/img/lecture_07/vector_inequality_ge.png
:width: 80%
:align: center

Vector inequality diagram 1 ($\geqslant$)
```

```{figure} _static/img/lecture_07/vector_inequality_gt.png
:width: 80%
:align: center

Vector inequality diagram 2 ($>$)
```

```{figure} _static/img/lecture_07/vector_inequality_gg.png
:width: 80%
:align: center

Vector inequality diagram 3 ($\gg$)
```


### Some examples of the vector inequalities

- Example 1: $(5,5) \gg(4,4),(5,5)>(4,4)$, and $(5,5) \geqslant(4,4)$.
- Example 2: $(5,4) \gg(4,3),(5,4)>(4,3)$, and $(5,4) \geqslant(4,3)$.
- Example 3: $(5,5>(5,4)$, and $(5,5) \geqslant(5,4)$.
- Example 4: $(5,5) \geqslant(5,5)$.
- Example 5: $(5,4)$ and $(4,5)$ cannot be compared using any of the three vector inequalities.



### Economic application: consumers who "prefer more to less"

We sometimes want to consider the behaviour of consumers who prefer "more to less".

Suppose that a consumer's preferences are defined over bundles of $L$ commodities. There are two common versions of "more is preferred to less" for such a consumer:
- Monotone preferences: The consumer will strictly prefer bundle $x$ to bundle $y$ if $x \gg y$ (that is, if $x_{l}>y_{l}$ for all $l \in\{1,2, \cdots, L\}$ ).
- Stongly monotone preferences: The consumer will strictly prefer bundle $x$ to bundle $y$ if $x>y$ (that is, if $x_{I} \geqslant y_{l}$ for all $l \in\{1,2, \cdots, L\}$, and $x_{k}>y_{k}$ for at least one $\left.k \in\{1,2, \cdots, L\}\right)$.






<br></br>
<br></br>
<br></br>
<br></br>
<br></br>





# Quadratic Forms 

## References and reading guide

- {cite:ps}`anton1987`: Chapter 6 and Chapter 7 (Section 3).
- {cite:ps}`chiang1984`: Chapter 11 (Section 3).
- {cite:ps}`debreu1952`
- {cite:ps}`hicks1939`: Mathematical Appendix (pp. 303-328).
- {cite:ps}`hicks1946`: Mathematical Appendix (pp. 303-328).
- {cite:ps}`mandy2013`
- {cite:ps}`mandy2018`
- {cite:ps}`mann1943`
- {cite:ps}`samuelson1947`: Mathematical Appendix A (pp. 357-379).
- {cite:ps}`silberberg2001`: Chapter 6.
- {cite:ps}`simon1994`: Chapter 13 (Section 3), Chapter 16, Chapter 17, Chapter 19, Chapter 21, and Chapter 23.
- {cite:ps}`sundaram1996`: Chapter 1 (Section 5).
- {cite:ps}`takayama1993`: Chapter 1 (Section 4).








## What is a quadratic form?

Suppose that

$$
A=\left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 n} \\
a_{21} & a_{22} & \cdots & a_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & \cdots & a_{n n}
\end{array}\right)
$$

is an $(n \times n)$ square matrix whose elements are all fixed parameters (constants) and

$$
x=\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)
$$

is an $(n \times 1)$ column vector whose elements are all variables.

Consider the function $f(x)=x^{T} A x$.

Note that

$$
\begin{aligned}
x^{T} A x & =\left(\begin{array}{llll}
x_{1} & x_{2} & \cdots & x_{n}
\end{array}\right)\left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 n} \\
a_{21} & a_{22} & \cdots & a_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & \cdots & a_{n n}
\end{array}\right)\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right) \\[10pt]
& =\left(\begin{array}{llll}
x_{1} & x_{2} & \cdots & x_{n}
\end{array}\right)\left(\begin{array}{c}
\sum_{i=1}^{n} a_{1 i} x_{i} \\
\vdots \\
\sum_{i=1}^{n} a_{n i}^{n} x_{i} x_{i}
\end{array}\right) \\[10pt]
& =\sum_{j=1}^{n}\left(x_{j}\left(\sum_{i=1}^{n} a_{j i} x_{i}\right)\right) \\
& =\sum_{j=1}^{n}\left(\sum_{i=1}^{n} a_{j i} x_{i} x_{j}\right) \\
& =\sum_{j=1}^{n} \sum_{i=1}^{n} a_{j i} x_{i} x_{j} \\
& =\left(\sum_{j=1}^{n} a_{j j} x_{j}^{2}\right)+\left(\sum_{j \neq i} \sum_{i=1}^{n} a_{j i} x_{i} x_{j}\right) 
\end{aligned}
$$

Clearly $f(x)=x^{T} A x$ is a quadratic function of the variables in the $x$ vector. It is for this reason that $x^{T} A x$ is known as a **quadratic form**.


## Symmetric and non-symmetric matrices

Suppose that a square matrix $A$ is not symmetric, so that $a_{i j} \neq a_{j i}$ for at least one $(i, j)$ pair for which $i \neq j$. We have already shown that

$$
x^{T} A x=\left(\sum_{j=1}^{n} a_{j j} x_{j}^{2}\right)+\left(\sum_{j \neq i} \sum_{i=1}^{n} a_{j i} x_{i} x_{j}\right)
$$

Note that the second component of the right hand side of this expression includes only terms for which either $j<i$ or $j>i$. If we collect like terms, we can rewrite this component as

$$
\sum_{j \neq i} \sum_{i=1}^{n} a_{j i} x_{i} x_{j}=\sum_{j<i} \sum_{i=1}^{n}\left(a_{j i}+a_{i j}\right) x_{i} x_{j}
$$

Suppose that we let $b_{i i}=a_{i i}$ and $b_{i j}=\frac{\left(a_{j i}+a_{i j}\right)}{2}=b_{j i}$.  This yields

$$
\begin{aligned}
x^{T} A x & =\left(\sum_{j=1}^{n} a_{j j} x_{j}^{2}\right)+\left(\sum_{j \neq i} \sum_{i=1}^{n} a_{j i} x_{i} x_{j}\right) \\
& =\left(\sum_{j=1}^{n} a_{j j} x_{j}^{2}\right)+\left(\sum_{j<i} \sum_{i=1}^{n}\left(a_{j i}+a_{i j}\right) x_{i} x_{j}\right) \\
& =\left(\sum_{j=1}^{n} b_{j j} x_{j}^{2}\right)+\left(\sum_{j<i} \sum_{i=1}^{n}\left(\frac{\left(a_{j i}+a_{i j}\right)}{2}+\frac{\left(a_{j i}+a_{i j}\right)}{2}\right) x_{i} x_{j}\right) \\
& =\left(\sum_{j=1}^{n} b_{j j} x_{j}^{2}\right)+\left(\sum_{j<i} \sum_{i=1}^{n}\left(b_{j i}+b_{j i}\right) x_{i} x_{j}\right)\\
& =\left(\sum_{j=1}^{n} b_{j j} x_{j}^{2}\right)+\left(\sum_{j \neq i} \sum_{i=1}^{n} b_{j i} x_{i} x_{j}\right) \\
& =x^{T} B x
\end{aligned}
$$

where

$$
B=\left(\begin{array}{cccc}
b_{11} & b_{12} & \cdots & b_{1 n} \\
b_{12} & b_{22} & \cdots & b_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
b_{1 n} & b_{2 n} & \cdots & b_{n n}
\end{array}\right)
$$

is a symmetric matrix.

Thus any quadratic form $x^{T} A x$ in which the matrix $A$ is not symmetric can also be expressed as a quadratic form $x^{T} B x$ in which the matrix $B$ is symmetric.


## The definiteness of a matrix

The "definiteness" of a square matrix $A$ is related to the sign of the quadratic form $x^{T} A x$ when the $x$ vector is not a null vector.

(Trivially, when $x$ is a null vector (that is, a vector of zeros), the quadratic form $x^{T} A x$ must be equal to zero.)

- The matrix $A$ is said to be **positive definite** if $x^{T} A x>0$ for all $x \neq 0$.
- The matrix $A$ is said to be **positive semi-definite** if $x^{T} A x \geqslant 0$ for all $x \neq 0$.
- The matrix $A$ is said to be **negative semi-definite** if $x^{T} A x \leqslant 0$ for all $x \neq 0$.
- The matrix $A$ is said to be **negative definite** if $x^{T} A x<0$ for all $x \neq 0$.
- The matrix $A$ is said to be **indefinite** if $x^{T} A x>0$ for at least one $x \neq 0$ and $x^{T} A x<0$ for at least one $x \neq 0$.


Sometimes the definition of the various types of matrix definiteness can be used to establish the definiteness of a particular square matrix. But often this is not a particularly convenient method for doing this.

When a square matrix is symmetric, there are two indirect approaches to determining its definiteness that are often much more convenient than attempting to directly employ the definition itself.
- One of these indirect methods involves an examination of the "eigenvalues" of the matrix.
- The other of these indirect methods involves an examination of the "leading principal minors" of the matrix.

But what if a square matrix is not symmetric?
- We can always use the technique discussed above to construct a symmetric square matrix that will have an identical definiteness to the original non-symmetric square matrix.



## Eigenvalues and definiteness

Suppose that, in addition to being a square matrix, the matrix $A$ is a symmetric matrix, so that $A^{T}=A$.
- The matrix $A$ will be positive definite if and only if all of its eigenvalues are strictly positive $(>0)$.
- The matrix $A$ will be positive semi-definite if and only if all of its eigenvalues are non-negative $(\geqslant 0)$.
- The matrix $A$ will be negative semi-definite if and only if all of its eigenvalues are non-positive $(\leqslant 0)$.
- The matrix $A$ will be negative definite if and only if all of its eigenvalues are strictly negative $(<0)$.
- The matrix $A$ will be indefinite if and only if it has both at least one strictly positive eigenvalue and at least one strictly negative eigenvalue.

But what are the eigenvalues of a matrix and how do we find them?


The **characteristic matrix** associated with a square matrix $A$ is defined to be the square matrix $(\lambda I-A)$, where $\lambda$ is a scalar variable and $I$ is the identity matrix that has the same dimensions as $A$.
- Sometimes the characteristic matrix is defined to be $(A-\lambda I)$.
- The **characteristic polynomial** associated with the matrix $A$ is defined to be $\operatorname{det}(\lambda I-A)$.
    - Sometimes the characteristic polynomial associated with the matrix $A$ is defined to be $\operatorname{det}(A-\lambda I)$.
    - If $A$ is an $(n \times n)$ matrix for some $n \in \mathbb{N}$, then the characteristic polynomial will be an $n$th degree polynomial function of the scalar variable $\lambda$.
- The **characteristic equation** associated with the matrix $A$ is defined to be $\operatorname{det}(\lambda I-A)=0$.
    - Sometimes the characteristic equation associated with the matrix $A$ is defined to be $\operatorname{det}(A-\lambda I)=0$.



- The **eigenvalues** $(\lambda)$ of a square matrix $(A)$ are the solutions to the characteristic equation.
    - Both versions of the characteristic equation will yield the same set of solution values for $\lambda$.
- If $A$ is a symmetric matrix, then all of its eigenvalues will be real numbers.
    - This is Part (a) of Theorem 23.16 in {cite:ps}`simon1994` (p. 621).



## Principal minors and definiteness

Once again, suppose that $A$ is a symmetric square matrix. Symmetry requires that $a_{i j}=a_{j i}$ for all $i \neq j$. In other words, if $A$ is an an $(n \times n)$ square matrix, then it takes the form

$$
A=\left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 n} \\
a_{12} & a_{22} & \cdots & a_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1 n} & a_{2 n} & \cdots & a_{n n}
\end{array}\right)
$$

The **leading principal sub-matrices** for $A$ are given by

$$
\begin{gathered}
A_{1}=\left(a_{11}\right) \\[10pt]
A_{2}=\left(\begin{array}{ll}
a_{11} & a_{12} \\
a_{12} & a_{22}
\end{array}\right)\\
A_{3}=\left(\begin{array}{lll}
a_{11} & a_{12} & a_{13} \\
a_{12} & a_{22} & a_{23} \\
a_{13} & a_{23} & a_{33}
\end{array}\right)
\end{gathered}
$$

and so on and so forth up until

$$
A_{n}=A=\left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 n} \\
a_{12} & a_{22} & \cdots & a_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1 n} & a_{2 n} & \cdots & a_{n n}
\end{array}\right)
$$

The **leading principal minors** for $A$ are given by $\operatorname{det}\left(A_{1}\right)$, $\operatorname{det}\left(A_{2}\right)$, $\operatorname{det}\left(A_{3}\right)$, and so on and so forth up until $\operatorname{det}\left(A_{n}\right)=\operatorname{det}(A)$.



- The matrix $A$ will be **positive definite** if and only if $\operatorname{det}\left(A_{i}\right)>0$ for all $i \in\{1,2, \cdots, n\}$.
- The matrix $A$ will be **negative definite** if and only if both $\operatorname{det}\left(A_{i}\right)<0$ for all odd $i$ and $\operatorname{det}\left(A_{i}\right)>0$ for all even $i$.
- Using the leading principal minors of a matrix to determine whether or not it is either positive semi-definite or negative semi-definite is slightly more complicated.
    - Unfortunately, we cannot just directly modify the strict inequalities in the positive definite and negative definite tests to incorporate weak inequalities.
    - Instead, we need the pattern implied by doing just that to hold for all possible permutations of the entries in the matrix.
    - This makes this approach to determining positive semi-definiteness or negative semi-definiteness somewhat cumbersome.
    - Further details can be found in {cite:ps}`sundaram1996` (pp. 50-55) and {cite:ps}`mandy2018` (pp. 1396-1398).
- In any other circumstance, the matrix $A$ is indefinite.


### Examples


```{dropdown} Example 1

Consider the matrix

$$
H=\left(\begin{array}{ccc}
\frac{\alpha}{x} & 0 & 0 \\
0 & \frac{\beta}{y} & 0 \\
0 & 0 & \frac{\gamma}{z}
\end{array}\right)
$$

where $x>0, y>0, z>0, \alpha>0, \beta>0, \gamma>0$ and $(\alpha+\beta+\gamma)=1$. Note that $H$ is a symmetric matrix.

First, let us see if we can determine the definiteness of the matrix $H$ by using the leading principal minors approach. Note that the first leading principal minor of $H$ is

$$
\operatorname{det}\left(H_{1}\right)=\operatorname{det}\left(\left(\frac{\alpha}{x}\right)\right)=\frac{\alpha}{x}>0
$$

because $x>0$ and $\alpha>0$.

Note that the second leading principal minor of $H$ is

$$
\begin{aligned}
\operatorname{det}\left(H_{2}\right) & =\operatorname{det}\left(\left(\begin{array}{cc}
\frac{\alpha}{x} & 0 \\
0 & \frac{\beta}{y}
\end{array}\right)\right) \\
& =\left(\frac{\alpha}{x}\right)\left(\frac{\beta}{y}\right)-(0)(0) \\
& =\frac{\alpha \beta}{x y}-0 \\
& =\frac{\alpha \beta}{x y} \\
& >0 \quad \text { (because } x>0, y>0, \alpha>0 \text { and } \beta>0) .
\end{aligned}
$$

Note that the third (and final) leading principal minor of $H$ is simply the determinant of the matrix $H$ itself. Upon employing a cofactor expansion along the first row of matrix $H$, we obtain

$$
\begin{aligned}
\operatorname{det}\left(H_{3}\right) & =\operatorname{det}(H) \\
& =\left(\frac{\alpha}{x}\right)(-1)^{1+1} \operatorname{det}\left(\left(\begin{array}{cc}
\frac{\beta}{y} & 0 \\
0 & \frac{\gamma}{z}
\end{array}\right)\right)+0+0 \\
& =\left(\frac{\alpha}{x}\right)(-1)^{2}\left[\left(\frac{\beta}{y}\right)\left(\frac{\gamma}{z}\right)-(0)(0)\right] \\
& =\left(\frac{\alpha}{x}\right)(1)\left[\frac{\beta \gamma}{y z}-0\right] \\
& =\left(\frac{\alpha}{x}\right)\left(\frac{\beta \gamma}{y z}\right) \\
& =\frac{\alpha \beta \gamma}{x y z} \\
& >0
\end{aligned}
$$

because $x>0, y>0, z>0, \alpha>0, \beta>0$, and $\gamma>0$.

Since $\operatorname{det}\left(H_{1}\right)>0$, $\operatorname{det}\left(H_{2}\right)>0$ and $\operatorname{det}\left(H_{3}\right)>0$, we can conclude that the matrix $H$ is positive definite.

Now let us now establish that $H$ is positive definite by using the eigenvalue approach. The characteristic matrix associated with $H$ is

$$
\begin{aligned}
\lambda I-H & =\lambda\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)-\left(\begin{array}{ccc}
\frac{\alpha}{x} & 0 & 0 \\
0 & \frac{\beta}{y} & 0 \\
0 & 0 & \frac{\gamma}{z}
\end{array}\right) \\
& =\left(\begin{array}{ccc}
\lambda & 0 & 0 \\
0 & \lambda & 0 \\
0 & 0 & \lambda
\end{array}\right)-\left(\begin{array}{ccc}
\frac{\alpha}{x} & 0 & 0 \\
0 & \frac{\beta}{y} & 0 \\
0 & 0 & \frac{\gamma}{z}
\end{array}\right) \\
& =\left(\begin{array}{ccc}
\lambda-\frac{\alpha}{x} & 0 & 0 \\
0 & \lambda-\frac{\beta}{y} & 0 \\
0 & 0 & \lambda-\frac{\gamma}{z}
\end{array}\right)
\end{aligned}
$$


It is straight-forward to establish that the characteristic polynomial for $H$ is

$$
\operatorname{det}(\lambda I-H)=\left(\lambda-\frac{\alpha}{x}\right)\left(\lambda-\frac{\beta}{y}\right)\left(\lambda-\frac{\gamma}{z}\right)
$$

You should establish the validity of this claim as a form of revision of the calculation of determinants. Thus the characteristic equation associated with $H$ is

$$
\operatorname{det}(\lambda I-H)=\left(\lambda-\frac{\alpha}{x}\right)\left(\lambda-\frac{\beta}{y}\right)\left(\lambda-\frac{\gamma}{z}\right)=0
$$

Clearly the eigenvalues for $H$ are $\lambda_{1}=\frac{\alpha}{x}, \lambda_{2}=\frac{\beta}{y}$, and $\lambda_{3}=\frac{\gamma}{z}$.

Since $x>0, y>0, z>0, \alpha>0, \beta>0$, and $\gamma>0$, we know that $\lambda_{1}>0, \lambda_{2}>0$, and $\lambda_{3}>0$.

Thus we can conclude that $H$ is a positive definite matrix.
```

```{dropdown} Example 2

Consider the matrix

$$
A=\left(\begin{array}{ll}
2 & 3 \\
3 & 7
\end{array}\right)
$$

Note that $A$ is a symmetric matrix.
The leading principal sub-matrices for this matrix are $A_{1}=(2)$ and $A_{2}=A$. As such, the leading principal minors for this matrix are

$$
\operatorname{det}\left(A_{1}\right)=2>0
$$

and

$$
\operatorname{det}\left(A_{2}\right)=\operatorname{det}(A)=(2)(7)-(3)(3)=14-9=5>0
$$

Thus we can conclude that $A$ is a positive definite matrix.

```



```{dropdown} Example 3 

Consider the matrix

$$
A=\left(\begin{array}{ll}
2 & 4 \\
4 & 7
\end{array}\right)
$$

Note that $A$ is a symmetric matrix. The leading principal sub-matrices for this matrix are $A_{1}=(2)$ and $A_{2}=A$. As such, the leading principal minors for this matrix are

$$
\operatorname{det}\left(A_{1}\right)=2>0
$$

and

$$
\operatorname{det}\left(A_{2}\right)=\operatorname{det}(A)=(2)(7)-(4)(4)=14-16=-2<0
$$

Since $\operatorname{det}\left(A_{1}\right)>0$ and $\operatorname{det}\left(A_{2}\right)=\operatorname{det}(A)<0$, we know that $A$ is neither negative definite nor positive definite. But is it negative semi-definite, positive semi-definite, or indefinite? Let us find the eigenvalues of $A$.

The characteristic matrix for $A$ is

$$
\begin{aligned}
\lambda I-A & =\lambda\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right)-\left(\begin{array}{ll}
2 & 4 \\
4 & 7
\end{array}\right) \\
& =\left(\begin{array}{ll}
\lambda & 0 \\
0 & \lambda
\end{array}\right)-\left(\begin{array}{ll}
2 & 4 \\
4 & 7
\end{array}\right) \\
& =\left(\begin{array}{cc}
\lambda-2 & -4 \\
-4 & \lambda-7
\end{array}\right)
\end{aligned}
$$


This means that the characteristic polynomial for $A$ is

$$
\begin{aligned}
\operatorname{det}(\lambda I-A) & =(\lambda-2)(\lambda-7)-(-4)(-4) \\
& =\lambda^{2}-7 \lambda-2 \lambda+14-16 \\
& =\quad \lambda^{2}-9 \lambda-2
\end{aligned}
$$

Thus the characteristic equation for $A$ is

$$
\operatorname{det}(\lambda I-A)=\lambda^{2}-9 \lambda-2=0
$$

Note that the characteristic equation for $A$ is a quadratic equation in the variable $\lambda$.


Upon applying the quadratic formula to the characteristic equation for $A$, we find that the eigenvalues for the matrix $A$ are $\lambda_{1}=\frac{9+\sqrt{89}}{2}$ and $\lambda_{2}=\frac{9-\sqrt{89}}{2}$.

Since $9^{2}=81<89<100=10^{2}$, we know that $9<\sqrt{89}<10$.

This means that $\lambda_{1}=\frac{9+\sqrt{89}}{2}>0$ and $\lambda_{2}=\frac{9-\sqrt{89}}{2}<0$.

Thus we can conclude that the $A$ is an indefinite matrix.
```


```{dropdown} Example 4

Consider the matrix

$$
A=\left(\begin{array}{ll}
0 & 0 \\
0 & 7
\end{array}\right)
$$

Note that $x^{T} A x=7 x_{2}^{2} \geqslant 0$ for all $\left(x_{1}, x_{2}\right)^{T} \neq(0,0)^{T}$. Thus this matrix is positive semi-definite.

This follows directly from the definition of positive semi-definiteness of a matrix. It is not positive definite because $x^{T} A x=7 x_{2}^{2}=0$ when $\left(x_{1}, x_{2}\right)^{T}=(1,0)$ and $(1,0) \neq(0,0)$.

```



```{dropdown} Example 5

Consider the matrix

$$
A=\left(\begin{array}{cc}
0 & 0 \\
0 & -7
\end{array}\right)
$$

Note that $x^{T} A x=-7 x_{2}^{2} \leqslant 0$ for all $\left(x_{1}, x_{2}\right)^{T} \neq(0,0)^{T}$.  Thus this matrix is negative semi-definite.

This follows directly from the definition of negative semi-definiteness of a matrix. It is not negative definite because $x^{T} A x=7 x_{2}^{2}=0$ when $\left(x_{1}, x_{2}\right)^{T}=(1,0)$ and $(1,0) \neq(0,0)$.)

```


```{dropdown} Example 6

Consider the matrix

$$
A=\left(\begin{array}{cc}
2 & 2 \\
2 & -1
\end{array}\right)
$$

Note that $A$ is a symmetric matrix. The leading principal sub-matrices for this matrix are $A_{1}=(2)$ and $A_{2}=A$. As such, the leading principal minors for this matrix are

$$
\operatorname{det}\left(A_{1}\right)=2>0
$$

and

$$
\operatorname{det}\left(A_{2}\right)=\operatorname{det}(A)=(2)(-1)-(2)(2)=-2-4=-6<0
$$

Since $\operatorname{det}\left(A_{1}\right)>0$ and $\operatorname{det}\left(A_{2}\right)=\operatorname{det}(A)<0$, we know that $A$ is neither negative definite nor positive definite. But is it negative semi-definite, positive semi-definite, or indefinite? Let us find the eigenvalues of $A$.

The characteristic matrix for $A$ is

$$
\begin{aligned}
\lambda I-A & =\lambda\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right)-\left(\begin{array}{cc}
2 & 2 \\
2 & -1
\end{array}\right) \\
& =\left(\begin{array}{ll}
\lambda & 0 \\
0 & \lambda
\end{array}\right)-\left(\begin{array}{cc}
2 & 2 \\
2 & -1
\end{array}\right) \\
& =\left(\begin{array}{cc}
\lambda-2 & -2 \\
-2 & \lambda+1
\end{array}\right)
\end{aligned}
$$

This means that the characteristic polynomial for $A$ is

$$
\begin{array}{rlr}
\operatorname{det}(\lambda I-A) & = & (\lambda-2)(\lambda+1)-(-2)(-2) \\
& = & \lambda^{2}+\lambda-2 \lambda-2-4 \\
& = & \lambda^{2}-\lambda-6 \\
& = & (\lambda+2)(\lambda-3) .
\end{array}
$$

Thus the characteristic equation for $A$ is

$$
\operatorname{det}(\lambda I-A)=(\lambda+2)(\lambda-3)=0 \text {. }
$$

This means that the eigenvalues for the matrix $A$ are $\lambda_{1}=-2$ and $\lambda_{2}=3$. Since $\lambda_{1}<0$ and $\lambda_{2}>0$, we can conclude that $A$ is an indefinite matrix.
```


````{dropdown} Example 7

Consider the matrix

$$
A=\left(\begin{array}{lll}
1 & 2 & 0 \\
2 & 4 & 5 \\
0 & 5 & 6
\end{array}\right)
$$

Note that $A$ is a symmetric matrix. The leading principal sub-matrices for this matrix are $A_{1}=(1)$,

$$
A_{2}=\left(\begin{array}{ll}
1 & 2 \\
2 & 4
\end{array}\right)
$$

and $A_{3}=A$.

As such, the leading principal minors for this matrix are $\operatorname{det}\left(A_{1}\right)=1$,

$$
\operatorname{det}\left(A_{2}\right)=(1)(4)-(2)(2)=4-4=0
$$

and

$$
\begin{aligned}
\operatorname{det}\left(A_{3}\right)= & \operatorname{det}(A) \\[5pt]
= & (1)(-1)^{1+1} \operatorname{det}\left(\left(\begin{array}{ll}
4 & 5 \\
5 & 6
\end{array}\right)\right) \\[5pt]
& +(2)(-1)^{1+2} \operatorname{det}\left(\left(\begin{array}{ll}
2 & 5 \\
0 & 6
\end{array}\right)\right) \\[5pt]
& +(0)(-1)^{1+3} \operatorname{det}\left(\left(\begin{array}{ll}
2 & 4 \\
0 & 5
\end{array}\right)\right) \\[5pt]
= & (1)(-1)^{2}\{(4)(6)-(5)(5)\} \\
& +(2)(-1)^{3}\{(2)(6)-(5)(0)\}+0 \\[5pt]
= & (1)(-1)^{2}\{(4)(6)-(5)(5)\} \\
& +(2)(-1)^{3}\{(2)(6)-(5)(0)\}+0 \\[5pt]
= & (1)(1)\{24-25\}+(2)(-1)\{12-0\} \\[5pt]
= & (1)(1)(-1)+(2)(-1)(12) \\[5pt]
= & -1+(-24) \\[5pt]
= & -1-24 \\[5pt]
= & -25
\end{aligned}
$$

Since $\operatorname{det}\left(A_{1}\right)>0$, $\operatorname{det}\left(A_{2}\right)=0$, and $\operatorname{det}\left(A_{3}\right)=\operatorname{det}(A)<0$, we know that $A$ is neither negative definite nor positive definite. But is it negative semi-definite, positive semi-definite, or indefinite?  Let us find the eigenvalues of $A$.


The characteristic matrix for $A$ is

$$
\begin{aligned}
\lambda I-A & =\lambda\left(\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)-\left(\begin{array}{lll}
1 & 2 & 0 \\
2 & 4 & 5 \\
0 & 5 & 6
\end{array}\right) \\
& =\left(\begin{array}{lll}
\lambda & 0 & 0 \\
0 & \lambda & 0 \\
0 & 0 & \lambda
\end{array}\right)-\left(\begin{array}{lll}
1 & 2 & 0 \\
2 & 4 & 5 \\
0 & 5 & 6
\end{array}\right) \\
& =\left(\begin{array}{ccc}
\lambda-1 & -2 & 0 \\
-2 & \lambda-4 & -5 \\
0 & -5 & \lambda-6
\end{array}\right)
\end{aligned}
$$

The characteristic polynomial for $A$ is

$$
\begin{aligned}
\operatorname{det}(\lambda I-A)= & (\lambda-1)(-1)^{1+1} \operatorname{det}\left(\left(\begin{array}{cc}
\lambda-4 & -5 \\
-5 & \lambda-6
\end{array}\right)\right) \\
& +(-2)(-1)^{1+2} \operatorname{det}\left(\left(\begin{array}{cc}
-2 & -5 \\
0 & \lambda-6
\end{array}\right)\right)+0 \\
= & (\lambda-1)(-1)^{2}[(\lambda-4)(\lambda-4)-25] \\
& +(-2)(-1)^{3}[-2(\lambda-6)-0] \\
= & (\lambda-1)(1)\left[\lambda^{2}-10 \lambda+24-25\right] \\
& -2(-1)[-2 \lambda+12] \\
= & (\lambda-1)\left[\lambda^{2}-10 \lambda-1\right]+2[-2 \lambda+12] \\
= & \lambda^{3}-10 \lambda^{2}-\lambda-\left[\lambda^{2}-10 \lambda-1\right]-4 \lambda+24 \\
= & \lambda^{3}-10 \lambda^{2}-\lambda-\lambda^{2}+10 \lambda+1-4 \lambda+24 \\
= & \lambda^{3}-11 \lambda^{2}+5 \lambda+25
\end{aligned}
$$


The characteristic equation for the matrix $A$ is

$$
\operatorname{det}(\lambda I-A)=\lambda^{3}-11 \lambda^{2}+5 \lambda+25=0
$$

Note that this is a cubic equation in the variable $\lambda$.
While there do not appear to be any "obvious" factorisations of this characteristic polynomial, it is possible to obtain numerical approximations to the eigenvalues of $A$ (that is, the solutions to the characteristic equation) by using Microsoft Excel.
Upon doing this we find that $\lambda_{1} \approx-1.2395, \lambda_{2} \approx 1.9627$, and $\lambda_{3} \approx 10.277$.
A graph of the characteristic polynomial can be found on the next slide.
Since the matrix $A$ has both positive and negative eigenvalues, we can conclude that it is indefinite.


```{figure} _static/img/lecture_07/example7.png
:width: 80%
:align: center

Graph of the characteristic polynomial
```
````


```{dropdown} Example 8

Consider the matrix

$$
A=\left(\begin{array}{ccc}
-1 & 1 & 0 \\
1 & -1 & 0 \\
0 & 0 & -2
\end{array}\right)
$$

Note that $A$ is a symmetric matrix. The characteristic matrix for $A$ is

$$
(\lambda I-A)=\left(\begin{array}{ccc}
\lambda+1 & -1 & 0 \\
-1 & \lambda+1 & 0 \\
0 & 0 & \lambda+2
\end{array}\right)
$$

The characteristic polynomial for $A$ is

$$
\begin{aligned}
\operatorname{det}(\lambda I-A) & =0+0+(\lambda+2)(-1)^{3+3} \operatorname{det}\left(\left(\begin{array}{cc}
\lambda+1 & -1 \\
-1 & \lambda+1
\end{array}\right)\right) \\
& =0+0+(\lambda+2)(-1)^{6}\left[(\lambda+1)^{2}-1\right] \\
& =(\lambda+2)(1)((\lambda+1)+1)((\lambda+1)-1) \\
& =(\lambda+2)(1)(\lambda+2)(\lambda) \\
& =(\lambda+2)^{2} \lambda
\end{aligned}
$$

The characteristic equation for the matrix $A$ is

$$
\operatorname{det}(\lambda I-A)=(\lambda+2)^{2} \lambda=0
$$

Clearly the eigenvalues for the matrix $A$ are $\lambda_{1}=-2, \lambda_{2}=-2$, and $\lambda_{3}=0$.

Since the matrix $A$ has both negative eigenvalues and a zero eigenvalue, we can conclude that it is negative semi-definite.

```



```{dropdown} Example 9

Consider the matrix

$$
A=\left(\begin{array}{cc}
-1 & -2 \\
4 & 3
\end{array}\right)
$$

Note that the matrix $A$ is _not_ symmetric.

Before proceeding, let us construct a symmetric matrix $B$ such that $x^{T} A x=x^{T} B x$ for all $x \in \mathbb{R}^{2}$.

We will do this by setting $b_{11}=a_{11}=-1, b_{22}=a_{22}=3$, and $b_{12}=b_{21}=\left(\frac{1}{2}\right)\left(a_{12}+a_{21}\right)=\frac{(-2+4)}{2}=\frac{2}{2}=1$.

The resulting matrix $B$ is

$$
B=\left(\begin{array}{cc}
-1 & 1 \\
1 & 3
\end{array}\right)
$$

Note that the matrix $B$ is symmetric.


The leading principal sub-matrices for this matrix are $B_{1}=(-1)$ and $B_{2}=B$. As such, the leading principal minors for this matrix are

$$
\operatorname{det}\left(B_{1}\right)=-1<0
$$

and

$$
\operatorname{det}\left(B_{2}\right)=\operatorname{det}(B)=(-1)(3)-(1)(1)=-3-1=-4<0
$$

Since $\operatorname{det}\left(B_{1}\right)<0$ and $\operatorname{det}\left(B_{2}\right)=\operatorname{det}(B)<0$, we know that $B$ (and hence $A$ ) is neither negative definite nor positive definite.  But is it negative semi-definite, positive semi-definite, or indefinite? Let us find the eigenvalues of $B$.

The characteristic matrix for $B$ is

$$
\begin{aligned}
\lambda I-B & =\lambda\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right)-\left(\begin{array}{cc}
-1 & 1 \\
1 & 3
\end{array}\right) \\
& =\left(\begin{array}{ll}
\lambda & 0 \\
0 & \lambda
\end{array}\right)-\left(\begin{array}{cc}
-1 & 1 \\
1 & 3
\end{array}\right) \\
& =\left(\begin{array}{cc}
\lambda+1 & -1 \\
-1 & \lambda-3
\end{array}\right)
\end{aligned}
$$

This means that the characteristic polynomial for $B$ is

$$
\begin{array}{rlr}
\operatorname{det}(\lambda I-B) & = & (\lambda+1)(\lambda-3)-(-1)(-1) \\
& = & \lambda^{2}-3 \lambda+\lambda-3-1 \\
& = & \lambda^{2}-2 \lambda-4
\end{array}
$$


Thus the characteristic equation for $B$ is

$$
\operatorname{det}(\lambda /-B)=\lambda^{2}-2 \lambda-4=0
$$

Note that the characteristic equation for $B$ is a quadratic equation in the variable $\lambda$.

Upon applying the quadratic formula to the characteristic equation for $B$, we find that the eigenvalues for the matrix $B$ are $\lambda_{1}=\frac{2+\sqrt{20}}{2}$ and $\lambda_{2}=\frac{2-\sqrt{20}}{2}$.

Since $4^{2}=16<20<25=5^{2}$, we know that $4<\sqrt{20}<5$.

This means that $\lambda_{1}=\frac{2+\sqrt{20}}{2}>0$ and $\lambda_{2}=\frac{2-\sqrt{20}}{2}<0$.

Thus we can conclude that the $B$ (and hence $A$ ) is an indefinite matrix.
```


```{dropdown} Example 10

Consider the matrix

$$
A=\left(\begin{array}{cc}
\frac{-\alpha}{x^{2}} & 0 \\
0 & \frac{-(1-\alpha)}{y^{2}}
\end{array}\right)
$$

where $x>0, y>0$, and $0<\alpha<1$.

The leading principal sub-matrices for this matrix are $A_{1}=\left(\frac{-\alpha}{x^{2}}\right)$ and $A_{2}=A$. As such, the leading principal minors for this matrix are

$$
\operatorname{det}\left(A_{1}\right)=\frac{-\alpha}{x^{2}}<0
$$

and

$$
\operatorname{det}\left(A_{2}\right)=\operatorname{det}(A)
$$


$$
\begin{aligned}
\operatorname{det}\left(A_{2}\right) & =\operatorname{det}(A) \\
& =\left(\frac{-\alpha}{x^{2}}\right)\left(\frac{-(1-\alpha)}{y^{2}}\right)-(0)(0) \\
& =\frac{\alpha(1-\alpha)}{x^{2} y^{2}}-0 \\
& =\frac{\alpha(1-\alpha)}{x^{2} y^{2}} \\
& >0 
\end{aligned}
$$

Since $\operatorname{det}\left(A_{1}\right)<0$ and $\operatorname{det}\left(A_{2}\right)=\operatorname{det}(A)>0$, we can conclude that $A$ is a negative definite matrix.
```



### Quadratic forms on sets of linear constraints

Up until now, we have been allowing the $x$ vector to vary over all of $\mathbb{R}^{n}$ (for some $n \in \mathbb{N}$ ) and examining the sign of the quadratic form $x^{T} A x$ for all such $x \neq 0$. In other words, we have been examining the sign of an "unconstrained" quadratic form in $x$.

In economics, however, we are often faced with situations in which the $x$ vector will be subject to one or more constraints. Is it possible to determine the sign of a quadratic form for all $x$ vectors that satisfy one or more constraints in addition to the standard constraint that $x \neq 0$? If so, how can this be done?

In the remainder of these notes, we (at least partially) address these questions for the case in which the $x$ vector is subject to one or more linear constraints (in addition to the standard constraint that $x \neq 0$ ).


Let $x \in \mathbb{R}^{n}$ be a vector of $n$ real-valued variables, $A$ be an $(n \times n)$ matrix of constant real-valued coefficients, $C$ be an $(m \times n)$ matrix of constant real-valued coefficients, $m \in \mathbb{N}, n \in \mathbb{N}$, and $m<n$.

Note that $x^{T} A x$ is a quadratic form in the $x$ vector and $C x$ is a linear form in the $x$ vector.

We have already seen that if $A$ is not a symmetric matrix, then there exists some other matrix $B$ such that $x^{T} A x=x^{T} B x$ for all $x \in \mathbb{R}^{n}$. As such, we can assume, without loss of generality, that $A$ is a symmetric $(n \times n)$ matrix. We will make this assumption for the remainder of these notes.


The linear form $C_{x}$ will represent the linear constraints that we are imposing on the $x$ vector in addition to the standard restriction that $x \neq 0$.

To be precise, the set of linear constraints that are being imposed are given by the matrix equation $C_{X}=0$, where 0 is an $(m \times 1)$ null vector (that is, a vector consisting entirely of entries that are zeros). It is important that $m<n$ because we do not want the $x$ vector to be completely determined by the additional constraints that are being imposed.

We can use the matrices $A$ and $C$ to form a "bordered matrix" $D$ that takes the partitioned form

$$
D=\left(\begin{array}{cc}
0 & C \\
C^{T} & A
\end{array}\right),
$$

where 0 is an $(m \times m)$ null matrix (that is, a matrix consisting entirely of entries that are zeros).

Note that $D$ is a symmetric square matrix.
- It is an $((m+n) \times(m+n))$ matrix.
- It is symmetric because we have assumed that $A$ is a symmetric matrix.

The theorem on the following two slides is contained within (that is, is part of) Theorem 16.4 in {cite:ps}`simon1994` (p. 389).

```{admonition} Theorem
:class: caution

**Theorem**: Consider the quadratic form $Q(x)=x^{T} A x$ subject to the set of linear constraints $C x=0_{(m \times 1)}$ and $x \neq 0_{(n \times 1)}$, where $x \in \mathbb{R}^{n}$ is an $(m \times 1)$ vector of real-valued variables, $A$ is an $(n \times n)$ symmetric matrix of constant real-valued coefficients, $C$ is an $(m \times n)$ matrix of constant real-valued coefficients, $m \in \mathbb{N}, n \in \mathbb{N}$, and $m<n$. Form the $((m+n) \times(m+n))$ symmetric "bordered matrix" (in partitioned form)

$$
D=\left(\begin{array}{cc}
0_{(m \times m)} & C \\
C^{T} & A
\end{array}\right)
$$


1. The quadratic form $Q(x)$ is negative definite on the constraint set if $\operatorname{det}(D)$ has the same sign as $(-1)^{n}$ **and** if the **last** $(n-m)$ leading principal minors alternate in sign.
2. The quadratic form $Q(x)$ is positive definite on the constraint set if the last $(n-m)$ leading principal minors all have the same sign as $(-1)^{m}$.
3. The quadratic form $Q(x)$ is indefinite on the constraint set if both condition (1) and condition (2) are violated by **non-zero** leading principal minors.
```


```{dropdown} Example 11

This is Example 16.7 from {cite:ps}`simon1994` (pp. 389-390).

Consider the case where $A$ is the symmetric $(4 \times 4)$ matrix

$$
A=\left(\begin{array}{cccc}
1 & 0 & 0 & -1 \\
0 & -1 & 2 & 0 \\
0 & 2 & 1 & 0 \\
-1 & 0 & 0 & 1
\end{array}\right)
$$

and $C$ is the $(2 \times 4)$ matrix

$$
C=\left(\begin{array}{cccc}
0 & 1 & 1 & 1 \\
1 & -9 & 0 & 1
\end{array}\right)
$$

Note that in this case there are four variables (that is, $n=4$ ) and two linear constraints other than the constraint that $x \neq 0$ (that is, $m=2)$.

The symmetric bordered matrix for this example is the $(6 \times 6)$ matrix

$$
D=\left(\begin{array}{cc}
0_{(2 \times 2)} & \\
& \\
C^{T} & A
\end{array}\right)=\left(\begin{array}{cccccc}
0 & 0 & 0 & 1 & 1 & 1 \\
0 & 0 & 1 & -9 & 0 & 1 \\
0 & 1 & 1 & 0 & 0 & -1 \\
1 & -9 & 0 & -1 & 2 & 0 \\
1 & 0 & 0 & 2 & 1 & 0 \\
1 & 1 & -1 & 0 & 0 & 1
\end{array}\right)
$$

Since $n=4$ and $m=2$ in this example, we need to check the signs of the last $(n-m)=(4-2)=2$ leading principal minors.

- The last leading principal minor is $\operatorname{det}\left(D_{6}\right)=\operatorname{det}(D)$.
- The second-last leading principal minor is $\operatorname{det}\left(D_{5}\right)$.


According to {cite:ps}`simon1994` (p. 390), $\operatorname{det}\left(D_{6}\right)=\operatorname{det}(D)=24>0$ and $\operatorname{det}\left(D_{5}\right)=77>0$.

You should confirm these values for the last two leading principle minors of $D$ for yourself.

Note that $(-1)^{m}=(-1)^{2}=1>0$. Thus we have

$$
\operatorname{sign}\left(\operatorname{det}\left(D_{6}\right)\right)=\operatorname{sign}\left(\operatorname{det}\left(D_{5}\right)\right)=\operatorname{sign}\left((-1)^{m}\right)=\operatorname{sign}\left((-1)^{2}\right) .
$$

This means that the matrix $A$ is positive definite on the set of constraints given by both $C_{x}=0$ and $x \neq 0$.

```
