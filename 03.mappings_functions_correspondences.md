
# Mappings: functions and correspondences

## Reading guide

### Introductory mathematical economics references
- {cite:ps}`haeussler1987`: Chapters 0, 3, 4, 5, and 17.1.
- {cite:ps}`sydsaeter2016`: Chapters 2, 4, 5, 9.6 (pp. 350-351 only), 11.1, and 11.5.
- {cite:ps}`shannon1995`: Chapters 1, 2, and 6.

### Advanced high school references
- {cite:ps}`coroneos1`: Chapters 1, 2, 4, and 7.
- {cite:ps}`coroneos2`: Chapters 1 and 2.

### Introductory mathematics references
- {cite:ps}`adams2018`: Chapters P, 3, and 11.
- {cite:ps}`kline1967`: pp. 419–432.
- {cite:ps}`silverman1969`: Chapters 7 and 14.
- {cite:ps}`spivak2006`: Chapters 1, 2, 3, 4, 16, 18, and 19.
- {cite:ps}`thomas1996`: Chapters P, 6, and 12.

### More advanced references
- {cite:ps}`banks2009`
- {cite:ps}`corbae2009`: Chapter 2 (pp. 15-71).
- {cite:ps}`kolmogorov1970`: Chapter 1 (pp. 1-36).
- {cite:ps}`simon1994`: Chapters 2 and 13 (pp. 10-38 and 273-299).


### Websites

The following websites contain discussions of the concept of relatively prime, or co-prime, numbers and polynomials. This is relevant to the topic of rational functions and partial fractions.
- https://www.mathsisfun.com/definitions/relatively-prime.html
- http://mathworld.wolfram.com/RelativelyPrime.html
- https://www.varsitytutors.com/hotmath/hotmath_help/topics/relatively-prime





## Mappings
Let $X$ and $Y$ be two sets. A rule $f$ that assigns one or more elements of $Y$ to each element of $X$ is called a **mapping** from $X$ into $Y$. It is denoted by $f: X \rightarrow Y$.

The set $X$ is known as the **domain** of the mapping $f$. The mapping must be defined for every element of $X$. This means that $x \in X \implies f(x) \in Y$.

The set $Y$ is known as the **co-domain** of the mapping $f$. Mappings are not required to generate $Y$ from $X$. This means that there might exist one or more elements $y \in Y$ such that $y \ne f(x)$ for any $x \in X$.

The set of values $y \in Y$ that can be generated from $X$ by the function $f$ is known as the **image** of $X$ under $f$. Sometimes the image of $X$ under $f$ is referred to as the **range** of $f$. We will denote the range of $f$ by $f(X)$.

```{figure} _static/img/lecture_03/mapping_diagram.png
:width: 80%
:align: center

Diagrammatic representation of a mapping
```

### Images
Consider a mapping $f: X \rightarrow Y$.
The *image of the point* $x \in X$ under the mapping $f$ is the point, or collection of points, given by $f(x) \in Y$.

The *image of the set* $A \subseteq X$ under the mapping $f$ is the set $f(A) = \{f(x) \in Y : x \in A\}$. 
- Clearly $f(A) \subseteq Y$.

The *image of the domain* ($X$) under $f$ is the set $f(X) = \{f(x) \in Y : x \in X \}$.
- Clearly $f(X) \subseteq Y$.

Note that if $f: X \rightarrow Y$ and $A \subseteq X$, then $f(A) \subseteq f(X) \subseteq Y$.


### Pre-images
Consider a mapping $f: X \rightarrow Y$.

The **pre-image** *of the point* $y \in Y$ under the mapping $f$ is the point, or collection of points, in $X$ for which $y = f(x)$.

It is possible for a point $y \in Y$ to have either 
- no pre-image 
- a unique pre-image, or 
- multiple pre-images. 
If a point $y \in Y$ has a unique pre-image under the mapping $f$, then it is denoted by $f^{−1}(y)$.

The *pre-image of the set* $B \subseteq Y$ under $f$ is the set $f^{−1}(B) = \{x \in X : f(x) \in B\}$.

Recall that it is possible for there to exist $y \in Y$ such that $y \ne f(x)$ for any $x \in X$. If the set B consists entirely of such points, then $f^{−1}(B) = \varnothing$.

Since $f(x) \in Y$ for all $x \in X$, it must be the case that $f^{-1}(Y) = X$.

### Types of mappings

There are four basic types of mappings. These are as follows.
- A one-to-one mapping:
    - Each point in $X$ has a unique image in $Y$; and
    - Each point in $Y$ has either a unique pre-image in $X$ or no pre-image in $X$.
- A many-to-one mapping:
    - Each point in $X$ has a unique image in $Y$; but
    - At least one point in $Y$ has multiple pre-images in $X$.
- A one-to-many mapping:
    - At least one point in $X$ has multiple images in $Y$; but
    - Each point in $Y$ has either a unique pre-image in $X$ or no pre-image in $X$.
- A many-to-many mapping:
    - At least one point in $X$ has multiple images in $Y$; and
    - At least one point in $Y$ has multiple pre-images in $X$.
    
Mappings whose domain points all have unique images are known as
**functions**. In other words, one-to-one mappings and many-to-one mappings are
known as functions.

A one-to-one function $f: X \rightarrow Y$ is called an **injection**. The pre-image of a one-to-one function is known as its **inverse**.  

Mappings that have at least one domain point with multiple images are known as **correspondences**. In other words, one-to-many mappings and many-to-many mappings are known as correspondences.

Note that we could express a correspondence of the form $f : X \rightarrow Y$ as a function of the form $g: X \rightarrow 2^Y$.

### Into and onto
Consider a mapping $f : X \rightarrow Y$. Clearly we must have $f(X) \subseteq Y$.
- If $f(X ) \subset Y$, so that $f(X) \ne Y$, we say that $f$ maps $X$ “into” $Y$.
- If $f(X) = Y$, we say that $f$ maps $X$ “onto” $Y$.
    - If $f(X) = Y$ and $f$ is a function, then we call $f$ a **surjection**.

A mapping that is both an injection and a surjection is called a **bijection**. In other words, a function that is *both one-to-one and onto* is called a
bijection.

### Examples

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x) = x$. This is sometimes called the identity map. Note that $f$ is both onto and one-to-one. This means that $f$ is a bijection. It also means that the pre-image of $f$ is an inverse function.

Consider the mapping $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x) = x^2$. Note that $f$ is into, but not onto, because $f^{−1}(R_{−−}) = \varnothing$, where $\mathbb{R}_{−−} = \{x \in \mathbb{R} : x < 0\}$.

Note also that f is many-to-one, and hence not one-to-one, because $2 \in \mathbb{R}, (−2) \in \mathbb{R}, 2 \ne (−2)$, and both $2^2 = 4$ and $(−2)^2 = 4$.
This means that f is neither an injection nor a surjection. Thus it cannot be a bijection. It also means that the pre-image of f is not an inverse function.

### Composite functions
Let $f : X \rightarrow Y$ be a function of the form $y = f(x)$.
Let $g : Y \rightarrow Z$ be a function of the form $z = g(y)$.
The composite function $h = g \circ f$ is defined by $h(x) = g(f(x))$.
The composite function $h = g \circ f$ is a mapping $h : X \rightarrow Z$ of the
form $z = h(x)$.

Example:
Let $f : \mathbb{R}^2_{++} \rightarrow \mathbb{R}_{++}$ be defined by 
$f(x_1, x_2) = x_1^\alpha \; x_2^{(1 − \alpha)}$
for some fixed $\alpha \in (0, 1)$.

Let $g : \mathbb{R}_{++} \rightarrow \mathbb{R}$ be defined by $g(x) = ln(x)$.

Then the composite function $h = g \circ f$ is a mapping $h: \mathbb{R}^2_{++} \rightarrow \mathbb{R}$ that is defined by $h(x_1, x_2) = g(f(x_1, x_2)) = ln(x_1^\alpha \; x_2^{(1 − \alpha)}) = \alpha \; ln(x_1) + (1 − \alpha) \; ln(x_2)$

### One-to-one functions
A function $f : X \rightarrow Y$ is **one-to-one** if
```{math}
x \ne y \iff f(x) \ne f(y)
```

The contra-positive version of this condition is that
```{math}
f(x) = f(y) \iff x = y
```

Examples:
- $f : \mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x ) = x$ is a one-to-one function.
- $f : \mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x ) = x^2$ is not a one-to-one function.
- $f : \mathbb{R}_+ \rightarrow \mathbb{R}$ defined by $f(x) = x^2$ is a one-to-one function.

### Non-decreasing and strictly increasing functions
A function $f : X \rightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}$, is said to be a **non-decreasing** function if
```{math}
x \leqslant y \iff f(x) \leqslant f(y)
```

A function $f : X \rightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}$, is said to be a **strictly increasing** function if both
- (a) $x = y \iff f(x) = f(y)$; and
- (b) $x < y \iff f(x) < f(y)$.

Note the following:
- A strictly increasing function is also a one-to-one function.
- There are some one-to-one functions that are not strictly increasing.
- A strictly increasing function is also a non-decreasing function.
- There are some non-decreasing functions that are not strictly increasing.

### Economic application: utility functions are not unique
Suppose that $U : X \rightarrow \mathbb{R}$ is a utility function that represents the weak preference relation $\succsim$. This means that $x \succsim y \iff U(x) \geqslant U(y)$.

Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a strictly increasing function. Consider the composite function $V = f \circ U$. It can be shown that $V$ is also a utility function that represents the weak preference relation $\succsim$. In other words, it can be shown that $x \succsim y \iff V(x) \geqslant V(y)$.

### Example: A Cobb-Douglas utility function
Consider a consumer whose preferences over bundles of strictly positive amounts of each of two commodities can be represented by a utility function $U : \mathbb{R}^2_{++} \rightarrow \mathbb{R}_{++}$ of the form
```{math}
U(x_1, x_2) = Ax_1^\alpha x_2^\beta
```
where $A > 0, \alpha > 0$, and $\beta > 0$.

Such preferences are known as Cobb-Douglas preferences.

- The function $f: \mathbb{R}_{++} \rightarrow \mathbb{R}_{++}$ defined by $f(x) = \left( \frac{1}{A} \right)x$ is strictly increasing. Thus we know that another utility function that represents this consumer’s preferences is
```{math}
V(x_1, x_2) = f(U((x_1, x_2)) = \left( \frac{1}{A} \right) (Ax_1^\alpha x_2^\beta) 
= x_1^\alpha x_2^\beta
```

- The function $g: \mathbb{R}_{++} \rightarrow \mathbb{R}_{++}$ defined by $g(x) = x^{\frac{1}{(\alpha + \beta)}}$ is strictly increasing. (If any relevant surd expression can be either positive or negative, then we will assume that the positive option is chosen throughout this Cobb-Douglas preferences example.) Thus we know that another utility function that represents this consumer’s preferences is
```{math}
W(x_1, x_2) = g(V((x_1, x_2)) = (x_1^\alpha x_2^\beta)^{\frac{1}{(\alpha + \beta)}}
= x_1^\gamma x_2^{(1 - \gamma)}
```
where $ \gamma = \frac{\alpha}{\alpha + \beta} \in (0, 1)$.

- The function $k: \mathbb{R}_{++} \rightarrow \mathbb{R}$ defined by $k(x) = ln(x )$ is strictly increasing. Thus we know that another utility function that represents this consumer’s preferences is
```{math}
Z(x_1, x_2) = k(W((x_1, x_2)) = ln (x_1^\gamma x_2^{(1 - \gamma)})
```
```{math}
= \gamma \; ln(x_1) + (1 − \gamma) \; ln(x_2) 
```


## Some types of functions
Some types of univariate functions include the following:
- Polynomial functions
    - These include constant functions, linear (and affine) functions, quadratic functions, and some power functions as special cases.
- Exponentional functions
- Power functions
- Logarithmic functions
- Trigonometric functions
    - We are unlikely to have enough time to cover trigonometric functions in this course.
There are also multivariate versions of these types of functions.

### Polynomial functions
A **polynomial function** (of one variable) is a function of the form

$$
\begin{align*}
f(x) &= \sum_{i = 0}^n a_i x^i
\\
&= a_n x^n + a_{n − 1} x^{n − 1} + · · · + a_1 x^1 + a_0 x^0
\\
&= a_n x^n + a_{n − 1} x^{n − 1} + · · · + a_1 x + a_0
\end{align*}
$$

In order to distinguish between different types of polynomials, we will typically assume that the coefficient on the term with the highest power of the variable $x$ is non-zero. The only exception is the case in which this term involves $x^0 = 1$, in which case we will allow both $a_0 \ne 0$ and $a_0 = 0$.

#### Examples
- A constant (degree zero) polynomial ($a_0 \ne 0$ or $a_0 = 0$):

$$
f(x) = a_0
$$

- A linear (degree one) polynomial ($a_1 \ne 0$):

$$
f(x) = a_1 x + a_0
$$
It is sometimes useful to distinguish between linear functions and affine functions. See below for details.

- A quadratic (degree two) polynomial ($a_2 \ne 0$):

$$
f(x) = a_2 x^2 + a_1 x + a_0
$$

- A cubic (degree three) polynomial ($a_3 \ne 0$):

$$
f(x) = a_3 x^3 + a_2 x^2 + a_1 x + a_0
$$

### Affine functions and linear functions
We often loosely speak about a linear function of one variable being a function of the form $f(x) = a_1 x + a_0$, where $a_0 \in \mathbb{R}$ and $a_1 \in \mathbb{R} \setminus \{0\}$ are fixed parameters, and $x \in \mathbb{R}$ is the single variable. 

Sometimes, we want to be more precise than this in order to identify the special case in which $a_0 = 0$. In such cases, we call a function of the general form $f(x) = a_1 x + a_0$, in which $a_0 \in \mathbb{R}$, an **affine function**; and a function of the specific form $f(x) = a_1 x$, in which $a_0 = 0$, a **linear function**.

Using this more precise terminology, the family of linear functions is a proper subset of the family of affine functions. Note that we assume that $a_1 \in \mathbb{R} \setminus \{0\}$ in both cases.

### Exponential functions
An **exponential function** is a non-linear function in which the independent variable appears as an exponent.

An example of an exponential function is

$$
f(x) = Ca^x
$$

where $C$ is a fixed parameter (called the **coefficient**), $a \in \mathbb{R}$ is a fixed parameter (called the **base**), and $x \in \mathbb{R}$ is an independent variable (called the **exponent**).

- Note that if $a = 0$, then $f(x)$ is only defined for $x > 0$.
- Note also that if $a < 0$, then sometimes $f(x) \notin \mathbb{R}$. An example of this is the case when $C = 1, a = (−1)$ and $x = \frac{1}{2}$. In this case, we have:

$$
f(\frac{1}{2}) = (1)(−1)^{\frac{1}{2}} = \sqrt{−1} = i \notin \mathbb{R}
$$


#### Popular choices of base
Two popular choices of base for exponential functions are $a = 10$ and $a = e$, where $e$ denotes Euler’s constant. Euler’s constant is defined to be the number

$$
e = \lim_{n \rightarrow \infty} \left(1 + \frac{1}{n} \right)^n
$$
Note that Euler’s constant is an irrational number. This means that it is a real number that cannot be represented as the ratio of an integer to a natural number.

The function $f(x) = e^x$ is sometimes called “the” exponential function.


#### Exponential arithmetic
Assuming that the expressions are well-defined, we have the following laws of exponential arithmetic.

- The power of zero: $a_0 = 1$ if $a \ne 0$, while $a^0$ is undefined if $a = 0$.
- Multiplication of two exponential functions with the same base:

$$
(Ca^x)(Da^y) = CD a^{(x + y)}
$$

- Division of two exponential functions with the same base:

$$
\frac{(Ca^x)}{(Da^y)} = \frac{C}{D} a^{(x - y)}
$$


- An exponential function whose base is an exponential function:

$$
(Ca^x)^y = C^y a^{xy}
$$

### Power functions
A **power function** takes the form

$$
f(x) = Cx^a
$$
where $C \in \mathbb{R}$ is a fixed parameter, $a \in \mathbb{R}$ is a fixed parameter, and $x \in \mathbb{R}$ is an independent variable.

- Note that when $a \in \mathbb{N}$, a power function can also be viewed as a polynomial function with a single term.
- Note that a power function can also be viewed as a type of exponential expression in which the base is $x$ and the exponent is $a$. This means that the laws of exponential arithmetic carry over to “power function arithmetic”.

#### Power function arithmetic
Assuming that the expressions are well-defined, we have the following laws of power function arithmetic.

- The power of zero: $x_0 = 1$ if $x \ne 0$, while $x^0$ is undefined if $x = 0$.
- Multiplication of two power functions with the same base:

$$
(Cx^a)(Dx^b) = CD x^{(a + b)}
$$

- Division of two power functions with the same base:

$$
\frac{(Cx^a)}{(Dx^b)} = \frac{C}{D} x^{(a - b)}
$$


- A power function whose base is itself a power function:

$$
(Cx^a)^b = C^b x^{ab}
$$


#### A rectangular hyperbola
Consider the function $f : \mathbb{R} \setminus \{0\} \rightarrow \mathbb{R}$ defined by $f(x) = \frac{a}{x}$, where $a \ne 0$. This is a special type of power function, as can be seen by noting that it can be rewritten as $f(x) = ax^{−1}$. The equation for the graph of this function is

$$
y = \frac{a}{x}
$$
Note that this equation can be rewritten as $xy = a$. This is the equation of a rectangular hyperbola.

*Graph it on the whiteboard for both the case where $a > 0$ and the case where $a < 0$.*
- Economic application: A constant “own-price elasticity of demand” demand curve.

### Logarithms
A **logarithm** undoes an exponent.
Thus we have

$$
log_a (a^x) = x
$$

The expression $log_a$ stands for “log base $a$” or “logarithm base $a$”. Popular choices of base are $a = 10$ and $a = e$.

The function

$$
g(x) = log_a(x)
$$

is the “logarithm base $a$” function. The “logarithm base $a$” function is the inverse for the “exponential base $a$” function. The reason for this is that

$$
g(f(x)) = g(a^x) = log_a(a^x) = x
$$

#### Natural (or Naperian) logarithms
A “logarithm base *e*” is known as a natural, or Naperian, logarithm. It is named after John Napier. See Shannon (1995, pp. 270-271) for a brief introduction to John Napier. The standard notation for a **natural logarithm** is $ln$, although you could also use $log_e$.

The function

$$ g(x) = ln(x) $$
is the “logarithm base $e$” function. The natural logarithm function is the inverse function for “the” exponential function, since

$$ g(f(x)) = g(e^x) = ln(e^x) = log_e(e^x) = x $$

*Illustrate the inverse (or “reflection through the 45 degree (y = x) line”) relationship between ln (x ) and ex on the whiteboard.*

#### Logarithmic arithmetic
Assuming that the expressions are well-defined, we have the following
laws of logarithmic arithmetic.
- Multiplication of two logarithmic functions with the same base:

$$ log_a(xy) = log_a(x) + log_a(y) $$

- Division of two logarithmic functions with the same base:

$$log_a \left( \frac{x}{y} \right) =  log_a(x) − log_a(y) $$

- A logarithmic function whose argument is an exponential function:

$$log_a(x^y) = y \; log_a(x)$$

Note that

$$log_a(a) = log_a(a^1) = 1$$



### Rational functions
A **rational function** $R(x)$ is simply the ratio of two polynomial functions, $P(x)$ and $Q(x)$. It takes the form

$$
R(x) = \frac{P(x)}{Q(x)} = 
\frac{a_mx^m + a_{m−1} x^{m−1} + · · · + a_1 x + a_0}
{b_n x^n + b_{n−1} x^{n−1} + · · · + b_1 x + b_0}
$$
where

$$ P(x) = a_mx^m + a_{m−1} x^{m−1} + · · · + a_1 x + a_0 $$
is an $m$-th order polynomial (so that $a_m \ne 0$), and

$$ Q(x) =  b_n x^n + b_{n−1} x^{n−1} + · · · + b_1 x + b_0 $$
is an $n$-th order polynomial (so that $b_n \ne 0$).

- Note that there is no requirement that the polynomial functions $P(x)$ and $Q(x)$ be of the same order. (In other words, we do not require that $m = n$.)
- The most interesting case is when $m < n$. In such cases, the rational function $R(x)$ is called a **“proper” rational function**.
- When $m > n$, then we can always use the process of long division to write the original rational function $R(x)$ as the sum of a polynomial function $Y(x)$ and another proper rational function $R^∗(x)$.

This is nicely illustrated by the following example from Chapter 14 of Silverman (1969).

Consider the rational function $R(x) = \frac{x^2 + x − 1}{x − 1}$. Note the following.

% $$
% \polylongdiv{x^2 + x − 1}{x − 1}
% $$

```{figure} _static/img/lecture_03/polynomial_long_div.png
:width: 40%
:align: center
```


Thus we have

$$
R(x) = \frac{x^2 + x − 1}{x − 1} = (x + 2) + \left( \frac{1}{x − 1} \right)
$$

If $R(x)$ is a non-proper rational function, then it can be written as the sum of a polynomial function ($Y(x)$) and a proper rational function ($R^∗(x)$), so that $R(x) = Y(x) + R^∗(x)$.  In the example above, we had $R(x) = \frac{x^2 + x − 1}{x − 1}$, $Y(x) = (x + 2)$, and $R^∗(x) = \left( \frac{1}{x − 1} \right)$, so that

$$
\frac{x^2 + x − 1}{x − 1} = (x + 2) + \left( \frac{1}{x − 1} \right)
$$
The proper rational function component is sometimes known as a “remainder” term.

In cases where $R(x)$ is a proper rational function to begin with, then we have $Y(x) = 0$, leaving us only with the remainder term.

The remainder term $R^∗(x)$ can sometimes be converted into a more convenient form by using the technique of “Partial Fractions”.

### Partial fractions
Consider two rational functions, $R_1(x) = \frac{P_1(x)}{Q_1(x)}$ and $R_2(x) = \frac{P_2(x)}{Q_2(x)}$.  Note that the sum of these two rational functions forms a third rational function, since

$$
R_3(x) = R_1(x) + R_2(x) 
= \frac{P_1(x)}{Q_1(x)} + \frac{P_2(x)}{Q_2(x)} \\
= \frac{(P_1(x) Q_2(x) + P_2(x) Q_1(x))}{Q_1(x) Q_2(x)}
$$
If we reverse this process, we can write the rational function $\frac{(P_1(x) Q_2(x) + P_2(x) Q_1(x))}{Q_1(x) Q_2(x)}$ as the sum of two other rational functions, $\frac{P_1(x)}{Q_1(x)}$ and $\frac{P_2(x)}{Q_2(x)}$. This decomposition is known as a **“partial fractions” decomposition**.


Suppose that $P(x)$ and $Q(x)$ are **relatively prime** polynomials.
- Two polynomials (or, indeed, two integers) are said to be “relatively prime” (or “co-prime”) if their “highest common factor” (which is also known as their “greatest common divisor”) is the number “one”.

Suppose also that the degree of the polynomial $M(x)$ is strictly less than the degree of the polynomial $P(x) Q(x)$.
- This means that the rational function $R^∗(x) = \frac{M(x)}{P(x) Q(x)}$ is a proper rational function. Under these circumstances there exist two unique polynomial functions, $A(x)$ and $B(x)$, such that

$$
R^∗(x) = \frac{M(x)}{P(x) Q(x)}
= \frac{A(x)}{P(x)} + \frac{B(x)}{Q(x)}
$$
where the degree of $A(x)$ is strictly less than the degree of $P(x)$ and
the degree of $B(x)$ is strictly less than the degree of $Q(x)$.

#### Example
The following example comes from Coroneos (Undated b, pp. 86–87, Example 1).

Consider the rational function $R(x) = \frac{7}{(x − 3)(x^2 + 4)}$. First, note that the degree of the numerator is zero, while the degree of the denominator is three. Thus $R(x)$ is a proper rational function.

Second, note that the only factors of $(x − 3)$ are $(x − 3)$ and $1$, while the only factors of $(x^2 + 4)$ are $(x^2 + 4)$, $1$, $(x + 2i)$ and $(x − 2i)$. (In this course, you will not need to worry about the last two of these factors, because we will restrict attention to only real roots of polynomials.) 

Thus the highest common factor of $(x − 3)$ and $(x^2 + 4)$ is $1$. This means that $(x − 3)$ and $(x^2 + 4)$ are relatively prime. Thus we know that a partial fractions decomposition of $R(x)$ exists for this example.

We want to find two polynomials, $A(x)$ and $B(x)$, such that 

$$
\frac{7}{(x − 3)(x^2 + 4)} = \frac{A(x)}{(x − 3)} + \frac{B(x)}{(x^2 + 4)}
$$
degree$(A(x)) < 1$, and degree$(B(x)) < 2$.

Note that 

$$
\frac{7}{(x − 3)(x^2 + 4)} = \frac{A(x)}{(x − 3)} + \frac{B(x)}{(x^2 + 4)} \\
\iff 
\frac{7}{(x − 3)(x^2 + 4)} = \frac{A(x) (x^2 + 4) + B(x) (x - 3)}{(x - 3)(x^2 + 4)}
$$

This requires that

$$ A(x) (x^2 + 4) + B(x) (x − 3) = 7 $$


- Since degree$(A(x)) < 1$, we know that degree$(A(x)) = 0$. This means that $A(x) = ax^0 = a$ for some $a \in \mathbb{R}$.
- Since degree$(B(x)) < 2$, we know that either degree$(B(x)) = 0$ or degree$(B(x)) = 1$. This means that $B(x) = bx^1 + cx^0 = bx + c$ for some $b \in \mathbb{R}$ and some $c \in \mathbb{R}$. (If degree$(B(x)) = 0$, then $b = 0$. If degree$(B(x)) = 1$, then $b \ne 0$.)

Thus we need

$$ a(x^2 + 4) + (bx + c)(x − 3) = 7 $$
This requires that

$$ ax^2 + 4a + bx^2 − 3bx + cx − 3c = 7 $$
which can be simplified to obtain

$$ (a + b)x^2 + (c − 3b)x + (4a − 3c) = 0x^2 + 0x + 7 $$ 

Thus we have the following system of three linear equations in three
unknown variables that we need to solve.
- Equation (1): $a + b = 0 \implies b = −a$.
- Equation (2) $c − 3b = 0 \implies c = 3b$.
- Equation (3): $4a − 3c = 7$.

Equations (1) and (2) imply that $c = −3a$. This result and Equation (3) imply that $13a = 7$.
Thus we know that $a = \frac{7}{13}$, $b = \frac{−7}{13}$, and $c = \frac{−21}{13}$.

This means that

$$ A(x) = a = \frac{7}{13}$$
and

$$B(x) = bx + c = \frac{−7}{13} x + \frac{(−21)}{13} 
= \frac{−7}{13} x - \frac{(21)}{13}
= \frac{−7}{13} (x + 3)
$$

As such, we can conclude that

$$
\frac{7}{(x − 3)(x^2 + 4)} = \frac{A(x)}{(x − 3)} + \frac{B(x)}{(x^2 + 4)} \\
= \frac{(\frac{7}{13})}{x - 3} + \frac{(\frac{-7}{13})(x + 3)}{x^2 + 4} \\
= \frac{7}{13(x - 3)} - \frac{7(x + 3)}{13(x^2 + 4)}
$$


## Some useful results about functions

% Fact 1
```{admonition} Fact
:class: important
The pre-image of the union of two sets is equal to the union of the pre-images of each of the two sets: $f^{−1}(A \cup B) = f^{−1}(A) \cup f^{−1}(B)$.
```

```{dropdown} Proof
Let $f: X \rightarrow Y$ and consider two sets, $A \subseteq Y$ and $B \subseteq Y$. We want to show that the pre-image of the union of two sets is equal
to the union of the pre-images of each of the two sets:
$f^{−1}(A \cup B) = f^{−1}(A) \cup f^{−1}(B)$.

Note that

$$
\begin{align*}
f^{−1}(A \cup B) &= \{x \in X: f(x) \in A \cup B\} \\
&= \{x \in X : f(x) \in A \text{, or } f(x) \in B \text{, or both}\} \\
&= \{x \in X : f(x) \in A\} \cup \{x \in X : f(x) \in B\} \\
&= f^{−1}(A) \cup f^{−1}(B)
\end{align*}
$$
```

```{dropdown} Example
Example: $f: \mathbb{Z} \rightarrow \mathbb{R}$ defined by $f(z) = z^2, A = [0, 9] \subset \mathbb{R}$, and $B = [5, 25] \subset \mathbb{R}$. $A \cup B = [0, 25] \subset \mathbb{R}$.

The pre-image mapping here is given by $f^{−1}(y) = \pm \sqrt{y}$ if $\pm \sqrt{y}$ is an integer, and $f^{−1}(y) = \varnothing$ otherwise. Note that this pre-image mapping is not an inverse function, because $\pm \sqrt{y}$ is sometimes multi-valued.

- $f^{−1}(A \cup B) = \{−5, −4, −3, −2, −1, 0, 1, 2, 3, 4, 5\}$
- $f^{−1}(A) = \{−3, −2, −1, 0, 1, 2, 3\}$
- $f^{−1}(B) = \{−5, −4, −3, 3, 4, 5\}$
- $f^{−1}(A) \cup f^{-1}(B) = \{−5, −4, −3, −2, −1, 0, 1, 2, 3, 4, 5\} = f^{−1}(A \cup B)$
```

% Fact 2
```{admonition} Fact
:class: important
The pre-image of the intersection of two sets is equal to the intersection of the pre-images of each of the two sets: $f^{−1}(A \cap B) = f^{−1}(A) \cap f^{−1}(B)$.
```

```{dropdown} Proof
Let $f: X \rightarrow Y$ and consider two sets, $A \subseteq Y$ and $B \subseteq Y$. We want to show that the pre-image of the intersection of two sets is equal to the intersection of the pre-images of each of the two sets: $f^{−1}(A \cap B) = f^{−1}(A) \cap f^{−1}(B)$.

Note that

$$
\begin{align*}
f^{−1}(A \cap B) &= \{x \in X: f(x) \in A \cap B\} \\
&= \{x \in X : f(x) \in A \text{ and } f(x) \in B\} \\
&= \{x \in X : f(x) \in A\} \cap \{x \in X : f(x) \in B\} \\
&= f^{−1}(A) \cap f^{−1}(B)
\end{align*}
$$
```

```{dropdown} Example
Example: $f: \mathbb{Z} \rightarrow \mathbb{R}$ defined by $f(z) = z^2, A = [0, 9] \subset \mathbb{R}$, and $B = [5, 25] \subset \mathbb{R}$. $A \cap B = [5, 9] \subset \mathbb{R}$.

The pre-image mapping here is given by $f^{−1}(y) = \pm \sqrt{y}$ if $\pm \sqrt{y}$ is an integer, and $f^{−1}(y) = \varnothing$ otherwise. Note that this pre-image mapping is not an inverse function, because $\pm \sqrt{y}$ is sometimes multi-valued.

- $f^{−1}(A \cap B) = \{−3, 3\}$
- $f^{−1}(A) = \{−3, −2, −1, 0, 1, 2, 3\}$
- $f^{−1}(B) = \{−5, −4, −3, 3, 4, 5\}$
- $f^{−1}(A) \cap f^{-1}(B) = \{−3, 3\} = f^{−1}(A \cap B)$
```



% Fact 3
```{admonition} Fact
:class: important
The image of the union of two sets is equal to the union of the images of each of the two sets: $f(A \cup B) = f(A) \cup f(B)$.
```

```{dropdown} Proof
Let $f: X \rightarrow Y$ and consider two sets, $A \subseteq Y$ and $B \subseteq Y$. We want to show that the image of the union of two sets is equal to the union of the images of each of the two sets: $f(A \cup B) = f(A) \cup f(B)$.

Note that

$$
\begin{align*}
f(A \cup B) &= \{y \in Y: y = f(x) \text{ and } x \in A \cup B\} \\
&= \{f(x) \in Y : x \in A \cup B\} \\
&= \{f(x) \in Y : x \in A \text{, or } x \in B \text{, or both}\} \\
&= \{f(x) \in Y : x \in A\} \cup \{f(x) \in Y : x \in B\} \\
&= f(A) \cup f(B)
\end{align*}
$$
```

```{dropdown} Example
Example: $f: \mathbb{Z} \rightarrow \mathbb{R}$ defined by $f(z) = z^2, A = \{1, 2, 3} \subset \mathbb{Z}$, and $B = \{2,3,4,5\} \subset \mathbb{Z}$. 

- $A \cup B = \{1,2,3,4,5\} \subset \mathbb{Z}$
- $f(A \cup B) = \{1, 4, 9, 16, 25\} \subset \mathbb{R}$
- $f(A) = \{1, 4, 9\} \subset \mathbb{R}$
- $f(B) = \{4, 9, 16, 25\} \subset \mathbb{R}$
- $f(A) \cup f(B) = \{1, 4, 9, 16, 25\} = f(A \cup B)$
```


% Fact 4

However, the image of the intersection of two sets is not necessarily equal to the intersection of the images of each of the two sets. In other words, there are some cases where $f(A \cap B) \ne f(A) \cap f(B)$.

```{dropdown} Example
Consider the mapping $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ defined by $f(x, y) = x$, along with the sets

$$ A = \{(x, y) \in \mathbb{R}^2: x = 5, y = 5\} = \{(5, 5)\} \subset \mathbb{R}^2$$
and

$$ B = \{(x, y) \in \mathbb{R}^2: x = 5, y = 5\} = \{(5, 10)\} \subset \mathbb{R}^2$$

Note that $A \cap B = \varnothing$, so that $f(A \cap B) = f(\varnothing) = \varnothing$. Note also that

$$f(A) = \{f(5, 5)\} = \{5\} \subset \mathbb{R}$$
and

$$f(B) = \{f(5, 10)\} = \{5\} \subset \mathbb{R}$$
so that

$$f(A) \cap f(B) = \{5\}$$
Clearly, we have

$$f(A \cap B) = \varnothing \ne \{5\} = f(A) \cap f(B)$$
```

## Size (cardinality) of a set

The **size (or cardinality)** of a set is closely related to the number of elements in the set. Indeed, the size of a finite set is equal to the number of elements contained in that set. But how do we generalise this idea to sets that have an infinite number of elements? This is done through the use of functions.

```{admonition} Fact
:class: important
The size of a finite set is equal to the number of elements contained in that set.
```

Because the size (or cardinality) of a set is closely related to the number of
elements in the set, this makes it easy to distinguish between the size of sets with finite, but different, numbers of elements.
- The set $A = \{1, 2, 3\}$ is smaller than the set $B = \{a, b, c, d, e\}$ because there are only three elements in $A$, while there are five elements in $B$ (and $3 < 5$).

It is also easy to distinguish between sets with finite numbers of elements and sets with an infinite number of elements.
- The set $A = \{1, 2, 3\}$ is smaller than the set $\mathbb{N} = \{1, 2, 3, · · · \}$ because there are only a finite number (three) of elements in $A$, while there are an infinite number of elements in $\mathbb{N}$ (and $3 < \infty$).

But how can we distinguish between the sizes of different sets when all of those sets have an infinite number of elements?
- Is $\mathbb{N}$ larger than, the same size as, or smaller than $\mathbb{Q}$?
- Is $\mathbb{N}$ larger than, the same size as, or smaller than $\mathbb{R}$?
- Is $\mathbb{Q}$ larger than, the same size as, or smaller than $\mathbb{R}$?

Let $\#A$ denote the size, or cardinality, of the set A. Since $\mathbb{N} \subset \mathbb{Q} \subset \mathbb{R}$, we might expect that $\#\mathbb{N} < \#\mathbb{Q} < \#\mathbb{R}$. However, it turns out that this is not quite right (although the direction of this intuition is partially correct).

This intuition can be used to accurately reveal that $\#\mathbb{N} \leqslant \#\mathbb{Q} \leqslant \#\mathbb{R}$.

A more precise statement of the actual relationship between the cardinalities of these three sets $\#\mathbb{N} = \#\mathbb{Q} < \#\mathbb{R}$.

One way to generalise the notion of set size being related to the number of elements of a set to infinite sets is to think about one-to-one mappings between sets.

- If it is possible to construct a one-to-one function $f: X \rightarrow Y$, then $Y$ must be at least as large as $X$.
- If it is possible to construct a one-to-one and onto function $f: X \rightarrow Y$, then both (a) $Y$ must be at least as large as $X$ and (b) $X$ must be at least as large as $Y$. This means that $X$ and $Y$ must have the same size.

We are particularly interested in three classes of set size: finite, countably infinite, and uncountably infinite.

```{admonition} Definition
:class: caution
A set is said to be a **finite set** if it has at most a finite number of
elements.

- Note that since zero is a finite number, $\varnothing$ is a finite set.
```

```{admonition} Definition
:class: caution
A set $X$ with an infinite number of elements is said to be **countably infinite** if there exists a one-to-one and onto function $f: X \rightarrow \mathbb{N}$.
```

Clearly, countably infinite sets are larger than finite sets. Examples of countably infinite sets include $\mathbb{N}$ and $\mathbb{Q}$.
Thus we have $\#\mathbb{N} = \#\mathbb{Q}$.

All other sets with an infinite number of elements are said to be **uncountably infinite**. Since the smallest set with an infinite number of elements is $\mathbb{N}$, uncountably infinite sets are larger than countably infinite sets. Examples of uncountably infinite sets include $\mathbb{R}$, $[a, b]$ where $a < b$ and $(a, b)$ where $a < b$. Thus we have $\#\mathbb{N} = \#\mathbb{Q} < \#\mathbb{R}$.

```{admonition} Definition
:class: caution
A set that is either finite or countably infinite is said to be **countable**. A set that is uncountably infinite is said to be **uncountable**.
```

Some useful results about countable and uncountable sets:
- Every subset of a countable set is countable.
- The union of a countable number of countable sets is itself a countable set.
- Every infinite set (countable or uncountable) has a countable subset.
- Every infinite set has the same size as (at least) one of its proper subsets.
- Every non-empty set $X$ is smaller than its power set $2^X$.
- (The Cantor-Bernstein Theorem): If $X_1 \subseteq X, Y_1 \subseteq Y, \#X_1 = \#Y \text{ and } \#Y_1 = \#X$, then $\#X = \#Y$.


## Some economic examples of mappings
- Utility functions
- Production functions
- Marshallian demand functions
- Marshallian demand correspondences
- Hicksian demand functions
- Hicksian demand correspondences
- Supply functions
- Best response functions
- Best response correspondences

### Utility functions
A utility function $U : X \rightarrow \mathbb{R}$ maps a consumption set $X$ into the real line. The consumption set $X$ will typically be a set of possible commodity bundles or a set of possible bundles of characteristics.

Often, it will be the case that $X = R^L_+$. Some examples include the following:
- Cobb-Douglas: $U(x_1, x_2) = x_1^\alpha x_2^{(1 − \alpha)}$ where $\alpha \in (0, 1)$.
- Perfect Substitutes: $U(x_1, x_2) = x_1 + x_2$.
- Perfect Complements (or Leontief): $U(x_1, x_2) = \text{min}(x_1, x_2)$.

### Marshallian demand functions
An individual’s Marshallian demand function or correspondence 
$d: \mathbb{R}^L_{++} \times \mathbb{R}_{++} \rightarrow \mathbb{R}^L_+$ maps the space of prices and income combinations into the space of commodity bundles. Example: Marshallian demands for Cobb-Douglas preferences in a two commodity world.

Here we have a Marshallian demand function:

$$
d(p_1, p_2, y) = (x_1^d (p_1, p_2, y), x_2^d(p_1, p_2, y)) 
= (\frac{\alpha y}{p_1}, \frac{(1 − \alpha)y}{p_2})
$$

Here we have a Marshallian demand correspondence:

$$
d(p_1, p_2, y) = (x_1^d (p_1, p_2, y), x_2^d(p_1, p_2, y)) 
= 
\begin{cases}
( \frac{y}{p_1}, 0) \quad \text{ if } p_1 < p_2 \\
(\alpha, \frac{y − p_1 \alpha}{p_2} ) \text{ where } \alpha \in [0, \frac{y}{p_1}] \quad \text{ if } p1 = p2 \\
(0, \frac{y}{p_2}) \quad \text{ if } p_1 > p_2
\end{cases}
$$




