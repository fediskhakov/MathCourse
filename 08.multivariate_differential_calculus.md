# Multivariate differential calculus

<small>‚è± <span class="eta"></span> | <span class="words"></span> words</small>

````{dropdown} Sources and reading guide

```{figure} _static/img/bibliography/shsc2016.png
:width: 100px
:align: left
```
{cite:ps}`sydsaeter2016`

Chapters 11 and 12 (pp. 407-494).

<div style="clear: both"></div>

Introductory level references:
- {cite:ps}`bradley2008`: Chapter 7, Sections 1 to 3 (pp. 360-408).
- {cite:ps}`haeussler1987`: Chapter 17, Sections 1 to 7 and Sections 9 to 10 (pp. 668-706 and 714-723).
- {cite:ps}`shannon1995`: Chapter 10, Sections 1 to 6 (pp. 450-489).

Intermediate mathematical economics textbooks:
- {cite:ps}`chiang1984`: Chapters 6-8 and Chapter 10 (pp. 127-227 and 268-306).
- {cite:ps}`chiang2005`: Chapters 6 to 10 (pp. 124-290).
- {cite:ps}`simon1994`: Chapters 2 to 5 (pp. 10-103).

Mathematics textbooks:
- {cite:ps}`spiegel1981a`: Chapters 4, 6, 7 and 8; pp. 57-79 and 101-179.
- {cite:ps}`spiegel1981b`: Chapters 3 and 4; pp. 35-81.
````



## Our objective

We want to extend our discussion of differential calculus to a setting in which there is more than one choice variable. In other words, we want to examine multivariate real-valued functions.

These are functions of the form $f: X \rightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}^{n}$ and $n \in \mathbb{N} \backslash\{1\}$ (that is, $n$ is a natural number other than the number one).

Often we will write such functions as $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$.

We will be focussing on two concepts. These are **partial derivatives** and **total differentials**.



### Single-real-valued multivariate functions

Let $f: X \longrightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}^{n}$ and $n \in \mathbb{N} \backslash\{1\}$. This type of function is a **single-real-valued multivariate function**.

- It is single-real-valued because, for any choice of inputs (independent variables), it produces a single real number as the output (dependent variable). The output might vary with the choice of inputs.
- It is multi-variate because there is more than one input (independent variable).

Recall the following:
- $X \subseteq \mathbb{R}^{n}$ is the domain of $f$.
- $\mathbb{R}$ is the co-domain of $f$.
- $f(X)=\{y \in \mathbb{R}: y=f(x)$ for some $x \in X\}$ is the image of $X$ under $f$.
- Sometimes we might refer to $f(X)$ as the range of the function $f$.
- Clearly $f(X) \subseteq \mathbb{R}$.
- Since $f$ is a function, we know that it associates only one point in $f(X)$ with any particular point in $X$.


## Partial derivatives

- Consider a function $f: X \longrightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}^{n}$. This function can be written as $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$.
- Suppose that we hold the values that are taken by $(n-1)$ of the independent variables constant and only allow one of the independent variables to take on different values. Specifically, suppose that we fix the value of $x_{i}=\bar{x}_{i}$ for all $i \in\{1,2, \cdots, n\} \backslash\{k\}$ and allow only $x_{k}$ to vary.
- To simplify notation, let

$$
x_{-k}=\left(x_{1}, x_{2}, \cdots, x_{k-1}, x_{k+1}, \cdots, x_{n}\right)
$$

and

$$
\bar{x}_{-k}=\left(\bar{x}_{1}, \bar{x}_{2}, \cdots, \bar{x}_{k-1}, \bar{x}_{k+1}, \cdots, \bar{x}_{n}\right) .
$$


We can now think about the multivariate function as a univariate function, with $x_{k}$ being the sole independent variable. To be precise, we have

$$
g\left(x_{k}\right)=f\left(x_{k} ; \bar{x}_{-k}\right) .
$$

Suppose that the univariate function $g\left(x_{k}\right)$ is differentiable and that its derivative is given by

$$
g^{\prime}\left(x_{k}\right)=\frac{d g\left(x_{k}\right)}{d x_{k}}
$$

We use this derivative to define the partial derivative of the function $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ with respect to the variable $x_{k}$. To be precise, we define this partial derivative as

$$
\frac{\partial f\left(x_{k}, x_{-k}\right)}{\partial x_{k}} \equiv \frac{d g\left(x_{k}\right)}{d x_{k}}
$$

As a matter of convenience will sometimes use the following notation for a partial derivative:

$$
f_{k}\left(x_{k}, x_{-k}\right)=\frac{\partial f\left(x_{k}, x_{-k}\right)}{\partial x_{k}} .
$$

Note that $f_{k}\left(x_{k} ; \bar{x}_{-k}\right)$ will potentially depend on the values taken by all of the independent variables, not just on the one that is allowed to vary.


The formal definition of the partial derivative of the function $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ with respect to the variable $x_{k}$ is

$$
\begin{aligned}
\frac{\partial f\left(x_{k}, x_{-k}\right)}{\partial x_{k}} & \equiv \lim _{h \rightarrow 0}\left\{\frac{f\left(x_{k}+h, x_{-k}\right)-f\left(x_{k}, x_{-k}\right)}{\left(x_{k}+h\right)-x_{k}}\right\} \\
& =\lim _{h \rightarrow 0}\left\{\frac{f\left(x_{k}+h, x_{-k}\right)-f\left(x_{k}, x_{-k}\right)}{h}\right\} .
\end{aligned}
$$

Note that if we think of $f\left(x_{k}, x_{-k}\right)$ as a univariate function of $x_{k}$ alone, then this definition collapses to the definition of the first-order derivative of a univariate function.


- Let $S \subseteq \mathbb{R}^{n}$ be an open set.
- Consider a function $f: S \longrightarrow \mathbb{R}^{n}$.
- This function can be written as $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$.
- The vector of first-order partial derivatives of $f$ at the point $x$, $\left(\frac{\partial f(x)}{\partial x_{1}}, \frac{\partial f(x)}{\partial x_{2}}, \cdots, \frac{\partial f(x)}{\partial x_{n}}\right)^{T}$, is sometimes known as the gradient vector of $f$ at $x$. It is commonly denoted by one of $D_{x} f(x), \nabla f(x)$, or $\operatorname{grad} f(x)$.
- Theorem: If all $n$ of the first-order partial derivatives of $f$ exist and are continuous at the point $x \in X \subseteq \mathbb{R}^{n}$, then the function $f$ is differentiable at the point $x \in X \subseteq \mathbb{R}^{n}$.
- Terminology: The function $f(x)$ is said to be "(at least) once continuously differentiable on $X$ " if and only if all of the first-order partial derivatives of $f$ exist and are continuous on $X$.


```{dropdown} Example 1

Let $f(x, y)=2 x^{2}-x y+y^{2}$. We will use the definition of a partial derivative find the partial derivative of $f$ with respect to $x$. We have

$$
\begin{aligned}
\frac{\partial f}{\partial x} &= \lim _{h \rightarrow 0}\left\{\frac{f(x+h, y)-f(x, y)}{(x+h)-x}\right\} \\
&= \lim _{h \rightarrow 0}\left\{\frac{f(x+h, y)-f(x, y)}{h}\right\} \\
&= \lim _{h \rightarrow 0}\left\{\frac{\left(2(x+h)^{2}-(x+h) y+y^{2}\right)}{h} -\frac{\left(2 x^{2}-x y+y^{2}\right)}{h}\right\} \\
&= \lim _{h \rightarrow 0}\left\{\frac{2\left(x^{2}+2 x h+h^{2}\right)-x y-h y}{h}
+ \frac{y^{2}-2 x^{2}+x y-y^{2}}{h}\right\} \\
&= \lim _{h \rightarrow 0}\left\{\frac{2 x^{2}+4 x h+2 h^{2}-x y-h y}{h} + \frac{y^{2}-2 x^{2}+x y-y^{2}}{h}\right\} \\
&= \lim _{h \rightarrow 0}\left\{\frac{4 x h+2 h^{2}-h y}{h}\right\} \\
&= \lim _{h \rightarrow 0}\{4 x+2 h-y\} \\
&= 4 x+2(0)-y \\
&= 4 x-y .
\end{aligned}
$$

```


```{dropdown} Example 2

Let $f(x, y)=x^{3} y+e^{x y^{2}}=x^{3} y+\exp \left(x y^{2}\right)$. We will use our knowledge of univariate calculus to find the partial derivative of $f$ with respect to $x$ and the partial derivative of $f$ with respect to $y$. We have

$$
\begin{aligned}
\frac{\partial f}{\partial x} & =\frac{\partial}{\partial x}\left\{x^{3} y+\exp \left(x y^{2}\right)\right\} \\
& =\frac{\partial}{\partial x}\left\{x^{3} y\right\}+\frac{\partial}{\partial x}\left\{\exp \left(x y^{2}\right)\right\} \\
& =3 x^{2} y+y^{2} \exp \left\{x y^{2}\right\}
\end{aligned}
$$

and

$$
\begin{aligned}
\frac{\partial f}{\partial y} & =\frac{\partial}{\partial y}\left\{x^{3} y+\exp \left(x y^{2}\right)\right\} \\
& =\frac{\partial}{\partial y}\left\{x^{3} y\right\}+\frac{\partial}{\partial y}\left\{\exp \left(x y^{2}\right)\right\} \\
& =x^{3}+2 x y \exp \left\{x y^{2}\right\}
\end{aligned}
$$


```


### Some economic applications

- Marginal utilities;
- Marginal products;
- Various elasticities of demand;
- Cournot aggregation; and
- Engel aggregation.



### Marginal utilities

We will provide some illustrations of an application of the concept of a partial derivative by using a variety of different two-good utility functions. These utility functions take the form $f: \mathbb{R}_{+}^{2} \longrightarrow \mathbb{R}$. We will think of this as a function that ranks bundles of two commodities according to their desirability to the consumer.  The utility function may be written as

$$
U=U\left(Q_{1}, Q_{2}\right)
$$

where $U$ is the level of utility (which is an ordinal concept, not a cardinal one, for people like me who worry about such things), $Q_{1}$ is the amount of good one that is consumed and $Q_{2}$ is the amount of good two that is consumed.


We will consider a perfect substitutes utility function, a generalised perfect substitutes utility function, a Cobb-Douglas utility function and a constant elasticity of substitution utility function.

We should emphasise that because utility is only an ordinal concept (only ranking is important), the concept of marginal utility is not a meaningful one. However, the related concept of a marginal rate of substitution is meaningful.


```{admonition} Perfect substitutes
:class: tip
Suppose that we have a perfect substitutes utility function:

$$
U\left(Q_{1}, Q_{2}\right)=Q_{1}+Q_{2}
$$

The marginal utility of good one is simply the first-order partial derivative of this utility function with respect to the quantity of good one that is consumed. Thus we have

$$
M U_{1}\left(Q_{1}, Q_{2}\right)=\frac{\partial U\left(Q_{1}, Q_{2}\right)}{\partial Q_{1}}=\frac{\partial\left(Q_{1}+Q_{2}\right)}{\partial Q_{1}}=1
$$

The marginal utility of good two is simply the first-order partial derivative of this utility function with respect to the quantity of good two that is consumed. Thus we have

$$
M U_{2}\left(Q_{1}, Q_{2}\right)=\frac{\partial U\left(Q_{1}, Q_{2}\right)}{\partial Q_{2}}=\frac{\partial\left(Q_{1}+Q_{2}\right)}{\partial Q_{2}}=1
$$
```


```{admonition} Generalised perfect substitutes
:class: tip
Suppose that we have a generalised perfect substitutes utility function:

$$
U\left(Q_{1}, Q_{2}\right)=\alpha Q_{1}+\beta Q_{2}
$$

The marginal utility of good one is simply the first-order partial derivative of this utility function with respect to the quantity of good one that is consumed. Thus we have

$$
M U_{1}\left(Q_{1}, Q_{2}\right)=\frac{\partial U\left(Q_{1}, Q_{2}\right)}{\partial Q_{1}}=\frac{\partial\left(\alpha Q_{1}+\beta Q_{2}\right)}{\partial Q_{1}}=\alpha
$$

The marginal utility of good two is simply the first-order partial derivative of this utility function with respect to the quantity of good two that is consumed. Thus we have

$$
M U_{2}\left(Q_{1}, Q_{2}\right)=\frac{\partial U\left(Q_{1}, Q_{2}\right)}{\partial Q_{2}}=\frac{\partial\left(\alpha Q_{1}+\beta Q_{2}\right)}{\partial Q_{2}}=\beta
$$
```


```{admonition} Cobb-Douglas
:class: tip
Suppose that we have a Cobb-Douglas utility function:

$$
U\left(Q_{1}, Q_{2}\right)=Q_{1}^{\alpha} Q_{2}^{1-\alpha}
$$

The marginal utility of good one is simply the first-order partial derivative of this utility function with respect to the quantity of good one that is consumed.
Thus we have

$$
\begin{aligned}
M U_{1}\left(Q_{1}, Q_{2}\right) & =\frac{\partial U\left(Q_{1}, Q_{2}\right)}{\partial Q_{1}} \\
& =\frac{\partial Q_{1}^{\alpha} Q_{2}^{1-\alpha}}{\partial Q_{1}} \\
& =\alpha Q_{1}^{\alpha-1} Q_{2}^{1-\alpha} \\
& =\alpha Q_{1}^{-(1-\alpha)} Q_{2}^{1-\alpha} \\
& =\alpha\left(\frac{Q_{2}^{1-\alpha}}{Q_{1}^{1-\alpha}}\right) \\
& =\alpha\left(\frac{Q_{2}}{Q_{1}}\right)^{1-\alpha}
\end{aligned}
$$


The marginal utility of good two is simply the first-order partial derivative of this utility function with respect to the quantity of good two that is consumed. Thus we have

$$
\begin{aligned}
M U_{2}\left(Q_{1}, Q_{2}\right) & =\frac{\partial U\left(Q_{1}, Q_{2}\right)}{\partial Q_{2}} \\
& =\frac{\partial Q_{1}^{\alpha} Q_{2}^{1-\alpha}}{\partial Q_{2}} \\
& =(1-\alpha) Q_{1}^{\alpha} Q_{2}^{1-\alpha-1} \\
& =(1-\alpha) Q_{1}^{\alpha} Q_{2}^{-\alpha} \\
& =(1-\alpha)\left(\frac{Q_{1}^{\alpha}}{Q_{2}^{\alpha}}\right) \\
& =(1-\alpha)\left(\frac{Q_{1}}{Q_{2}}\right)^{\alpha}
\end{aligned}
$$
```

```{admonition} CES utility
:class: tip

Suppose that we have a constant elasticity of substitution utility function:

$$
U\left(Q_{1}, Q_{2}\right)=\left(\alpha Q_{1}^{\rho}+\beta Q_{2}^{\rho}\right)^{\frac{1}{\rho}}
$$

The marginal utility of good one is simply the first-order partial derivative of this utility function with respect to the quantity of good one that is consumed. Thus we have

$$
\begin{aligned}
M U_{1}\left(Q_{1}, Q_{2}\right) & =\frac{\partial U\left(Q_{1}, Q_{2}\right)}{\partial Q_{1}} \\
& =\frac{\partial\left(\left(\alpha Q_{1}^{\rho}+\beta Q_{2}^{\rho}\right)^{\frac{1}{\rho}}\right)}{\partial Q_{1}} \\
& =\left(\frac{1}{\rho}\right)\left(\alpha Q_{1}^{\rho}+\beta Q_{2}^{\rho}\right)^{\frac{1}{\rho}-1}\left(\rho \alpha Q_{1}^{\rho-1}\right) \\
& =\alpha Q_{1}^{\rho-1}\left(\alpha Q_{1}^{\rho}+\beta Q_{2}^{\rho}\right)^{\frac{(1-\rho)}{\rho}} .
\end{aligned}
$$

The marginal utility of good two is simply the first-order partial derivative of this utility function with respect to the quantity of good two that is consumed. Thus we have

$$
\begin{aligned}
M U_{2}\left(Q_{1}, Q_{2}\right) & =\frac{\partial U\left(Q_{1}, Q_{2}\right)}{\partial Q_{2}} \\
& =\frac{\partial\left(\left(\alpha Q_{1}^{\rho}+\beta Q_{2}^{\rho}\right)^{\frac{1}{\rho}}\right)}{\partial Q_{2}} \\
& =\left(\frac{1}{\rho}\right)\left(\alpha Q_{1}^{\rho}+\beta Q_{2}^{\rho}\right)^{\frac{1}{\rho}-1}\left(\rho \beta Q_{2}^{\rho-1}\right) \\
& =\beta Q_{2}^{\rho-1}\left(\alpha Q_{1}^{\rho}+\beta Q_{2}^{\rho}\right)^{\frac{(1-\rho)}{\rho}} .
\end{aligned}
$$
```


### Marginal products

We will provide some illustrations of an application of the concept of a partial derivative by using a variety of different two-input and one-output production technologies. These technologies can be represented by a production function of the form $f: \mathbb{R}_{+}^{2} \longrightarrow \mathbb{R}_{+}$. We will think of this as a function that turns bundles of labour and capital into an amount of output.

The production function may be written as

$$
Q=f(L, K)
$$

where $Q=f(L, K)$ is the quantity of output that is produced when $L$ units of labour and $K$ units of capital are employed.

We will consider a perfect substitutes production function, a generalised perfect substitutes production function, a Cobb-Douglas production function and a constant elasticity of substitution production function.


```{admonition} Perfect substitutes
:class: tip

Suppose that we have a perfect substitutes production function:

$$
f(L, K)=L+K .
$$

The marginal product of labour is simply the first-order partial derivative of this production function with respect to the quantity of labour that is employed. Thus we have

$$
M P_{L}(L, K)=\frac{\partial f(L, K)}{\partial L}=\frac{\partial(L+K)}{\partial L}=1 .
$$

The marginal product of capital is simply the first-order partial derivative of this production function with respect to the quantity of capital that is employed. Thus we have

$$
M P_{K}(L, K)=\frac{\partial f(L, K)}{\partial K}=\frac{\partial(L+K)}{\partial K}=1
$$

```


```{admonition} Generalised perfect substitutes
:class: tip

Suppose that we have a generalised perfect substitutes production function:

$$
f(L, K)=\alpha L+\beta K .
$$

The marginal product of labour is simply the first-order partial derivative of this production function with respect to the quantity of labour that is employed. Thus we have

$$
M P_{L}(L, K)=\frac{\partial f(L, K)}{\partial L}=\frac{\partial(\alpha L+\beta K)}{\partial L}=\alpha
$$

The marginal product of capital is simply the first-order partial derivative of this production function with respect to the quantity of capital that is employed. Thus we have

$$
M P_{K}(L, K)=\frac{\partial f(L, K)}{\partial K}=\frac{\partial(\alpha L+\beta K)}{\partial K}=\beta .
$$

```

```{admonition} Cobb-Douglas
:class: tip

Suppose that we have a Cobb-Douglas production function:

$$
f(L, K)=A L^{\alpha} K^{\beta} .
$$

The marginal product of labour is simply the first-order partial derivative of this production function with respect to the quantity of labour that is employed. Thus we have

$$
\begin{aligned}
M P_{L}(L, K) & =\frac{\partial f(L, K)}{\partial L} \\
& =\frac{\partial\left(A L^{\alpha} K^{\beta}\right)}{\partial L} \\
& =\alpha A L^{\alpha-1} K^{\beta}
\end{aligned}
$$


The marginal product of capital is simply the first-order partial derivative of this production function with respect to the quantity of capital that is employed. Thus we have

$$
\begin{aligned}
M P_{K}(L, K) & =\frac{\partial f(L, K)}{\partial K} \\
& =\frac{\partial\left(A L^{\alpha} K^{\beta}\right)}{\partial K} \\
& =\beta A L^{\alpha} K^{\beta-1}
\end{aligned}
$$

```


```{admonition} CES
:class: tip
Suppose that we have a constant elasticity of substitution production function:

$$
f(L, K)=\left(\alpha L^{\rho}+\beta K^{\rho}\right)^{\frac{1}{\rho}} .
$$

The marginal product of labour is simply the first-order partial derivative of this production function with respect to the quantity of labour that is employed. Thus we have

$$
\begin{aligned}
M P_{L}(L, K) & =\frac{\partial f(L, K)}{\partial L} \\
& =\frac{\partial\left(\left(\alpha L^{\rho}+\beta K^{\rho}\right)^{\frac{1}{\rho}}\right)}{\partial L} \\
& =\left(\frac{1}{\rho}\right)\left(\alpha L^{\rho}+\beta K^{\rho}\right)^{\frac{1}{\rho}-1}\left(\rho \alpha L^{\rho-1}\right) \\
& =\alpha L^{\rho-1}\left(\alpha L^{\rho}+\beta K^{\rho}\right)^{\frac{(1-\rho)}{\rho}} .
\end{aligned}
$$

The marginal product of capital is simply the first-order partial derivative of this production function with respect to the quantity of capital that is employed. Thus we have

$$
\begin{aligned}
M P_{K}(L, K) & =\frac{\partial f(L, K)}{\partial K} \\
& =\frac{\partial\left(\left(\alpha L^{\rho}+\beta K^{\rho}\right)^{\frac{1}{\rho}}\right)}{\partial K} \\
& =\left(\frac{1}{\rho}\right)\left(\alpha L^{\rho}+\beta K^{\rho}\right)^{\frac{1}{\rho}-1}\left(\rho \beta K^{\rho-1}\right) \\
& =\beta K^{\rho-1}\left(\alpha L^{\rho}+\beta K^{\rho}\right)^{\frac{(1-\rho)}{\rho}} .
\end{aligned}
$$

```

### Elasticities of demand

Suppose that an individual's Marshallian (or "Walrasian" or "ordinary" or "uncompensated") demand function for commodity $k$ is given by

$$
Q_{k}=D_{k}\left(p_{1}, p_{2}, \cdots, p_{n}, y\right),
$$

where $p_{i}$ is the price of commodity $i$ for each $i \in\{1,2, \cdots, n\}$ and $y$ is the consumer's income.

- The own-price elasticity of demand for commodity $k$ for this consumer is

$$
\begin{aligned}
\varepsilon_{k}^{k}\left(p_{1}, p_{2}, \cdots, p_{n}, y\right) & =\left(\frac{p_{k}}{Q_{k}}\right)\left(\frac{\partial D_{k}}{\partial p_{k}}\right) \\
& =\left(\frac{p_{k}}{D_{k}\left(p_{1}, p_{2}, \cdots, p_{n}, y\right)}\right)\left(\frac{\partial D_{k}}{\partial p_{k}}\right) .
\end{aligned}
$$

- The cross-price elasticity of demand for commodity $k$ with respect to the price of commodity $l$ for this consumer is

$$
\begin{aligned}
\varepsilon_{l}^{k}\left(p_{1}, p_{2}, \cdots, p_{n}, y\right) & =\left(\frac{p_{l}}{Q_{k}}\right)\left(\frac{\partial D_{k}}{\partial p_{l}}\right) \\
& =\left(\frac{p_{l}}{D_{k}\left(p_{1}, p_{2}, \cdots, p_{n}, y\right)}\right)\left(\frac{\partial D_{k}}{\partial p_{l}}\right) .
\end{aligned}
$$

- The income elasticity of demand for commodity $k$ for this consumer is

$$
\begin{aligned}
\varepsilon_{y}^{k}\left(p_{1}, p_{2}, \cdots, p_{n}, y\right) & =\left(\frac{y}{Q_{k}}\right)\left(\frac{\partial D_{k}}{\partial y}\right) \\
& =\left(\frac{y}{D_{k}\left(p_{1}, p_{2}, \cdots, p_{n}, y\right)}\right)\left(\frac{\partial D_{k}}{\partial y}\right) .
\end{aligned}
$$


We can use these elasticities to classify the types of commodities that are being considered:
- If $\varepsilon_{y}^{k}>0$, then commodity $k$ is a normal good. If $\varepsilon_{y}^{k}<0$, then commodity $k$ is an inferior good.
- If $\varepsilon_{l}^{k}>0$, then commodities $k$ and $l$ are substitutes. If $\varepsilon_{l}^{k}<0$, then commodities $k$ and $l$ are complements.
- The Marshallian demand curve for most commodities will usually slope down. As such, we would usually expect $\varepsilon_{k}^{k}<0$.
- However, there are circumstances in which the Marshallian demand curve for a commodity can slope up over some range of prices (at least in theory). Such commodities are known as Giffen goods. In such circumstances, we would have $\varepsilon_{k}^{k}>0$ over the relevant range of prices. Note that a necessary, but not sufficient, condition for a commodity to be a Giffen good is that it be an inferior good.



### Cournot aggregation

Suppose that a consumer's preferences over bundles of $L$ commodities are locally non-satiated in the neighbourhood of any potentially feasible commodity bundle. Then we know that budget exhaustion (which is sometimes called Walras' law for the individual) must hold for the consumer. This ensures that

$$
\sum_{l=1}^{L} p_{l} x_{l}(p, y)=y
$$

where $p=\left(p_{1}, p_{2}, \cdots, p_{n}\right)=\left(p_{k}, p_{-k}\right)$ is the price vector, $y$ is the consumer's income and $x_{l}(p, y)$ is the consumer's Marshallian demand for good $l$.

Note that this can be rewritten as

$$
p_{k} x_{k}\left(p_{k}, p_{-k}, y\right)+\sum_{l \neq k} p_{l} x_{l}(p, y)=y
$$


Partially differentiating both sides of this equation with respect to the price of commodity $k$, we obtain

$$
\left\{(1) x_{k}(p, y)+p_{k}\left(\frac{\partial x_{k}(p, y)}{\partial p_{k}}\right)\right\}+\sum_{l \neq k} p_{l}\left(\frac{\partial x_{l}(p, y)}{\partial p_{k}}\right)=0 .
$$

This can be simplified to obtain

$$
x_{k}(p, y)+\sum_{l=1}^{L} p_{l} \frac{\partial x_{l}(p, y)}{\partial p_{k}}=0
$$

This can be rearranged to obtain

$$
\sum_{l=1}^{L} p_{l} \frac{\partial x_{l}(p, y)}{\partial p_{k}}=-x_{k}(p, y)
$$

This can be rewritten as

$$
\sum_{l=1}^{L}\left(\frac{p_{k}}{p_{k}}\right)\left(\frac{x_{l}(p, y)}{x_{l}(p, y)}\right)\left(\frac{y}{y}\right) p_{l} \frac{\partial x_{l}(p, y)}{\partial p_{k}}=-x_{k}(p, y) .
$$

This can be rearranged to obtain

$$
\sum_{l=1}^{L}\left(\frac{p_{l} x_{l}(p, y)}{y}\right)\left(\frac{p_{k}}{x_{l}(p, y)}\right)\left(\frac{\partial x_{l}(p, y)}{\partial p_{k}}\right)=-\left(\frac{p_{k} x_{k}(p, y)}{y}\right) .
$$

This can be rewritten as

$$
\sum_{l=1}^{L} s_{l} \varepsilon_{k}^{l}=-s_{k}
$$

where

$$
s_{l}=\frac{p_{l} x_{l}(p, y)}{y}
$$

is the budget share of commodity $l$ and

$$
\varepsilon_{k}^{\prime}=\left(\frac{p_{k}}{x_{l}(p, y)}\right)\left(\frac{\partial x_{l}(p, y)}{\partial p_{k}}\right)
$$

is the $k$ th commodity-price elasticity of demand for commodity $l$.

The above formula is a result known as Cournot aggregation. It provides a relationship between the $k$ th price elasticities of demand for the various commodities.



### Engel aggregation

Suppose that a consumer's preferences over bundles of $L$ commodities are locally non-satiated in the neighbourhood of any potentially feasible commodity bundle. Then we know that budget exhaustion (which is sometimes called Walras' law for the individual) must hold for the consumer. This ensures that

$$
\sum_{l=1}^{L} p_{l} x_{l}(p, y)=y
$$

where $p=\left(p_{1}, p_{2}, \cdots, p_{n}\right)$ is the price vector, $y$ is the consumer's income and $x_{l}(p, y)$ is the consumer's Marshallian demand for good $I$.

Partially differentiating both sides of this equation with respect to income, we obtain

$$
\sum_{l=1}^{L} p_{l}\left(\frac{\partial x_{l}(p, y)}{\partial y}\right)=1
$$

This can be rewritten as

$$
\sum_{l=1}^{L}\left(\frac{y}{y}\right)\left(\frac{x_{l}(p, y)}{x_{l}(p, y)}\right) p_{l}\left(\frac{\partial x_{l}(p, y)}{\partial y}\right)=1
$$

This can be simplified to obtain

$$
\sum_{l=1}^{L}\left(\frac{p_{l} x_{l}(p, y)}{y}\right)\left(\frac{y}{x_{l}(p, y)}\right)\left(\frac{\partial x_{l}(p, y)}{\partial y}\right)=1
$$

This can be rewritten as

$$
\sum_{l=1}^{L} s_{l} \varepsilon_{y}^{l}=1
$$

where

$$
s_{l}=\frac{p_{l} x_{l}(p, y)}{y}
$$

is the budget share of commodity $l$ and

$$
\varepsilon_{y}^{l}=\left(\frac{y}{x_{l}(p, y)}\right)\left(\frac{\partial x_{l}(p, y)}{\partial y}\right)
$$

is the income elasticity of demand for commodity $l$.

The above formula is a result known as Engel aggregation. It provides a relationship between the income elasticities of demand for the various commodities.



## Higher order derivatives

Let $X \subseteq \mathbb{R}^{n}$ be an open set and $f: X \longrightarrow \mathbb{R}$ be a function that is differentiable on $X$, with $D_{X} f=\left(\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}, \cdots, \frac{\partial f}{\partial x_{n}}\right)^{T}$. The vector $D_{x} f(x)$ is called the **gradient vector** of $f$ at the point $x \in S$.

Note that $\frac{\partial f}{\partial x_{k}}: X \longrightarrow \mathbb{R}$ is itself a function.

Suppose that the function $\frac{\partial f}{\partial x_{k}}(x)$ is itself differentiable at the point $x \in X$. Often, we will want this to be the case for all $k \in\{1,2, \cdots, n\}$.
- Consider one such partial derivative, say $\frac{\partial f}{\partial x_{i}}$.
- For each $i \in\{1,2, \cdots, n\}$, we have $n$ "second-order" partial derivatives of $\frac{\partial f}{\partial x_{i}}$. These take the form $\frac{\partial\left(\frac{\partial f}{\partial x_{i}}\right)}{\partial x_{k}}=\frac{\partial^{2} f}{\partial x_{k} \partial x_{i}}$ for each $k \in\{1,2, \cdots, n\}$.
- Note that there are $n^{2}$ such second-order partial derivatives of $f$ in total.
- Thus we have $D_{x^{T}}\left(\frac{\partial f}{\partial x_{i}}\right)=\left(\frac{\partial^{2} f}{\partial x_{1} \partial x_{i}}, \frac{\partial^{2} f}{\partial x_{2} \partial x_{i}}, \cdots, \frac{\partial^{2} f}{\partial x_{n} \partial x_{i}}\right)$ for each $i \in\{1,2, \cdots, n\}$.


Suppose that each of the partial derivatives that make up $D_{x} f$ are themselves differentiable functions at the point $x \in X$. The derivative of the gradient vector $D_{x} f$ at the point $x \in X$ is given by the matrix

$$
D_{x^{T}}\left(D_{x} f(x)\right)=D_{x x^{T}}^{2} f(x)=\left(\begin{array}{cccc}
\frac{\partial^{2} f(x)}{\partial x_{1}^{2}} & \frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{1}} & \cdots & \frac{\partial^{2} f(x)}{\partial x_{n} \partial x_{1}} \\
\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{2}} & \frac{\partial^{2} f(x)}{\partial x_{2}^{2}} & \cdots & \frac{\partial^{2} f(x)}{\partial x_{n} \partial x_{2}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{n}} & \frac{\partial^{2} f(x)}{\partial x_{2} \partial x_{n}} & \cdots & \frac{\partial^{2} f(x)}{\partial x_{n}^{2}}
\end{array}\right)
$$

where $\frac{\partial^{2} f(x)}{\partial x_{i}^{2}}=\frac{\partial^{2} f(x)}{\partial x_{i} \partial x_{i}}$


The matrix $D_{x x^{T}}^{2}(x)$ is called the **Hessian matrix** of $f$ at the point $x \in S$.
- If $f$ is twice-differentiable at every point $x \in X$, then $f$ is said to be twice-differentiable on $X$.
- If $f$ is twice-differentiable on $X$ and $\frac{\partial^{2} f}{\partial x_{k} \partial x_{i}}$ is a continuous function on $X$ for all $(i, k) \in\{1,2, \cdots, n\} \times\{1,2, \cdots, n\}$, then $f$ is said to be twice continuously differentiable on $X$. This is denoted by $f \in C^{2}$ on $X$.

```{admonition} Young's theorem
:class: caution
If $f \in C^{2}$ on $X$, then $\frac{\partial^{2} f}{\partial x_{k} \partial x_{i}}=\frac{\partial^{2} f}{\partial x_{i} \partial x_{k}}$ for all $i \neq k$.

Young's theorem ensures that the Hessian matrix for $f$ will be symmetric when $f \in C^{2}$ on $X$.
```

It is possible to extend the process of differentiation for multivariate functions to even higher orders than second-order derivatives. Doing so for partial derivatives is relatively straight-forward. However, this will not be done in this course.




## Second-order partial derivatives

Consider a function $f: X \longrightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}^{n}$ is an open set. This function can be written as $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$. Assume that this function is at least twice continuously differentiable with respect to all of its arguments.

Suppose that we want to find the second-order partial derivative of the underlying function $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ with respect to the variable $x_{i}$ first and then with respect to the variable $x_{j}$ second. This notation for this second-order derivative is

$$
\frac{\partial^{2} f\left(x_{1}, x_{2}, \cdots, x_{n}\right)}{\partial x_{j} \partial x_{i}}=f_{i j}\left(x_{1}, x_{2}, \cdots, x_{n}\right)
$$

Note that the first-order partial derivative of this function with respect to the variable $x_{i}$ is a function in its own right:

$$
\begin{aligned}
\frac{\partial f\left(x_{1}, x_{2}, \cdots, x_{n}\right)}{\partial x_{i}} & =f_{i}\left(x_{1}, x_{2}, \cdots, x_{n}\right) \\
& =g^{i}\left(x_{1}, x_{2}, \cdots, x_{n}\right)
\end{aligned}
$$

We can use this function $g^{i}\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ to define the second-order partial derivative of the underlying function $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ with respect to the variable $x_{i}$ first and then with respect to the variable $x_{j}$ second To be precise, we have

$$
f_{i j}\left(x_{1}, x_{2}, \cdots, x_{n}\right) \equiv \frac{\partial g^{i}\left(x_{1}, x_{2}, \cdots, x_{n}\right)}{\partial x_{j}}
$$

Thus we have

$$
\frac{\partial^{2} f\left(x_{1}, x_{2}, \cdots, x_{n}\right)}{\partial x_{j} \partial x_{i}}=\frac{\partial}{\partial x_{j}}\left(\frac{\partial f\left(x_{1}, x_{2}, \cdots, x_{n}\right)}{\partial x_{i}}\right)
$$


### Young's theorem

Consider a function $f: X \longrightarrow \mathbb{R}$, where $X \subseteq \mathbb{R}^{n}$ is an open set. This function can be written as $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$.
Suppose that this function is at least twice continuously differentiable with respect to all of its arguments in a non-empty neighbourhood around the point

$$
x^{0}=\left(x_{1}^{0}, x_{2}^{0}, \cdots, x_{n}^{0}\right) .
$$

In this case, the order of differentiation will not affect the second-order partial derivatives evaluated at the point $x^{0}$.  More precisely, in this case we will have

$$
\frac{\partial^{2} f\left(x_{1}^{0}, x_{2}^{0}, \cdots, x_{n}^{0}\right)}{\partial x_{j} \partial x_{i}}=\frac{\partial^{2} f\left(x_{1}^{0}, x_{2}^{0}, \cdots, x_{n}^{0}\right)}{\partial x_{i} \partial x_{j}}
$$

or, if you prefer,

$$
f_{i j}\left(x_{1}^{0}, x_{2}^{0}, \cdots, x_{n}^{0}\right)=f_{j i}\left(x_{1}^{0}, x_{2}^{0}, \cdots, x_{n}^{0}\right) .
$$

This result is known as Young's theorem.  The result is trivially true when $i=j$. This is the case of second-order own partial derivatives.

The result is also true, and much more interesting, when $i \neq j$. This is the case of second-order cross partial derivatives.

```{dropdown} Example 1
Recall the example in which

$$
f(x, y)=x^{3} y+e^{x y^{2}}=x^{3} y+\exp \left(x y^{2}\right)
$$

We have already found that

$$
f_{x}(x, y)=\frac{\partial f}{\partial x}=3 x^{2} y+y^{2} \exp \left\{x y^{2}\right\}
$$

and

$$
f_{y}(x, y)=\frac{\partial f}{\partial y}=x^{3}+2 x y \exp \left\{x y^{2}\right\}
$$

This means that

$$
\begin{aligned}
& f_{x x}(x, y) \\
= & \frac{\partial f_{x}(x, y)}{\partial x} \\
= & \frac{\partial\left(3 x^{2} y+y^{2} \exp \left\{x y^{2}\right\}\right)}{\partial x} \\
= & (2)\left(3 x^{2-1} y\right)+y^{2} \exp \left\{x y^{2}\right\}\left(y^{2}\right) \\
= & 6 x y+y^{4} \exp \left\{x y^{2}\right\}
\end{aligned}
$$


It also means that

$$
\begin{aligned}
& f_{x y}(x, y) \\
= & \frac{\partial f_{x}(x, y)}{\partial y} \\
= & \frac{\partial\left(3 x^{2} y+y^{2} \exp \left\{x y^{2}\right\}\right)}{\partial y} \\
= & 3 x^{2}+\left\{(2 y)\left(\exp \left\{x y^{2}\right\}\right)\right. \\
& \left.+\left(\exp \left\{x y^{2}\right\}\right)(2 x y)\left(y^{2}\right)\right\} \\
= & 3 x^{2}+2 y \exp \left\{x y^{2}\right\}+2 x y^{3} \exp \left\{x y^{2}\right\} .
\end{aligned}
$$

It also means that

$$
\begin{aligned}
& f_{y x}(x, y) \\
= & \frac{\partial f_{y}(x, y)}{\partial x} \\
= & \frac{\partial\left(x^{3}+2 x y \exp \left\{x y^{2}\right\}\right)}{\partial x} \\
= & 3 x^{2}+\left\{(2 y)\left(\exp \left\{x y^{2}\right\}\right)\right. \\
& \left.+\left(y^{2} \exp \left\{x y^{2}\right\}\right)(2 x y)\right\} \\
= & 3 x^{2}+2 y \exp \left\{x y^{2}\right\}+2 x y^{3} \exp \left\{x y^{2}\right\}
\end{aligned}
$$

It also means that

$$
\begin{aligned}
& f_{y y}(x, y) \\
= & \frac{\partial f_{y}(x, y)}{\partial y} \\
= & \frac{\partial\left(x^{3}+2 x y \exp \left\{x y^{2}\right\}\right)}{\partial y} \\
= & 0+\left\{(2 x)\left(\exp \left\{x y^{2}\right\}\right)+\left(2 x y \exp \left\{x y^{2}\right\}\right)(2 x y)\right\} \\
= & 2 x \exp \left\{x y^{2}\right\}+4 x^{2} y^{2} \exp \left\{x y^{2}\right\} .
\end{aligned}
$$

Note that $f_{x y}(x, y)=f_{y x}(x, y)$ in this example.
```



```{dropdown} Example 2

Young's theorem is only guaranteed to hold when the underlying function $f$ is at least twice continuously differentiable in the neighbourhood of the point of interest.

An example in which Young's theorem cannot be applied because the underlying function is not twice continuously differentiable at the point of interest is provided by {cite:ps}`spiegel1981a` (p. 126, Worked Problem 43). This example is presented below.


Consider the function $f: \mathbb{R}^{2} \longrightarrow \mathbb{R}$ defined by

$$
f(x, y)=\left\{\begin{array}{l}
0 \text { if }(x, y)=(0,0) \\
x y\left(\frac{x^{2}-y^{2}}{x^{2}+y^{2}}\right) \text { if }(x, y) \neq(0,0)
\end{array}\right.
$$


If $(x, y)=(0,0)$, then the first-order partial derivatives are

$$
f_{x}(0,0)=\lim _{h \rightarrow 0} \frac{f(h, 0)-f(0,0)}{h}=\lim _{h \rightarrow 0} \frac{0}{h}=0
$$

and

$$
f_{y}(0,0)=\lim _{k \rightarrow 0} \frac{f(0, k)-f(0,0)}{k}=\lim _{k \rightarrow 0} \frac{0}{k}=0
$$

If $(x, y) \neq(0,0)$, then

$$
f_{x}(x, y)=\frac{\partial}{\partial x}\left\{x y\left(\frac{x^{2}-y^{2}}{x^{2}+y^{2}}\right)\right\}=x y\left(\frac{4 x y^{2}}{\left(x^{2}+y^{2}\right)^{2}}\right)+y\left(\frac{x^{2}-y^{2}}{x^{2}+y^{2}}\right)
$$

and

$$
f_{y}(x, y)=\frac{\partial}{\partial y}\left\{x y\left(\frac{x^{2}-y^{2}}{x^{2}+y^{2}}\right)\right\}=x y\left(\frac{-4 x^{2} y}{\left(x^{2}+y^{2}\right)^{2}}\right)+x\left(\frac{x^{2}-y^{2}}{x^{2}+y^{2}}\right)
$$


If $(x, y)=(0,0)$, then the second-order partial derivatives are

$$
\begin{gathered}
f_{x x}(0,0)=\lim _{h \rightarrow 0} \frac{f_{x}(h, 0)-f_{x}(0,0)}{h}=\lim _{h \rightarrow 0} \frac{0}{h}=0, \\
f_{y y}(0,0)=\lim _{k \rightarrow 0} \frac{f_{y}(0, k)-f_{y}(0,0)}{k}=\lim _{k \rightarrow 0} \frac{0}{k}=0, \\
f_{x y}(0,0)=\lim _{k \rightarrow 0} \frac{f_{x}(0, k)-f_{x}(0,0)}{k}=\lim _{k \rightarrow 0} \frac{-k}{k}=-1
\end{gathered}
$$

and

$$
f_{y x}(0,0)=\lim _{h \rightarrow 0} \frac{f_{y}(h, 0)-f_{y}(0,0)}{h}=\lim _{h \rightarrow 0} \frac{h}{h}=1 .
$$

Note that $f_{x y}(0,0) \neq f_{y x}(0,0)$.
```



## Total differentials

Consider a function $f: S \longrightarrow \mathbb{R}$, where $S \subseteq \mathbb{R}^{n}$ is an open set. This function can be written as $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$. Suppose that this function is at least twice continuously differentiable with respect to all of its arguments.

Suppose that we want to consider the impact of a small change in each of the independent variables on the value of the function.

Imagine that the initial values of the independent variables are

$$
x=\left(x_{1}, x_{2}, \cdots, x_{n}\right)
$$

Following the change, the new values of the independent variables are

$$
x+d x=\left(x_{1}+d x_{1}, x_{2}+d x_{2}, \cdots, x_{n}+d x_{n}\right)
$$

Note that the vector of changes in the independent variables is

$$
\begin{aligned}
d x &= x+d x-x \\
&= \left(x_{1}+d x_{1}, x_{2}+d x_{2}, \cdots, x_{n}+d x_{n}\right)
-\left(x_{1}, x_{2}, \cdots, x_{n}\right) \\
&=  \left(d x_{1}, d x_{2}, \cdots, d x_{n}\right) .
\end{aligned}
$$

The actual change in the value of the function that is induced by these changes in the independent variables is

$$
d f=f(x+d x)-f(x)
$$

When the function is non-linear, this actual change can be rather complicated to calculate explicitly.


As such, we will often employ a first-order (or linear) differential approximation for $d f$. This approximation is known as the **(first-order) total differential** of the function. It is given by

$$
d f \approx \sum_{i=1}^{n}\left(\frac{\partial f}{\partial x_{i}}\right) d x_{i}
$$

If the function is twice continuously differentiable in all of its arguments and the change in each of the independent variables is sufficiently small, then this approximation for $d f$ will be reasonably accurate.



## The implicit function theorem

This is Theorem 15.2 in {cite:ps}`simon1994` (p. 341).

Suppose that $G\left(x_{1}, x_{2}, \cdots, x_{n}, y\right)$ is a function that is at least once continuously differentiable around the point $\left(x_{1}^{*}, x_{2}^{*}, \cdots, x_{n}^{*}, y^{*}\right)$. Suppose also that

$$
G\left(x_{1}^{*}, x_{2}^{*}, \cdots, x_{n}^{*}, y^{*}\right)=c \text { for some constant } c
$$

and that

$$
\frac{\partial G}{\partial y}\left(x_{1}^{*}, x_{2}^{*}, \cdots, x_{n}^{*}, y^{*}\right) \neq 0
$$


Under these conditions, there is a function $y\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ defined on some open ball $B$ around the point $\left(x_{1}^{*}, x_{2}^{*}, \cdots, x_{n}^{*}\right)$ that is at least once continuously differentiable such that:

$$
\begin{gathered}
\text { (a) } G\left(x_{1}, x_{2}, \cdots, x_{n}, y\left(x_{1}, x_{2}, \cdots, x_{n}\right)\right)=c \\
\text { for all }\left(x_{1}, x_{2}, \cdots, x_{n}\right) \in B, \\
\text { (b) } y^{*}=y\left(x_{1}^{*}, x_{2}^{*}, \cdots, x_{n}^{*}\right) \text {; and } \\
\text { (c) } \frac{\partial y}{\partial x_{i}}\left(x_{1}^{*}, x_{2}^{*}, \cdots, x_{n}^{*}\right)=-\left(\frac{\frac{\partial G}{\partial x_{i}}\left(x_{1}^{*}, x_{2}^{*}, \cdots, x_{n}^{*}, y^{*}\right)}{\frac{\partial G}{\partial y}\left(x_{1}^{*}, x_{2}^{*}, \cdots, x_{n}^{*}, y^{*}\right)}\right) \\
\text { for each } i \in\{1,2, \cdots, n\} .
\end{gathered}
$$


### Derivatives of implicit functions

Suppose that we know that some variable $y$ is a function of $n$ other variables $\left(x_{1}, x_{2}, \cdots, x_{n}\right)$. Sometimes it is not easy to explicitly characterise this function in the form

$$
y=f\left(x_{1}, x_{2}, \cdots, x_{n}\right) .
$$

In some cases, we might only be able to characterise the function implicitly, along the lines of a relationship of the form

$$
g\left(y, x_{1}, x_{2}, \cdots, x_{n}\right)=b
$$

where $b \in \mathbb{R}$ is some fixed constant.


Suppose that we want to obtain the partial derivative of $y$ with respect to $x_{k}$ in such a case. How would we do this?
Since $b \in \mathbb{R}$ is some fixed constant, we must have $d g=0$.
Note that the total differential for $g$ is

$$
d g=\left(\frac{\partial g}{\partial y}\right) d y+\sum_{i=1}^{n}\left(\frac{\partial g}{\partial x_{i}}\right) d x_{i}
$$

Thus we must have

$$
\left(\frac{\partial g}{\partial y}\right) d y+\sum_{i=1}^{n}\left(\frac{\partial g}{\partial x_{i}}\right) d x_{i}=0
$$


Suppose that the only variables that are allowed to change are $x_{k}$ and $y$. This means that $d x_{k} \neq 0$ and $d y \neq 0$. It also means that $d x_{i}=0$ for all $i \neq k$. Substituting these values into the above equation yields

$$
\left(\frac{\partial g}{\partial y}\right) d y+\left(\frac{\partial g}{\partial x_{k}}\right) d x_{k}=0
$$

This can be rearranged to obtain

$$
\left.\frac{d y}{d x_{k}}\right|_{d x_{i}=0 \text { for all } i \neq k}=\frac{-\left(\frac{\partial g}{\partial x_{k}}\right)}{\left(\frac{\partial g}{\partial y}\right)}
$$

Since we are holding $d x_{i}=0$ for all $i \neq k$, this is really a partial derivative. Thus we have

$$
\frac{\partial y}{\partial x_{k}}=\frac{-\left(\frac{\partial g}{\partial x_{k}}\right)}{\left(\frac{\partial g}{\partial y}\right)}
$$


### Some applications of implicit functions

Some potential applications of the implicit function theorem include the following:
- The slope of an indifference curve
- The slope of an iso-expenditure ("budget") line
- The slope of an isoquant
- The slope of an isocost

Before discussing these examples, we will consider the related concepts of level sets, upper contour sets and lower contour sets.


## Level sets and contour sets

Consider a function $f: X \rightarrow Y$, where $X \subseteq \mathbb{R}^{L}$ and $Y \subseteq \mathbb{R}$.

- A **level set** for the function $f$ defined with respect to a point in the domain $\widehat{x} \in X$ is defined to be

$$
f_{\widehat{x}}^{0}=\{x \in X: f(x)=f(\widehat{x})\} .
$$

- A **weak upper contour** set for the function $f$ defined with respect to a point in the domain $\widehat{x} \in X$ is defined to be

$$
f_{\widehat{x}}^{+}=\{x \in X: f(x) \geq f(\widehat{x})\} .
$$

- A **strong upper contour set** for the function $f$ defined with respect to a point in the domain $\widehat{X} \in X$ is defined to be

$$
f_{\widehat{x}}^{++}=\{x \in X: f(x)>f(\widehat{x})\} .
$$

- A **weak lower contour set** for the function $f$ defined with respect to a point in the domain $\widehat{X} \in X$ is defined to be

$$
f_{\widehat{x}}^{-}=\{x \in X: f(x) \leq f(\widehat{x})\} .
$$

- A **strong lower contour set** for the function $f$ defined with respect to a point in the domain $\widehat{X} \in X$ is defined to be

$$
f_{\widehat{x}}^{--}=\{x \in X: f(x)<f(\widehat{x})\} .
$$


### The slope of an indifference curve

Suppose that $U: \mathbb{R}_{+}^{L} \rightarrow \mathbb{R}$ is a utility function.

The level sets for a utility function are known as indifference curves. The indifference curve that passes through consumption bundle $\widehat{x} \in \mathbb{R}_{+}^{L}$ is defined to be

$$
U_{\widehat{x}}^{0}=\left\{x \in \mathbb{R}_{+}^{L}: U(x)=U(\widehat{x})\right\} .
$$

The (weak) upper contour sets for a utility function could be called consumption requirement sets. The consumption requirement set that is associated with the reference consumption bundle $\widehat{x} \in \mathbb{R}_{+}^{L}$ is defined to be

$$
U_{\widehat{x}}^{+}=\left\{x \in \mathbb{R}_{+}^{L}: U(x) \geq U(\widehat{x})\right\}
$$


Note that we could also define the level set and the (weak) upper contour set for a utility function in terms of a reference utility level, $\bar{U}$. I will leave the modification of the definitions for the case in which a reference utility level is employed as an exercise for you to attempt.
Note that any point $x \in \mathbb{R}_{+}^{L}$ that belongs to an indifference curve must yield the same level of utility.
This means that the change in utility around any indifference curve is zero.
In other words, $d U=0$ along an indifference curve.


Suppose that $L=2$. We have

$$
d U=\left(\frac{\partial U}{\partial x_{1}}\right) d x_{1}+\left(\frac{\partial U}{\partial x_{2}}\right) d x_{2}
$$

Along any indifference curve, we must have

$$
d U=\left(\frac{\partial U}{\partial x_{1}}\right) d x_{1}+\left(\frac{\partial U}{\partial x_{2}}\right) d x_{2}=0
$$

This can be rearranged to obtain

$$
\left.\frac{d x_{2}}{d x_{1}}\right|_{d U=0}=\frac{-\left(\frac{\partial U}{\partial x_{1}}\right)}{\left(\frac{\partial U}{\partial x_{2}}\right)}= -MRS_{12}(x)
$$

which is the formula for the slope of an indifference curve when $U: \mathbb{R}_{+}^{2} \rightarrow \mathbb{R}$.


### The slope of a budget line

A consumer's expenditure is given by the linear function $E: \mathbb{R}_{+}^{L} \rightarrow \mathbb{R}$ that is defined by

$$
E(x)=p^{T} x=\sum_{l=1}^{L} p_{l} x_{l}
$$

where $p \in \mathbb{R}_{++}^{L}$, so that $p_{l}>0$ for all $l \in\{1,2, \cdots, L\}$.

We will refer to this as the budget function, because the term "expenditure function" has a specific meaning in microeconomics that differs somewhat from this function.


The level sets for a budget function are known budget lines (or, if you prefer, budget hyper-planes).

The budget line that passes through consumption bundle $\omega \in \mathbb{R}_{+}^{L}$ is defined to be

$$
B_{\omega}^{0}(p, \boldsymbol{\omega})=\left\{x \in \mathbb{R}_{+}^{L}: E(x)=E(\boldsymbol{\omega})\right\} .
$$

In this case, we might think of $\omega \in \mathbb{R}_{+}^{L}$ as an endowment bundle.

The budget line that is consistent with an exogenous money income equal to $M \in \mathbb{R}_{+}$ is defined to be

$$
B_{M}^{0}(p, M)=\left\{x \in \mathbb{R}_{+}^{L}: E(x)=M\right\}
$$


The (weak) lower contour sets for a budget function are known budget sets. The budget set that is associated with the endowment bundle $\omega \in \mathbb{R}_{+}^{L}$ is defined to be

$$
B_{\omega}^{-}(p, \mathfrak{\omega})=\left\{x \in \mathbb{R}_{+}^{L}: E(x) \leq E(\mathfrak{\omega})\right\}
$$

The (weak) lower contour sets for a budget function are known budget sets. The budget set that is associated with an exogenous money income equal to $M \in \mathbb{R}_{+}$ is defined to be

$$
B_{M}^{-}(p, M)=\left\{x \in \mathbb{R}_{+}^{L}: E(x) \leq M\right\}
$$


Note that any point $x \in \mathbb{R}_{+}^{L}$ that belongs to a budget line must require the same level of expenditure. This means that the change in expenditure along any budget line is zero. In other words, $d E=0$ along a budget line.

Suppose that $L=2$. We have

$$
\begin{aligned}
d E &= \left(\frac{\partial E}{\partial x_{1}}\right) d x_{1}+\left(\frac{\partial E}{\partial x_{2}}\right) d x_{2} \\
&= \left(\frac{\partial\left(p_{1} x_{1}+p_{2} x_{2}\right)}{\partial x_{1}}\right) d x_{1} 
 +\left(\frac{\partial\left(p_{1} x_{1}+p_{2} x_{2}\right)}{\partial x_{2}}\right) d x_{2} \\
&= p_{1} d x_{1}+p_{2} d x_{2}
\end{aligned}
$$



Along any budget line, we must have

$$
d E=p_{1} d x_{1}+p_{2} d x_{2}=0
$$


This can be rearranged to obtain

$$
\left.\frac{d x_{2}}{d x_{1}}\right|_{d E=0}=\frac{-p_{1}}{p_{2}}
$$

which is the formula for the slope of a budget line when the consumption set is given by $\mathbb{R}_{+}^{2}$.



### The slope of an isoquant

Suppose that $F: \mathbb{R}_{+}^{K} \rightarrow \mathbb{R}_{+}$ is a single-product production function.

The level sets for a production function are known as isoquants. The isoquant that is consistent with $y \in \mathbb{R}_{+}$ units of output being produced is defined to be

$$
F_{y}^{0}=\left\{x \in \mathbb{R}_{+}^{K}: F(x)=y\right\}
$$

The (weak) upper contour sets for a production function are known as input requirement sets. The input requirement set that is associated with $y \in \mathbb{R}_{+}$ units of output being produced is defined to be

$$
F_{y}^{+}=\left\{x \in \mathbb{R}_{+}^{K}: F(x) \geq y\right\}
$$


Note that we could also define the level set and the (weak) upper contour set for a production function in terms of a reference input bundle, $\widehat{x}$. I will leave the modification of the definitions for the case in which a reference input bundle is employed as an exercise for you to attempt.
Note that any point $x \in \mathbb{R}_{+}^{K}$ that belongs to an isoquant must yield the same level of output.
This means that the change in output around any isoquant is zero.
In other words, $d F=0$ along an isoquant.


Suppose that $K=2$. We have

$$
d F=\left(\frac{\partial F}{\partial x_{1}}\right) d x_{1}+\left(\frac{\partial F}{\partial x_{2}}\right) d x_{2}
$$

Along any isoquant, we must have

$$
d F=\left(\frac{\partial F}{\partial x_{1}}\right) d x_{1}+\left(\frac{\partial F}{\partial x_{2}}\right) d x_{2}=0
$$

This can be rearranged to obtain

$$
\left.\frac{d x_{2}}{d x_{1}}\right|_{d F=0}=\frac{-\left(\frac{\partial F}{\partial x_{1}}\right)}{\left(\frac{\partial F}{\partial x_{2}}\right)}=-\operatorname{MRTS}_{12}(x)
$$

which is the formula for the slope of an isoquant when $F: \mathbb{R}_{+}^{2} \rightarrow \mathbb{R}_{+}$.



### The slope of an isocost

A firm's expenditure on inputs is given by the linear function $E: \mathbb{R}_{+}^{K} \rightarrow \mathbb{R}$ that is defined by

$$
E(x)=w^{T} x=\sum_{k=1}^{K} w_{k} x_{k}
$$

where $w \in \mathbb{R}_{++}^{K}$ is the input price vector.

Note that $w \in \mathbb{R}_{++}^{K}$ means that $w_{k}>0$ for all $k \in\{1,2, \cdots, K\}$.

We will refer to this as the budget function, because the term "cost function" has a specific meaning in microeconomics that differs somewhat from this function.



The level sets for a firm's budget function are known as isocosts.

The isocost line that passes through input bundle $\widehat{x} \in \mathbb{R}_{+}^{K}$ is defined to be

$$
B_{\widehat{x}}^{0}(w, \widehat{x})=\left\{x \in \mathbb{R}_{+}^{K}: E(x)=E(\widehat{x})\right\} .
$$

In this case, we might think of $\omega \in \mathbb{R}_{+}^{L}$ as an endowment bundle.

The isocost line that is consistent with a particular level of expenditure on inputs equal to $C \in \mathbb{R}_{+}$ is defined to be

$$
B_{C}^{0}(p, M)=\left\{x \in \mathbb{R}_{+}^{K}: E(x)=C\right\} .
$$


Note that any point $x \in \mathbb{R}_{+}^{K}$ that belongs to an isocost line must involve the same level of expenditure on inputs.
This means that the change in expenditure on inputs along any isocost line is zero.
In other words, $d E=0$ along an isocost line.

Suppose that $K=2$. We have

$$
\begin{aligned}
d E &= \left(\frac{\partial E}{\partial x_{1}}\right) d x_{1}+\left(\frac{\partial E}{\partial x_{2}}\right) d x_{2} \\
&= \left(\frac{\partial\left(w_{1} x_{1}+w_{2} x_{2}\right)}{\partial x_{1}}\right) d x_{1} 
 +\left(\frac{\partial\left(w_{1} x_{1}+w_{2} x_{2}\right)}{\partial x_{2}}\right) d x_{2} \\
&= w_{1} d x_{1}+w_{2} d x_{2}
\end{aligned}
$$


Along any isocost line, we must have

$$
d E=w_{1} d x_{1}+w_{2} d x_{2}=0
$$

This can be rearranged to obtain

$$
\left.\frac{d x_{2}}{d x_{1}}\right|_{d E=0}=\frac{-w_{1}}{w_{2}}
$$

which is the formula for the slope of an isocost line for a firm with a production technology that involves two inputs.




## The inverse function theorem

This is Theorem 15.9 in {cite:ps}`simon1994` (p. 367).

Consider a function of the form $f: \mathbb{R}^{n} \longrightarrow \mathbb{R}^{n}$.

Suppose that:
- (a) $f: \mathbb{R}^{n} \longrightarrow \mathbb{R}^{n}$ is at least once continuously differentiable;
- (b) $f\left(x^{*}\right)=y^{*}$; and
- (c) The Jacobian matrix $D_{x} f$ is non-singular at the point $x^{*}$.

If this is the case, then:
- (i) There exists an open ball $B_{r}\left(x^{*}\right)$ around the point $x^{*}$, and an open set $V$ around the point $y^{*}$, such that $f: B_{r}\left(x^{*}\right) \longrightarrow V$ is both one-to-one and onto;
- (ii) The inverse map $f^{-1}: V \longrightarrow B_{r}\left(x^{*}\right)$ is at least once continuously differentiable; and
- (iii) The derivative of the inverse function at the point $x^{*}$ is equal to the inverse of the derivative function at the point $x^{*}$ (that is $\left(D_{x} f^{-1}\right)\left(f\left(x^{*}\right)=\left(D_{x} f\left(x^{*}\right)\right)^{-1}\right)$.



### An elasticity application of the inverse function theorem

Consider a function of the form $f: S \longrightarrow \mathbb{R}$, where $S \subseteq \mathbb{R}^{n}$ is an open set.
Suppose that this function is at least once continuously differentiable in all of its arguments.

The elasticity of $f$ with respect to the variable $x_{i}$ at the point $(x, f(x))$ is given by the formula

$$
\varepsilon_{i}^{f}=\left(\frac{x_{i}}{f(x)}\right)\left(\frac{\partial f}{\partial x_{i}}\right)
$$

Another formula for the elasticity of $f$ with respect to the variable $x_{i}$ at the point $(x, f(x))$ that is sometimes useful is

$$
\varepsilon_{i}^{f}=\frac{\partial \ln (f(x))}{\partial \ln \left(x_{i}\right)}
$$

We will use the chain rule of differentiation and the inverse function theorem to show that these two formulas are equivalent. Note that

$$
\frac{\partial \ln (f(x))}{\partial \ln \left(x_{i}\right)}=\left(\frac{\partial \ln (f(x))}{\partial f(x)}\right)\left(\frac{\partial f(x)}{\partial x_{i}}\right)\left(\frac{\partial x_{i}}{\partial \ln \left(x_{i}\right)}\right)
$$

from the chain rule of differentiation.

We know from the inverse function theorem that

$$
\left(\frac{\partial x_{i}}{\partial \ln \left(x_{i}\right)}\right)=\frac{1}{\left(\frac{\partial \ln \left(x_{i}\right)}{\partial x_{i}}\right)}
$$

Thus we have

$$
\frac{\partial \ln (f(x))}{\partial \ln \left(x_{i}\right)}=\left(\frac{\partial \ln (f(x))}{\partial f(x)}\right)\left(\frac{\partial f(x)}{\partial x_{i}}\right)\left(\frac{1}{\left(\frac{\partial \ln \left(x_{i}\right)}{\partial x_{i}}\right)}\right)
$$

Note that $\frac{\partial \ln (f(x))}{\partial f(x)}=\frac{1}{f(x)}$ and $\frac{\partial \ln \left(x_{i}\right)}{\partial x_{i}}=\frac{1}{x_{i}}$. This means that

$$
\frac{\partial \ln (f(x))}{\partial \ln \left(x_{i}\right)}=\left(\frac{1}{f(x)}\right)\left(\frac{\partial f(x)}{\partial x_{i}}\right)\left(\frac{1}{\frac{1}{x_{i}}}\right)=\left(\frac{1}{f(x)}\right)\left(\frac{\partial f(x)}{\partial x_{i}}\right)\left(x_{i}\right)
$$

This can be rearranged to obtain

$$
\frac{\partial \ln (f(x))}{\partial \ln \left(x_{i}\right)}=\left(\frac{x_{i}}{f(x)}\right)\left(\frac{\partial f}{\partial x_{i}}\right) .
$$


## Homogeneous functions

Consider a function $f: S \longrightarrow \mathbb{R}$, where $S \subseteq \mathbb{R}^{n}$.

This function can be written as $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$.

Let $\lambda>0$ be a positive real number.

The function $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ is said to be **homogeneous of degree $r$** if

$$
f\left(\lambda x_{1}, \lambda x_{2}, \cdots, \lambda x_{n}\right)=\lambda^{r} f\left(x_{1}, x_{2}, \cdots, x_{n}\right)
$$

for all $\left(x_{1}, x_{2}, \cdots, x_{n}\right) \in S$ and all $\lambda>0$.

A function that is homogeneous of degree one is said to be **linearly homogeneous**.


Homogeneity is a cardinal property, not an ordinal one.
This means that homogeneity will not necessarily be preserved under a strictly increasing transformation.
An ordinal property that is somewhat similar to homogeneity is that of homotheticity.
A discussion of homotheticity can be found in {cite:ps}`simon1994` (pp. 483 and 500-504).


## Euler's theorem

Suppose that the function $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ is homogeneous of degree $r$. In this case we have

$$
\sum_{i=1}^{n} x_{i}\left(\frac{\partial f}{\partial x_{i}}\right)=r f\left(x_{1}, x_{2}, \cdots, x_{n}\right) .
$$

This result is known as **Euler's theorem**.

It is straightforward to explicitly derive this result from the definition of homogeneity of degree $r$. The following proof is based on the one in {cite:ps}`haeussler1987` (p. 723).

Since the function $f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ is homogeneous of degree $r$, we know that

$$
f\left(\lambda x_{1}, \lambda x_{2}, \cdots, \lambda x_{n}\right)=\lambda^{r} f\left(x_{1}, x_{2}, \cdots, x_{n}\right)
$$

for any choice of $\lambda>0$.

Consider the left-hand side of this definition first. Let $z_{i}=\lambda x_{i}$ for all $i \in\{1,2, \cdots, n\}$. Totally differentiating

$$
f\left(\lambda x_{1}, \lambda x_{2}, \cdots, \lambda x_{n}\right)=f\left(z_{1}, z_{2}, \cdots, z_{n}\right)
$$

yields

$$
d f=\sum_{i=1}^{n}\left(\frac{\partial f}{\partial z_{i}}\right) d z_{i}
$$

Dividing both sides of this total differential by $d \lambda \neq 0$ yields

$$
\frac{d f}{d \lambda}=\sum_{i=1}^{n}\left(\frac{\partial f}{\partial z_{i}}\right)\left(\frac{d z_{i}}{d \lambda}\right)
$$


Suppose that we hold all of the $x_{i}$ variables constant and only allow $\lambda$ to vary. This means that $d x_{i}=0$ for all $i \in\{1,2, \cdots, n\}$ and $d \lambda \neq 0$. This yields

$$
\left.\frac{d f}{d \lambda}\right|_{d x_{i}=0 \text { for all } i}=\sum_{i=1}^{n}\left(\frac{\partial f}{\partial z_{i}}\right)\left(\left.\frac{d z_{i}}{d \lambda}\right|_{d x_{i}=0 \text { for all } i}\right) .
$$

Note that

$$
\left.\frac{d f}{d \lambda}\right|_{d x_{i}=0 \text { for all } i}=\frac{\partial f}{\partial \lambda}
$$

and

$$
\left.\frac{d z_{i}}{d \lambda}\right|_{d x_{i}=0 \text { for all } i}=\frac{\partial z_{i}}{\partial \lambda}
$$


Thus we have

$$
\frac{\partial f}{\partial \lambda}=\sum_{i=1}^{n}\left(\frac{\partial f}{\partial z_{i}}\right)\left(\frac{\partial z_{i}}{\partial \lambda}\right)
$$

Note that

$$
\frac{\partial z_{i}}{\partial \lambda}=\frac{\partial\left(\lambda x_{i}\right)}{\partial \lambda}=x_{i}
$$

for all $i \in\{1,2, \cdots, n\}$.

Thus we know that the partial derivative of

$$
f\left(\lambda x_{1}, \lambda x_{2}, \cdots, \lambda x_{n}\right)
$$

with respect to $\lambda$ is

$$
\frac{\partial f\left(\lambda x_{1}, \lambda x_{2}, \cdots, \lambda x_{n}\right)}{\partial \lambda}=\sum_{i=1}^{n}\left(\frac{\partial f}{\partial z_{i}}\right) x_{i}
$$

where $z_{i}=\lambda x_{i}$ for all $i \in\{1,2, \cdots, n\}$.


Now consider the right hand-side of the following definition for a function that is homogeneous of degree $r$ :

$$
f\left(\lambda x_{1}, \lambda x_{2}, \cdots, \lambda x_{n}\right)=\lambda^{r} f\left(x_{1}, x_{2}, \cdots, x_{n}\right)
$$

for any choice of $\lambda>0$.

Note that $\lambda$ does not appear in the term $\left(x_{1}, x_{2}, \cdots, x_{n}\right)$. As such, the partial derivative of $\lambda^{r} f\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ with respect to $\lambda$ is

$$
\frac{\partial \lambda^{r} f\left(x_{1}, x_{2}, \cdots, x_{n}\right)}{\partial \lambda}=r \lambda^{r-1} f\left(x_{1}, x_{2}, \cdots, x_{n}\right) .
$$


Thus we have

$$
\begin{aligned}
f\left(\lambda x_{1}, \lambda x_{2}, \cdots, \lambda x_{n}\right) &= \lambda^{r} f\left(x_{1}, x_{2}, \cdots, x_{n}\right) \\[5pt]
\Longleftrightarrow \\[5pt]
\frac{\partial f\left(\lambda x_{1}, \lambda x_{2}, \cdots, \lambda x_{n}\right)}{\partial \lambda} &=\frac{\partial \lambda^{r} f\left(x_{1}, x_{2}, \cdots, x_{n}\right)}{\partial \lambda} \\[5pt]
\Longleftrightarrow \\[5pt]
\sum_{i=1}^{n}\left(\frac{\partial f}{\partial z_{i}}\right) x_{i} &= r \lambda^{r-1} f\left(x_{1}, x_{2}, \cdots, x_{n}\right) .
\end{aligned}
$$


Note that when $\lambda=1$ we have $z_{i}=1 x_{i}=x_{i}$ for all $i \in\{1,2, \cdots, n\}$. This means that

$$
\left(\frac{\partial f}{\partial z_{i}}\right)=\left(\frac{\partial f}{\partial x_{i}}\right)
$$

for all $i \in\{1,2, \cdots, n\}$ when $\lambda=1$.

Note also that when $\lambda=1$ we have

$$
\lambda^{r-1}=1^{r-1}=1
$$

Now suppose that we evaluate both sides of this last equation at the point $\lambda=1$. This yields

$$
\sum_{i=1}^{n}\left(\frac{\partial f}{\partial x_{i}}\right) x_{i}=r f\left(x_{1}, x_{2}, \cdots, x_{n}\right)
$$

which is the result from Euler's theorem.


### Applications of homogeneity

Some applications of homogeneity include the following:
- Returns to scale for production technologies in general.
- Returns to scale for Cobb-Douglas production technologies.
- Euler aggregation for a Marshallian demand function.
- Product exhaustion under perfect competition and constant returns to scale.


### Returns to scale

Suppose that an $n$ input and one output production technology can be represented by a production function of the form

$$
Q=f\left(L_{1}, L_{2}, \cdots, L_{n}\right) .
$$

What happens if we increase all of the inputs by the same proportion?

Specifically, suppose that we move to an input bundle of the form $\left(\lambda L_{1}, \lambda L_{2}, \cdots, \lambda L_{n}\right)$, where $\lambda>0$.


#### Decreasing returns to scale

The production technology is said to display **decreasing returns to scale** if this increase in all of the inputs induces a less than proportionate change in the amount of output that can be produced.

In other words, production technology is said to display decreasing returns to scale if

$$
f\left(\lambda L_{1}, \lambda L_{2}, \cdots, \lambda L_{n}\right)<\lambda Q
$$

which is equivalent to

$$
f\left(\lambda L_{1}, \lambda L_{2}, \cdots, \lambda L_{n}\right)<\lambda f\left(L_{1}, L_{2}, \cdots, L_{n}\right) .
$$

Thus the production function for a production technology that displays decreasing returns to scale is either homogeneous of degree less than one or it is not homogeneous.

```{admonition} Note
:class: tip

A simple replication argument would suggest that decreasing returns to scale do not make any sense.

Studies that find decreasing returns to scale are probably really finding diminishing marginal products.

There is probably some unobserved (or imprecisely observed) input that is implicitly being held fixed or, at least, not varying in the same proportion as all of the other inputs.
```


#### Constant returns to scale
The production technology is said to display **constant returns to scale** if this increase in all of the inputs induces a proportionate change in the amount of output that can be produced.

In other words, production technology is said to display constant returns to scale if

$$
f\left(\lambda L_{1}, \lambda L_{2}, \cdots, \lambda L_{n}\right)=\lambda Q
$$

which is equivalent to

$$
f\left(\lambda L_{1}, \lambda L_{2}, \cdots, \lambda L_{n}\right)=\lambda f\left(L_{1}, L_{2}, \cdots, L_{n}\right) .
$$

Thus the production function for a production technology that displays constant returns to scale will be homogeneous of degree one. (Homogeneity of degree one is sometimes referred to as linear homogeneity.)

```{admonition} Note
:class: tip

Constant returns to scale production technologies are employed in the standard versions of the neoclassical model of economic growth. This model is often referred to as the Solow-Swan model of economic growth, because one of the seminal papers on this model was written by Robert Solow and the other was written by Trevor Swan. Note that the late Trevor Swan was an Australian economist who worked at the Australian National University.

The relevant references are:
- Solow, RM (1956), "A contribution to the theory of economic growth", The Quarterly Journal of Economics 70(1), February, pp. 65-94; and
- Swan, TW (1956), "Economic growth and capital accumulation", The Economic Record 32, November, pp. 334-361.
```

#### Increasing returns to scale
The production technology is said to display **increasing returns to scale** if this increase in all of the inputs induces a more than proportionate change in the amount of output that can be produced.

In other words, production technology is said to display increasing returns to scale if

$$
f\left(\lambda L_{1}, \lambda L_{2}, \cdots, \lambda L_{n}\right)>\lambda Q
$$

which is equivalent to

$$
f\left(\lambda L_{1}, \lambda L_{2}, \cdots, \lambda L_{n}\right)>\lambda f\left(L_{1}, L_{2}, \cdots, L_{n}\right) .
$$

Thus the production function for a production technology that displays increasing returns to scale is either homogeneous of degree more than one or it is not homogeneous.


```{admonition} Note
:class: tip

The returns to scale displayed by some production technologies might vary with input vector.  In such cases, the production technology will not necessarily display either constant returns to scale or increasing returns to scale over the entire set of possible values for the input vector.
```


### Cobb-Douglas production functions

Suppose that an two input and one output production technology can be represented by a production function of the form

$$
Q=f(L, K)=A L^{\alpha} K^{\beta} .
$$

This type of production function is known as a Cobb-Douglas production function.


Suppose that $\lambda>0$. Note that

$$
\begin{aligned}
f(\lambda L, \lambda K) & =A(\lambda L)^{\alpha}(\lambda K)^{\beta} \\
& =A \lambda^{\alpha} L^{\alpha} \lambda^{\beta} K^{\beta} \\
& =\lambda^{\alpha+\beta} A L^{\alpha} K^{\beta} \\
& =\lambda^{\alpha+\beta} f(L, K)
\end{aligned}
$$

Thus we know that the Cobb-Douglas production function is homogeneous of degree $(\alpha+\beta)$.

- If $(\alpha+\beta)<1$, then the Cobb-Douglas production function displays decreasing returns to scale;
- If $(\alpha+\beta)=1$, then the Cobb-Douglas production function displays constant returns to scale; and
- If $(\alpha+\beta)>1$, then the Cobb-Douglas production function displays increasing returns to scale.


### Euler aggregation

Suppose that a consumer's Marshallian demand for commodity $k$ is given by the function

$$
\begin{aligned}
Q_{k} & =x_{k}\left(p_{1}, p_{2}, \cdots, p_{n}, y\right) \\
& =x_{k}(p, y) .
\end{aligned}
$$

It can be shown that under certain circumstance, Marshallian demand functions are homogeneous of degree zero.

We will not provide a formal proof of this proposition here. However, the result is fairly intuitive. If all prices and income increase by the same proportion, then the budget constraint for the consumer does not change. If his or her preferences remain the same, then the fact that the budget constraint has not changed would suggest that the choice problem that faces the consumer has not changed in any real sense. As such, the consumer's optimal consumption bundle should not have changed either.

If Marshallian demand functions are homogeneous of degree zero, then the Marshallian demand for commodity $k$ will be homogeneous of degree zero.
In such circumstances, we know from Euler's theorem that

$$
\begin{aligned}
& y\left(\frac{\partial x_{k}(p, y)}{\partial y}\right)+\sum_{i=1}^{n} p_{i}\left(\frac{\partial x_{k}(p, y)}{\partial p_{i}}\right) \\
= & (0) x_{k}\left(p_{1}, p_{2}, \cdots, p_{n}, y\right) .
\end{aligned}
$$

This can be simplified to obtain

$$
y\left(\frac{\partial x_{k}(p, y)}{\partial y}\right)+\sum_{i=1}^{n} p_{i}\left(\frac{\partial x_{k}(p, y)}{\partial p_{i}}\right)=0
$$


This can be rearranged to obtain

$$
\sum_{i=1}^{n} p_{i}\left(\frac{\partial x_{k}(p, y)}{\partial p_{i}}\right)=-y\left(\frac{\partial x_{k}(p, y)}{\partial y}\right)
$$

Upon dividing both sides of this equation by $x_{k}(p, y)$, we obtain

$$
\begin{aligned}
& \sum_{i=1}^{n}\left(\frac{p_{i}}{x_{k}(p, y)}\right)\left(\frac{\partial x_{k}(p, y)}{\partial p_{i}}\right) \\
= & -\left(\frac{y}{x_{k}(p, y)}\right)\left(\frac{\partial x_{k}(p, y)}{\partial y}\right) .
\end{aligned}
$$

Thus we have

$$
\sum_{i=1}^{n} \varepsilon_{i}^{k}=-\varepsilon_{y}^{k}
$$

where

$$
\varepsilon_{i}^{k}=\left(\frac{p_{i}}{x_{k}(p, y)}\right)\left(\frac{\partial x_{k}(p, y)}{\partial p_{i}}\right)
$$

is the elasticity of demand for commodity $k$ with respect to the price of good $l$ and

$$
\varepsilon_{y}^{k}=\left(\frac{y}{x_{k}(p, y)}\right)\left(\frac{\partial x_{k}(p, y)}{\partial y}\right)
$$

is the income elasticity of demand for commodity $k$.

As far as I am aware, this well known result does not have a name. But it could be referred to as Euler aggregation, because it makes use of Euler's theorem.


### Product exhaustion

Consider a profit-maximising firm that has a single output $(Q)$ and two inputs $(L$ and $K$ ) production technology that displayys constant returns to scale. This production technology can be represented by a production function of the form

$$
Q=f(L, K)
$$

Suppose that this firm is a price-taker in both the ouput market and all input markets.
Constant returns to scale imply that the production function is homogeneous of degree one. Thus we know from Euler's theorem that

$$
\left(\frac{\partial f}{\partial L}\right) L+\left(\frac{\partial f}{\partial K}\right) K=f(L, K)
$$


Recall that the first-order conditions for profit maximisation by a firm with these characteristics are

$$
\left(\frac{\partial f}{\partial L}\right)=\frac{w}{p}
$$

and

$$
\left(\frac{\partial f}{\partial K}\right)=\frac{r}{p}
$$

where $p$ is the output price, $w$ is the input price for labour $(L)$ and $r$ is the input price for capital $(K)$.

Upon substituting these FOCs into Equation (1), we obtain

$$
\left(\frac{w}{p}\right) L+\left(\frac{r}{p}\right) K=f(L, K) .
$$

This can be rearranged to obtain

$$
w L+r K=p f(L, K) .
$$

In other words, the firm's revenue from sales of its output will be exactly equal to the firm's expenditure on inputs. This result, which implies zero profits for a perfectly competitive firm with a constant returns to scale production technology, is known as product exhaustion.

(Note that profit here means economic profit, not accounting profit.)

